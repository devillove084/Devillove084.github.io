{"posts":[{"title":"C++调试（1）—— 认识Dwarf格式","text":"DWARF全称为“Debugging With Attributed Record Formats”，其设计初衷是为了配合ELF格式进行UNIX可执行文件的调试信息生成。DWARF调试信息主要面向开发者用以指导如何生成调试信息以及如何使用调试信息。比如编译器、链接器开发者需要参考DWARF来生成调试信息，而调试器开发者需要参考DWARF来使用调试信息。DWARF开始时主要是为 UNIX 下的调试器提供必要的调试信息，例如内存地址对应的文件名以及代码行号等信息。GCC和Clang以及Go和Rust都使用该格式生成调试信息，与该格式相对的，Windows平台使用PDB（Program Database）作为调试信息的主要格式。2017年，DWARF v5发布，提供了更好的数据压缩能力，调试信息与可执行程序的分离，对macro宏和源代码文件的更好的描述以及更快速的符号搜索、还有对编译器优化后代码的更好描述等等。 DWARF调试信息根据描述对象的不同，在最终存储到不同的section，section名称均以前缀.debug_开头。为了提升效率，对DWARF数据的大多数引用都是通过相对于当前编译单元的偏移量引用。 常见的ELF sections及其存储的内容如下: .debug_abbrev, 存储.debug_info中使用的缩写信息； .debug_arranges, 存储一个加速访问的查询表，通过内存地址查询对应编译单元信息； .debug_frame, 存储调用栈帧信息； .debug_info, 存储核心DWARF数据，包含了描述变量、代码等的DIEs； .debug_line, 存储行号表程序 (程序指令由行号表状态机执行，执行后构建出完整的行号表) .debug_loc, 存储location描述信息； .debug_macinfo, 存储宏相关描述信息； .debug_pubnames, 存储一个加速访问的查询表，通过名称查询全局对象和函数； .debug_pubtypes, 存储一个加速访问的查询表，通过名称查询全局类型； .debug_ranges, 存储DIEs中引用的address ranges； .debug_str, 存储.debug_info中引用的字符串表，也是通过偏移量来引用； .debug_types, 存储描述数据类型相关的DIEs； 符号级调试器需要两张大表，一个是行号表（Line Number Table），一个是调用栈信息表（Call Frame Information）。 行号表, 将程序代码段的指令地址映射为源文件中的地址，如“源文件名:行号”。当然如果指定了源文件中的位置，也可以将其映射为程序代码段中的指令地址。 调用栈信息表, 它允许调试器根据指令地址来定位其在调用栈上的栈帧。 DWARF使用调试信息条目DIE（Debugging Information Entry）来表示每一个编译单元，也即变量、函数、指针等。每个DIE都包含一个tag（如DW_TAG_variable表示变量，DW_TAG_pointer_type表示指针类型，DW_TAG_subprogram表示一个函数等）以及一系列的attributes。每个DIE还可以包含子DIE，结合构成树结构共同描述一个变量、数据类型、函数、编译单元等不同的程序构造，DIE中的每个attribute可以引用另一个DIE（类似指针），例如一个描述变量的DIE，它会包含一个属性DW_AT_type来指向一个描述变量数据类型的DIE。 生成时机 dSYM 文件和 DWARF 文件在编译时生成是根据链接动作中链接脚本下的符号解析与重定位构建。 DIE具体的，每个DIE都包含一个标签（tag）以及一系列的属性（attributes），存储在.debug_info和.debug_types中： tag指明了当前调试信息条目描述的程序构造属于哪种类型，如类型、变量、函数、编译单元等； attribute定义了调试信息条目的一些特征，如函数的返回值类型是int类型。 基本类型描述DW_TAG_base_type，用来描述多种基本类型，包括：整数，地址，字符和浮点数。早期 DWARF 和其他调试信息格式，都假设编译器和调试器对基本类型的大小达成共识，例如int是8位，16位还是32位。但是对于不同的硬件平台和编程语言位宽不一致， DWARF v2以后提供了基础类型和具体硬件上实现的映射解决该问题。 复合类型描述DWARF支持通过组合或者链接其他基本数据类型来定义新的数据类型。TAG为DW_TAG_base_type，表示它是一个基本数据类型，具体为4字节有符号整数。 数组DW_TAG_array_type，结合一些相关attributes共同来描述数组，数组对应的DIE，该DIE包含了这样的一些属性来描述数组元素： DW_AT_ordering：描述数组是按照“行主序”还是按照“列主序”存储，如Fortran是按照列主序存储，C和C++是按照行主序存储。如果未指定该属性值，则使用DW_AT_language指定编程语言的默认数组排列规则； DW_AT_type：描述数组中各个元素的类型信息； DW_AT_byte_stride/DW_AT_bit_stride：如果数组中每个元素的实际大小和分配的空间大小不同的话，可以通过这两个属性来说明； 数组的索引值范围，DIE中也需要通过指定最小、最大索引值来给出一个有效的索引值区间。这样DWARF就可以既能够描述C风格的数组（用0作为数组起始索引），也能够描述Pascal和Ada的数组（其数组最小索引值、最大索引值是可以变化的）。数组维度一般是通过换一个TAG为DW_TAG_subrange_type或者DW_TAG_enumeration_type的DIE来描述。 类组合多种不同的数据类型来定义一个新的数据类型是编程语言的基本功能，DWARF中分别使用下述tag来描述不同类型： DW_TAG_structure_type，描述结构体struct； DW_TAG_class_type，描述类class； DW_TAG_union_type，描述联合union； DW_TAG_interface_type，描述interface。 如果class实例的大小在编译时可以确定，比如都是基础类型或者没有堆上指针，描述class的DIE就会多一个属性DW_AT_byte_size以描述类的大小。 变量DW_TAG_variable，用来描述变量，变量名代指存储变量值的内存位置，变量的类型描述了包含的值及其是否可以修改的修饰（例如const）。对变量进行区分的关键是变量的存储位置和作用域，变量可以被存储在全局数据区（.data section）、栈、堆或者寄存器中，变量的作用域，描述了它在程序中什么时候是可见的，某种程度上，变量作用域是由其声明时的位置确定的，在DWARF中通过三元组（文件名，行号，列号）对变量声明位置进行描述。 位置信息DWARF提供了一种非常通用的机制描述如何确定变量的数据位置，就是通过属性DW_AT_location，该属性允许指定一个操作序列，来告知调试器如何确定数据的位置。调试信息必须为调试器提供一种方法，使其能够查找程序变量的位置、确定动态数组和字符串的范围，以及能找到函数栈帧的基地址或函数返回地址的方法。 而位置信息描述可以分为两类： 位置表达式，是与语言无关的寻址规则表示形式，它是由一些基本构建块、操作序列组合而成的任意复杂度的寻址规则。 只要对象的生命周期是静态或与拥有它的词法块相同，并且在整个生命周期内都不会移动，它们就足以描述任何对象的位置。 位置列表，用于描述生命周期有限的对象或在整个生命周期内可能会更改位置的对象。 位置表达式由零个或多个位置操作组成。 如果没有位置运算表达式，则表示该对象在源代码中存在，但是在目标代码中不存在，可能是由于编译器优化导致的。 可执行描述DW_TAG_subprogram用来描述函数: 函数DIE具有属性 DW_AT_low_pc、DW_AT_high_pc，以给出函数占用的内存地址空间的上下界。 函数的内存地址可能是连续的，也可能不是连续的。如果不连续，则会有一个内存范围列表。一般DW_AT_low_pc的值为函数入口点地址，除非明确指定了另一个地址； 函数的返回值类型由属性 DW_AT_type 描述。 如果没有返回值，则此属性不存在。如果在此函数的相同范围内定义了返回类型，则返回类型DIE将作为此函数DIE的兄弟DIE； 函数可能具有零个或多个形式参数，这些参数由DIE DW_TAG_formal_parameter 描述，这些形参DIE的位置被安排在函数DIE之后，并且各形参DIE的顺序按照形参列表中出现的顺序； 函数主体可能包含局部变量，这些变量由DIE DW_TAG_variables 在形参DIE之后列出。 编译单元在生成程序时，多个源文件都被视为一个独立的编译单元，并被编译为独立的*.o文件（例如C），然后链接器会将这些目标文件、系统特定的启动代码、系统库链接在一起以生成完整的可执行程序。DWARF中采用了C语言中的术语编译单元（compilation unit）作为DIE的名称 DW_TAG_compilation_unit。DIE包含有关编译的常规信息，包括源文件对应的目录和文件名、使用的编程语言、DWARF信息的生产者，以及有助于定位行号和宏信息的偏移量等等。 如果编译单元占用了连续的内存（即，它会被装入一个连续的内存区域），那么该单元的低内存地址和高内存地址将有值，即低pc和高pc属性。 这有助于调试器更轻松地确定特定地址处的指令是由哪个编译单元生成的。 如果编译单元占用的内存不连续，则编译器和链接器将提供代码占用的内存地址列表，每个编译单元都由一个“公共信息条目CIE（Common Information Entry）”表示，编译单元中除了CIE以外，还包含了一系列的帧描述条目FDE（Frame Description Entrie）。 其他信息除了这些内容以外，DWARF调试信息中还有几种非常重要的信息需要描述，符号级调试器非常依赖这些数据。这几种重要的调试信息主要包括： 加速访问 调试器经常需要根据符号名、类型名、指令地址，快速定位到对应的源代码行。DWARF为了加速查询，在DWARF信息生成的时候允许编译器额外创建3张表用来加速查询，加速符号名查询的.debug_pubnames，加速类型名查询的.debug_pubtypes（查询类型），加速指令地址查询的.debug_aranges。 行号表 DWARF行号表，包含了可执行程序机器指令的内存地址和对应的源代码行之间的映射关系。 宏信息 DWARF调试信息中包含了对程序中定义的宏的描述。这是非常基本的信息，但是调试器可以使用它来显示宏的值或将宏翻译成相应的源语言。 调用栈信息 DWARF中的调用栈信息（Call Frame Information，CFI）为调试器提供了如下信息，函数是如何被调用的，如何找到函数参数，如何找到调用函数（caller）的栈帧信息。调试器借助CFI可以展开调用栈、查找上一个函数、确定当前函数的被调用位置以及传递的参数值。 变长数据 DWARF定义了一种可变长度的整数，称为Little Endian Base 128（带符号整数为LEB128或无符号整数为ULEB128），LEB128可以压缩占用的字节来表示整数值以节省存储空间。 压缩DWARF数据 简单玩法为了不编译和链接C++标准库依赖，简化示例，给出下列代码： 12345678int foo(int x, int y) { return x + y;}int main() { int ans = foo(1, 2); return 0;} 执行命令： 12clang -O0 -gdwarf-5 test.cpp -o test # 生成dwarf和调试信息objdump --dwarf=info test 打出来的结果应该如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364test: file format elf64-x86-64Contents of the .debug_info section: Compilation Unit @ offset 0: Length: 0x66 (32-bit) Version: 5 Unit Type: DW_UT_compile (1) Abbrev Offset: 0 Pointer Size: 8 &lt;0&gt;&lt;c&gt;: Abbrev Number: 1 (DW_TAG_compile_unit) &lt;d&gt; DW_AT_producer : (indexed string: 0): Debian clang version 16.0.6 (27) &lt;e&gt; DW_AT_language : 33 (C++14) &lt;10&gt; DW_AT_name : (indexed string: 0x1): test.cpp &lt;11&gt; DW_AT_str_offsets_base: 0x8 &lt;15&gt; DW_AT_stmt_list : 0 &lt;19&gt; DW_AT_comp_dir : (indexed string: 0x2): /home/luhuanbing &lt;1a&gt; DW_AT_low_pc : (index: 0): 0x1130 &lt;1b&gt; DW_AT_high_pc : 0x49 &lt;1f&gt; DW_AT_addr_base : 0x8 &lt;1&gt;&lt;23&gt;: Abbrev Number: 2 (DW_TAG_subprogram) &lt;24&gt; DW_AT_low_pc : (index: 0): 0x1130 &lt;25&gt; DW_AT_high_pc : 0x12 &lt;29&gt; DW_AT_frame_base : 1 byte block: 56 (DW_OP_reg6 (rbp)) &lt;2b&gt; DW_AT_linkage_name: (indexed string: 0x3): _Z3fooii &lt;2c&gt; DW_AT_name : (indexed string: 0x4): foo &lt;2d&gt; DW_AT_decl_file : 0 &lt;2e&gt; DW_AT_decl_line : 1 &lt;2f&gt; DW_AT_type : &lt;0x65&gt; &lt;33&gt; DW_AT_external : 1 &lt;2&gt;&lt;33&gt;: Abbrev Number: 3 (DW_TAG_formal_parameter) &lt;34&gt; DW_AT_location : 2 byte block: 91 7c (DW_OP_fbreg: -4) &lt;37&gt; DW_AT_name : (indexed string: 0x7): x &lt;38&gt; DW_AT_decl_file : 0 &lt;39&gt; DW_AT_decl_line : 1 &lt;3a&gt; DW_AT_type : &lt;0x65&gt; &lt;2&gt;&lt;3e&gt;: Abbrev Number: 3 (DW_TAG_formal_parameter) &lt;3f&gt; DW_AT_location : 2 byte block: 91 78 (DW_OP_fbreg: -8) &lt;42&gt; DW_AT_name : (indexed string: 0x8): y &lt;43&gt; DW_AT_decl_file : 0 &lt;44&gt; DW_AT_decl_line : 1 &lt;45&gt; DW_AT_type : &lt;0x65&gt; &lt;2&gt;&lt;49&gt;: Abbrev Number: 0 &lt;1&gt;&lt;4a&gt;: Abbrev Number: 4 (DW_TAG_subprogram) &lt;4b&gt; DW_AT_low_pc : (index: 0x1): 0x1150 &lt;4c&gt; DW_AT_high_pc : 0x29 &lt;50&gt; DW_AT_frame_base : 1 byte block: 56 (DW_OP_reg6 (rbp)) &lt;52&gt; DW_AT_name : (indexed string: 0x6): main &lt;53&gt; DW_AT_decl_file : 0 &lt;54&gt; DW_AT_decl_line : 5 &lt;55&gt; DW_AT_type : &lt;0x65&gt; &lt;59&gt; DW_AT_external : 1 &lt;2&gt;&lt;59&gt;: Abbrev Number: 5 (DW_TAG_variable) &lt;5a&gt; DW_AT_location : 2 byte block: 91 78 (DW_OP_fbreg: -8) &lt;5d&gt; DW_AT_name : (indexed string: 0x9): ans &lt;5e&gt; DW_AT_decl_file : 0 &lt;5f&gt; DW_AT_decl_line : 6 &lt;60&gt; DW_AT_type : &lt;0x65&gt; &lt;2&gt;&lt;64&gt;: Abbrev Number: 0 &lt;1&gt;&lt;65&gt;: Abbrev Number: 6 (DW_TAG_base_type) &lt;66&gt; DW_AT_name : (indexed string: 0x5): int &lt;67&gt; DW_AT_encoding : 5 (signed) &lt;68&gt; DW_AT_byte_size : 4 &lt;1&gt;&lt;69&gt;: Abbrev Number: 0 总结DWARF的基本概念为在程序编译的链接的符号重定向阶段对符号进行解析并生成对应的信息： 程序被描述为“DIE节点构成的树”，抽象表示源码中的各种函数、数据和类型； 行号表提供了可执行指令地址和生成它们的源码之间的映射关系； CFI描述了如何虚拟地展开堆栈（unwind的用处）。","link":"/2024/06/15/Debug-1/"},{"title":"DuckDB-源码分析（1）背景与应用","text":"2019年，CWI发表了一篇关于DuckDB的论文：《DuckDB: an Embeddable Analytical Database》，旨在OLAP领域构建一个嵌入式的数据库，解决单点交互式数据分析的问题，并给边缘计算提供除SQLite之外的更优选择。在论文中，作者们总结DuckDB为在一个进程内的SQL OLAP DBMS，这句话至少有两个解读点： DuckDB的工作方式应该和SQLite一致，即在进程内部运行，并不需要类似MySQL或者PG的等单独起一个数据库服务； 作为AP系统，可以支撑一定复杂的数据分析和查询任务。 SQLite是在全球运行最多的关系型数据库管理系统，每个浏览器和操作系统以及各种嵌入式设备都能找到该数据库使用的痕迹。但是SQLite更多为TP任务服务，很难利用向量化、内存加速来提高数据分析的速度。所以DuckDB弥补了SQLite这方面的空白。 DuckDB 支持多种数据格式： CSV: 批量加载 CSV 文件并自动映射列内容； DataFrames: DuckDB 可以直接处理同一个 Python 进程中的 DataFrame 内存内容；JSON: DuckDB可以直接将JSON转换为关系型表格。也提供 JSON 数据类型； Parquet: DuckDB 可以查询 Parquet 文件及其架构元数据。查询中使用的谓词会下推到 Parquet 存储层进行计算，以减少加载的数据量。Parquet 是数据湖泊理想的列式格式，可用于读写数据； Apache Arrow: DuckDB 可以通过ADBC直接访问 Apache Arrow 列式数据，无需复制和转换数据； 云存储: DuckDB 可以访问云存储桶（例如 S3 或 GCP）中的数据，减少传输和复制基础设施，并允许廉价处理大量数据。 DuckDB官方提供了一个简单的WASM版本的在线环境：https://shell.duckdb.org/，可以直接在内部执行SQL，简单如下所示： 1234567891011121314151617181920212223242526272829duckdb&gt; SELECT count(*) FROM 'https://shell.duckdb.org/data/tpch/0_01/parquet/lineitem.parquet';┌──────────────┐│ count_star() │╞══════════════╡│ 60175 │└──────────────┘duckdb&gt; SELECT count(*) FROM 'https://shell.duckdb.org/data/tpch/0_01/parquet/customer.parquet';┌──────────────┐│ count_star() │╞══════════════╡│ 1500 │└──────────────┘duckdb&gt; SELECT * FROM 'https://shell.duckdb.org/data/tpch/0_01/parquet/orders.parquet' LIMIT 10;┌────────────┬───────────┬───────────────┬──────────────┬─────────────┬─────────────────┬─────────────────┬────────────────┬───────────────────────────────────────────────────────────────────────────┐│ o_orderkey ┆ o_custkey ┆ o_orderstatus ┆ o_totalprice ┆ o_orderdate ┆ o_orderpriority ┆ o_clerk ┆ o_shippriority ┆ o_comment │╞════════════╪═══════════╪═══════════════╪══════════════╪═════════════╪═════════════════╪═════════════════╪════════════════╪═══════════════════════════════════════════════════════════════════════════╡│ 1 ┆ 370 ┆ O ┆ 172799.49 ┆ 1996-01-02 ┆ 5-LOW ┆ Clerk#000000951 ┆ 0 ┆ nstructions sleep furiously among ││ 2 ┆ 781 ┆ O ┆ 38426.09 ┆ 1996-12-01 ┆ 1-URGENT ┆ Clerk#000000880 ┆ 0 ┆ foxes. pending accounts at the pending, silent asymptot ││ 3 ┆ 1234 ┆ F ┆ 205654.3 ┆ 1993-10-14 ┆ 5-LOW ┆ Clerk#000000955 ┆ 0 ┆ sly final accounts boost. carefully regular ideas cajole carefully. depos ││ 4 ┆ 1369 ┆ O ┆ 56000.91 ┆ 1995-10-11 ┆ 5-LOW ┆ Clerk#000000124 ┆ 0 ┆ sits. slyly regular warthogs cajole. regular, regular theodolites acro ││ 5 ┆ 445 ┆ F ┆ 105367.67 ┆ 1994-07-30 ┆ 5-LOW ┆ Clerk#000000925 ┆ 0 ┆ quickly. bold deposits sleep slyly. packages use slyly ││ 6 ┆ 557 ┆ F ┆ 45523.1 ┆ 1992-02-21 ┆ 4-NOT SPECIFIED ┆ Clerk#000000058 ┆ 0 ┆ ggle. special, final requests are against the furiously specia ││ 7 ┆ 392 ┆ O ┆ 271885.66 ┆ 1996-01-10 ┆ 2-HIGH ┆ Clerk#000000470 ┆ 0 ┆ ly special requests ││ 32 ┆ 1301 ┆ O ┆ 198665.57 ┆ 1995-07-16 ┆ 2-HIGH ┆ Clerk#000000616 ┆ 0 ┆ ise blithely bold, regular requests. quickly unusual dep ││ 33 ┆ 670 ┆ F ┆ 146567.24 ┆ 1993-10-27 ┆ 3-MEDIUM ┆ Clerk#000000409 ┆ 0 ┆ uriously. furiously final request ││ 34 ┆ 611 ┆ O ┆ 73315.48 ┆ 1998-07-21 ┆ 3-MEDIUM ┆ Clerk#000000223 ┆ 0 ┆ ly final packages. fluffily final deposits wake blithely ideas. spe │└────────────┴───────────┴───────────────┴──────────────┴─────────────┴─────────────────┴─────────────────┴────────────────┴───────────────────────────────────────────────────────────────────────────┘ 再测试一下语言的API使用，也非常简单，这里以Python为例： 12345678910111213141516171819202122import duckdbcon = duckdb.connect()con.sql(&quot;SELECT * FROM 'orders.parquet' LIMIT 10&quot;).show()con.close()┌────────────┬───────────┬───────────────┬──────────────┬─────────────┬─────────────────┬─────────────────┬────────────────┬───────────────────────────────────────────────────────────────────────────┐│ o_orderkey │ o_custkey │ o_orderstatus │ o_totalprice │ o_orderdate │ o_orderpriority │ o_clerk │ o_shippriority │ o_comment ││ int32 │ int32 │ varchar │ double │ date │ varchar │ varchar │ int32 │ varchar │├────────────┼───────────┼───────────────┼──────────────┼─────────────┼─────────────────┼─────────────────┼────────────────┼───────────────────────────────────────────────────────────────────────────┤│ 1 │ 370 │ O │ 172799.49 │ 1996-01-02 │ 5-LOW │ Clerk#000000951 │ 0 │ nstructions sleep furiously among ││ 2 │ 781 │ O │ 38426.09 │ 1996-12-01 │ 1-URGENT │ Clerk#000000880 │ 0 │ foxes. pending accounts at the pending, silent asymptot ││ 3 │ 1234 │ F │ 205654.3 │ 1993-10-14 │ 5-LOW │ Clerk#000000955 │ 0 │ sly final accounts boost. carefully regular ideas cajole carefully. depos ││ 4 │ 1369 │ O │ 56000.91 │ 1995-10-11 │ 5-LOW │ Clerk#000000124 │ 0 │ sits. slyly regular warthogs cajole. regular, regular theodolites acro ││ 5 │ 445 │ F │ 105367.67 │ 1994-07-30 │ 5-LOW │ Clerk#000000925 │ 0 │ quickly. bold deposits sleep slyly. packages use slyly ││ 6 │ 557 │ F │ 45523.1 │ 1992-02-21 │ 4-NOT SPECIFIED │ Clerk#000000058 │ 0 │ ggle. special, final requests are against the furiously specia ││ 7 │ 392 │ O │ 271885.66 │ 1996-01-10 │ 2-HIGH │ Clerk#000000470 │ 0 │ ly special requests ││ 32 │ 1301 │ O │ 198665.57 │ 1995-07-16 │ 2-HIGH │ Clerk#000000616 │ 0 │ ise blithely bold, regular requests. quickly unusual dep ││ 33 │ 670 │ F │ 146567.24 │ 1993-10-27 │ 3-MEDIUM │ Clerk#000000409 │ 0 │ uriously. furiously final request ││ 34 │ 611 │ O │ 73315.48 │ 1998-07-21 │ 3-MEDIUM │ Clerk#000000223 │ 0 │ ly final packages. fluffily final deposits wake blithely ideas. spe │├────────────┴───────────┴───────────────┴──────────────┴─────────────┴─────────────────┴─────────────────┴────────────────┴───────────────────────────────────────────────────────────────────────────┤│ 10 rows 9 columns │└──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘ DuckDB的实现主要分为几个部分，parser、logical planner、optimizer、physical planner、execution engine和storage engine。 ParserDuckDB的SQL parser直接使用Postgres的libpg_query，可以直接得到一个C结构体表示的parse tree。DuckDB将其转换成自己内部的C++对象，无缝接入。 Logical PlannerDuckDB的Logical planner由binder和plan generator两部分组成，binder将parse tree与schema的信息（列名、类型等）绑定。plan generator将parse tree转换成一棵逻辑查询算子（scan, filter, project等）结构树。经过planning阶段后生成logical plan，DuckDB同时保存所存储数据的统计信息，这些数据在logical plan生成阶段通过不同的expression trees向下传播，用于optimizer阶段优化查询计划。 OptimizerDuckDB实现了RBO和CBO的优化器。Optimizer将前面logical planner生成的logical plan转换成一个等价但执行代价更小的计划。常见的优化方式有predicate pushdown、expression rewriting、join ordering等。 Physical PannerPhysical planner将logical plan转换成physical plan。在生成physical plan过程中，会选择合适的实现。 Execution EngineDuckDB实现了一个向量化执行引擎，execution engine以火山的模式开始执行查询计划的。Query execution以从物理执行计划的root节点拉取chunk data开始。chunk data是结果集中间表或者基表的水平子集，递归的从子节点拉取数据，到scan operator为止。scan operator从persistent tables中读取chunk data。回溯到root节点的chunk是空时则代表该计划已经执行完。 可移植性问题没有选择JIT实现，因为JIT依赖编译组件，比如LLVM。 TransactionDuckDB使用MVCC提供了ACID，DuckDB实现了HyPer的MVCC变体，就地更新数据，并将previous states保存在一个单独的undo buffer，用于并发事务或者中止。 StorageDuckDB使用列式存储，并使用了读优化的数据存储布局。逻辑表水平划分为chunks of columns，这些chunk of columns压缩成physical block。physical block中保存min/max索引，用于在查询时判断该Block是否相关，裁剪block读的大小。也为每个column保留索引，优化列读。 作为新手学习OLAP，DuckDB是一个非常好的开始，相比于Clickhouse或者其他大型AP实现，其抽象和代码都相对清晰和完善，但是因为使用场景有限，面向云上和多核以及分布式下的设计就需要参考其他的数据库，接下来笔者会从不同部分对DuckDB进行展开和详细剖析。","link":"/2024/05/04/DuckDB-1/"},{"title":"DuckDB-源码分析（2）Parquet读设计","text":"在ParquetReader的构造函数中，会打开文件句柄，加载元数据，并初始化。Scan函数负责实际的扫描过程，处理行组，应用过滤器，读取数据到DataChunk中。首先，在元数据加载部分，parquet文件的元数据通常位于文件末尾，包含schema、行组信息、统计信息等。LoadMetadata函数通过读取文件末尾的元数据部分，解析出FileMetaData结构。 DeriveLogicalType函数根据Parquet的SchemaElement中的类型信息（如Type、converted_type、logicalType）转换为DuckDB的LogicalType。例如，Parquet的INT32可能对应DuckDB的INTEGER，而带有converted_type为DATE的INT32会被转换为DATE类型。这里还处理了时间戳、UUID等复杂类型的转换。 创建列读取器的过程在CreateReaderRecursive中完成，这个函数递归地遍历SchemaElement树，根据每个元素的类型和子元素创建相应的ColumnReader。例如，遇到STRUCT类型时，会创建StructColumnReader，并为每个子列创建对应的读取器，对于重复的字段（如LIST或MAP），会使用ListColumnReader来处理嵌套结构。 初始化时，InitializeSchema调用CreateReader创建根读取器，通常是StructColumnReader，因为它对应Parquet文件的根结构。然后根据读取到的列信息填充columns向量，记录每个列的名称和类型。 扫描数据的过程由Scan函数处理。ParquetReaderScanState用于跟踪扫描的状态，包括当前处理的行组、文件句柄、过滤器等。PrepareRowGroupBuffer函数准备当前行组的数据，可能应用统计信息进行过滤，跳过不需要读取的行组。然后通过ColumnReader的Read方法将数据读取到DataChunk中，应用过滤器（如果有的话），最终将结果返回。 过滤器处理部分，ApplyFilter函数根据过滤条件对读取的数据进行过滤。例如，等值比较、范围比较、IS NULL等条件会被应用到数据上，减少需要处理的数据量，提升查询性能。 初始化12345678910111213141516171819202122ParquetReader::ParquetReader(ClientContext &amp;context, string file_name, ParquetOptions parquet_options, shared_ptr&lt;ParquetFileMetadataCache&gt; metadata) : fs(FileSystem::GetFileSystem(context)), allocator(BufferAllocator::Get(context)), parquet_options(std::move(parquet_options)) { // 1. 打开文件 file_handle = fs.OpenFile(file_name, FileFlags::FILE_FLAGS_READ); // 2. 加载/获取元数据缓存 if (!metadata) { this-&gt;metadata = LoadMetadata(context, allocator, *file_handle, parquet_options.encryption_config, *encryption_util); } else { this-&gt;metadata = std::move(metadata); } // 3. 初始化Schema InitializeSchema(context);} 关键步骤： 文件打开：使用DuckDB统一文件系统接口 元数据加载： 优先使用传入的缓存元数据; 无缓存时调用LoadMetadata解析 Schema初始化：构建列读取器的树 MetaData加载12345678910111213141516171819202122shared_ptr&lt;ParquetFileMetadataCache&gt; LoadMetadata(...) { // 创建Thrift协议解析器 auto file_proto = CreateThriftFileProtocol(allocator, file_handle, false); // 读取文件尾部8字节 transport.read(buf.ptr, 8); if (memcmp(buf.ptr+4, &quot;PAR1&quot;,4)==0) { footer_encrypted = false; // 标准Parquet文件 } else if (memcmp(..., &quot;PARE&quot;, ...)) { footer_encrypted = true; // 加密文件 ParquetCrypto::Read(...); // AES-GCM解密 } // 反序列化元数据 auto metadata = make_uniq&lt;FileMetaData&gt;(); metadata-&gt;read(file_proto.get()); // 处理GeoParquet扩展 auto geo_metadata = GeoParquetFileMetadata::TryRead(*metadata, context); return make_shared_ptr&lt;ParquetFileMetadataCache&gt;(...);} 元数据结构： 12345struct ParquetFileMetadataCache { unique_ptr&lt;FileMetaData&gt; metadata; // Thrift生成的元数据 time_t read_time; // 缓存时间戳 unique_ptr&lt;GeoParquetFileMetadata&gt; geo_metadata; // 地理扩展}; Schema 初始化12345678910111213141516171819202122void ParquetReader::InitializeSchema(ClientContext &amp;context) { auto file_meta_data = GetFileMetadata(); // 1. 创建根读取器 root_reader = CreateReader(context); // 2. 构建列定义 auto &amp;struct_reader = root_reader-&gt;Cast&lt;StructColumnReader&gt;(); for (idx_t i = 0; i &lt; child_types.size(); i++) { columns.emplace_back( MultiFileReaderColumnDefinition( child_types[i].first, // 列名 child_types[i].second // 逻辑类型 ) ); } // 3. 处理生成列（如file_row_number） if (parquet_options.file_row_number) { columns.emplace_back(&quot;file_row_number&quot;, LogicalType::BIGINT); }} 结构说明graph TD A[ParquetReader] --> B[FileHandle] A --> C[ParquetFileMetadataCache] C --> D[FileMetaData] C --> E[GeoParquetMetadata] A --> F[ColumnReader树] subgraph 元数据 D --> G[SchemaElements] D --> H[RowGroups] H --> I[ColumnChunks] I --> J[Statistics] I --> K[EncryptionInfo] end subgraph 读取器结构 F --> L[StructColumnReader] L --> M[ColumnReader...] M --> N[ListColumnReader] M --> O[TemplatedColumnReader] end B -->|读取| P[Parquet文件] style A fill:#f9f,stroke:#333 style C fill:#bbf,stroke:#333 style F fill:#9f9,stroke:#333 关键数据结构说明 FileMetaData 元数据类结构 123456789struct FileMetaData { 1: required i32 version 2: required list&lt;SchemaElement&gt; schema 3: required i64 num_rows 4: required list&lt;RowGroup&gt; row_groups 5: optional list&lt;KeyValue&gt; key_value_metadata 6: optional string created_by 7: optional list&lt;ColumnOrder&gt; column_orders} ColumnReader 类层次 12345678910111213141516171819class ColumnReader {public: virtual void Read(...) = 0; virtual void Skip(...) = 0; virtual unique_ptr&lt;BaseStatistics&gt; Stats(...) = 0;};class StructColumnReader : public ColumnReader { vector&lt;unique_ptr&lt;ColumnReader&gt;&gt; child_readers;};class ListColumnReader : public ColumnReader { unique_ptr&lt;ColumnReader&gt; child_reader;};template &lt;class T&gt;class TemplatedColumnReader : public ColumnReader { // 物理类型特化实现}; 初始化阶段性能优化 元数据缓存 12345metadata = ObjectCache::Get(file_name);if (!metadata || expired) { metadata = LoadMetadata(...); ObjectCache::Put(file_name, metadata);} 延迟加载 123456void StructColumnReader::InitializeRead(...) { // 按需初始化子读取器 for (auto &amp;child : child_readers) { if (child) child-&gt;InitializeRead(...); }} 内存预分配 12ResizeableBuffer define_buf;define_buf.resize(allocator, STANDARD_VECTOR_SIZE); // 2048 初始化时序图sequenceDiagram participant Client participant ParquetReader participant FileSystem participant ThriftParser participant ColumnReader Client->>ParquetReader: 创建Reader(file_name) ParquetReader->>FileSystem: OpenFile() FileSystem-->>ParquetReader: 返回FileHandle alt 无缓存 ParquetReader->>ThriftParser: LoadMetadata() ThriftParser->>FileHandle: 读取文件尾部 ThriftParser-->>ParquetReader: FileMetaData else 有缓存 ParquetReader->>ObjectCache: GetMetadata() end ParquetReader->>ColumnReader: CreateReaderRecursive() loop 递归创建 ColumnReader->>ColumnReader: 创建子读取器 end ParquetReader-->>Client: 初始化完成 扫描数据过程123456789101112131415161718192021222324252627void ParquetReader::InitializeScan(ClientContext &amp;context, ParquetReaderScanState &amp;state, vector&lt;idx_t&gt; groups_to_read) { // 重置扫描状态 state.current_group = -1; state.group_offset = 0; state.finished = false; // 独立文件句柄（支持多线程） state.file_handle = fs.OpenFile(file_name, FileFlags::FILE_FLAGS_READ | (prefetch ? FILE_FLAGS_DIRECT_IO : 0)); // 创建Thrift协议解析器（每个扫描状态独立） state.thrift_file_proto = CreateThriftFileProtocol(allocator, *state.file_handle, state.prefetch_mode); // 创建列读取器树（可能带谓词过滤） state.root_reader = CreateReader(context); // 初始化缓冲区 state.define_buf.resize(allocator, STANDARD_VECTOR_SIZE); state.repeat_buf.resize(allocator, STANDARD_VECTOR_SIZE); // 配置预取模式 Value prefetch_all; context.TryGetCurrentSetting(&quot;prefetch_all_parquet_files&quot;, prefetch_all); state.prefetch_mode = prefetch_all.GetValue&lt;bool&gt;();} 关键点： 每个扫描状态维护独立的文件句柄和Thrift解析器，支持并发扫描 根据配置选择直接IO模式（减少内核缓存开销） 列读取器树可能根据谓词进行剪枝（减少不必要的列读取） RowGroup处理123456789101112131415161718192021void ParquetReader::PrepareRowGroupBuffer(ParquetReaderScanState &amp;state, idx_t out_col_idx) { auto &amp;group = GetGroup(state); auto column_id = reader_data.column_ids[out_col_idx]; auto &amp;column_reader = state.root_reader-&gt;Cast&lt;StructColumnReader&gt;().GetChildReader(column_id); // 1. 检查行组统计信息 auto stats = column_reader.Stats(state.current_group, group.columns); if (stats &amp;&amp; reader_data.filters) { auto global_id = reader_data.column_mapping[out_col_idx]; auto filter = reader_data.filters-&gt;filters[global_id]; // 2. 应用统计过滤 if (stats-&gt;CheckFilter(*filter) == FilterPropagateResult::FILTER_ALWAYS_FALSE) { state.group_offset = group.num_rows; // 跳过整个行组 return; } } // 3. 初始化该RowGroup列读取器 column_reader.InitializeRead(state.current_group, group.columns, *state.thrift_file_proto);} 过滤逻辑： 使用行组的min/max值快速判断是否跳过; 对字符串列使用更精确的字典过滤; ScanInternal 函数ScanInternal函数负责从Parquet文件中读取数据到DataChunk中，处理行组切换、预取、数据解码和过滤等步骤。函数开始检查是否完成扫描，然后处理行组切换。在切换行组时，会进行预取操作，包括整个行组的预取或按列预取。之后，函数计算需要读取的行数，初始化过滤掩码，然后按列读取数据，应用过滤器，最后更新状态。在预取部分，代码根据是否启用预取模式（prefetch_mode）以及扫描的列比例（scan_percentage）来决定是预取整个行组还是按列预取。 行组切换与预取处理 123456789101112131415161718192021222324252627282930313233343536373839if (state.current_group &lt; 0 || (int64_t)state.group_offset &gt;= GetGroup(state).num_rows) { // 切换到下一个行组 state.current_group++; state.group_offset = 0; auto &amp;trans = reinterpret_cast&lt;ThriftFileTransport &amp;&gt;(*state.thrift_file_proto-&gt;getTransport()); trans.ClearPrefetch(); // 清空之前的预取 // 检查行组边界 if ((idx_t)state.current_group == state.group_idx_list.size()) { state.finished = true; return false; } // 计算需要扫描的压缩字节数 uint64_t to_scan_compressed_bytes = 0; for (所有列) { PrepareRowGroupBuffer(...); // 准备列读取器 to_scan_compressed_bytes += 列压缩大小; } // 预取决策逻辑 if (state.prefetch_mode) { double scan_percentage = 扫描字节数 / 行组总跨度; if (scan_percentage &gt; 0.95) { // 全行组预取 trans.Prefetch(起始偏移, 行组总跨度); } else { // 按列预取 for (所有列) { if (无过滤 || 列有过滤条件) { 注册列预取; } } trans.PrefetchRegistered(); // 触发预取 } }} 预取策略： 条件 策略 适用场景 scan_percentage &gt; 95% 预取整个行组 全表扫描 存在过滤器 仅预取过滤列 条件查询 无过滤器 预取所有列 投影查询 读取逻辑 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748// 计算本次读取行数idx_t this_output_chunk_rows = Min(STANDARD_VECTOR_SIZE, GetGroup().num_rows - state.group_offset);result.SetCardinality(this_output_chunk_rows);// 初始化过滤掩码parquet_filter_t filter_mask;filter_mask.set(); // 初始全有效for (i &gt;= this_output_chunk_rows) filter_mask.reset(i); // 屏蔽超出部分// 列处理流程if (存在过滤器) { // 第一阶段：处理过滤列 for (每个过滤条件) { if (过滤列是常量) { ApplyFilter(常量向量, 过滤条件); } else { 读取过滤列数据到result_vector; ApplyFilter(result_vector, 过滤条件); } } // 第二阶段：处理其他列 for (剩余列) { if (过滤结果全无效) { Skip(); // 跳过解码 } else { 读取列数据到result_vector; } } // 生成选择向量 SelectionVector sel; for (i=0 到 this_output_chunk_rows) { if (filter_mask.test(i)) sel.append(i); } result.Slice(sel); // 压缩结果} else { // 无过滤直接读取 for (所有列) { 读取列数据到result_vector; }}// 更新偏移量state.group_offset += this_output_chunk_rows; ThriftFileTransport 数据结构123456789struct ReadHead { ReadHead(idx_t location, uint64_t size) : location(location), size(size) {}; idx_t location; // 数据块起始位置 uint64_t size; // 数据块大小 AllocatedData data; // 分配的存储空间 bool data_isset = false; // 数据是否已加载标记 idx_t GetEnd() const { return size + location; } // 获取数据块结束位置 void Allocate(Allocator &amp;allocator) { data = allocator.Allocate(size); } // 分配内存空间}; ReadHead表示一个数据块的基本信息，包括起始位置、大小、存储空间等信息。初始化时传入起始位置和大小，并且提供获取数据块结束位置的方法，以及提供内存分配方法，将数据加载到data缓冲区。 1234567891011121314struct ReadHeadComparator { static constexpr uint64_t ALLOW_GAP = 1 &lt;&lt; 14; // 16 KiB bool operator()(const ReadHead *a, const ReadHead *b) const { auto a_start = a-&gt;location; auto a_end = a-&gt;location + a-&gt;size; auto b_start = b-&gt;location; if (a_end &lt;= NumericLimits&lt;idx_t&gt;::Maximum() - ALLOW_GAP) { a_end += ALLOW_GAP; } return a_start &lt; b_start &amp;&amp; a_end &lt; b_start; }}; ReadHeadComparator用于比较两个ReadHead对象的位置关系，判断是否允许合并或重叠。具体的，类中定义了一个允许的隔距ALLOW_GAP（16KiB），用来比较两个数据块的起始位置和结束位置：如果a的结束位置（加上允许隔距）在b的起始位置之前，则认为两者不重叠。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465struct ReadAheadBuffer { ReadAheadBuffer(Allocator &amp;allocator, FileHandle &amp;handle) : allocator(allocator), handle(handle) {} std::list&lt;ReadHead&gt; read_heads; // 存储所有预读数据块 std::set&lt;ReadHead *, ReadHeadComparator&gt; merge_set; // 用于合并数据块 Allocator &amp;allocator; // 内存分配器 FileHandle &amp;handle; // 文件句柄 idx_t total_size = 0; // 总预读大小 // 添加一个预读数据块 void AddReadHead(idx_t pos, uint64_t len, bool merge_buffers = true) { // 尝试合并现有数据块 if (merge_buffers) { ReadHead new_read_head {pos, len}; auto lookup_set = merge_set.find(&amp;new_read_head); if (lookup_set != merge_set.end()) { auto existing_head = *lookup_set; auto new_start = MinValue(existing_head-&gt;location, new_read_head.location); auto new_length = MaxValue(existing_head-&gt;GetEnd(), new_read_head.GetEnd()) - new_start; existing_head-&gt;location = new_start; existing_head-&gt;size = new_length; return; } } read_heads.emplace_front(ReadHead(pos, len)); total_size += len; auto &amp;read_head = read_heads.front(); if (merge_buffers) { merge_set.insert(&amp;read_head); } // 检查数据块是否超出文件范围 if (read_head.GetEnd() &gt; handle.GetFileSize()) { throw std::runtime_error(&quot;Prefetch registered for bytes outside file&quot;); } } // 获取相关数据块 ReadHead *GetReadHead(idx_t pos) { for (auto &amp;read_head : read_heads) { if (pos &gt;= read_head.location &amp;&amp; pos &lt; read_head.GetEnd()) { return &amp;read_head; } } return nullptr; } // 预读所有数据块 void Prefetch() { for (auto &amp;read_head : read_heads) { read_head.Allocate(allocator); if (read_head.GetEnd() &gt; handle.GetFileSize()) { throw std::runtime_error(&quot;Prefetch requested for bytes outside file&quot;); } handle.Read(read_head.data.get(), read_head.size, read_head.location); read_head.data_isset = true; } }}; ReadAheadBuffer 结构用来管理预读数据块的队列和合并操作，list存储所有未读取的数据块，set配合ReadHeadComparator合并相邻或重叠的数据块，减少存储和合并IO，具体的： AddReadHead添加预读数据块，并尝试与现有数据块合并； GetReadHead根据给定位置返回对应的数据块； Prefetch将所有预读数据块从文件中读取并加载到内存。 12345678910111213141516171819202122232425262728293031323334353637class ThriftFileTransport { // void Prefetch(idx_t pos, uint64_t len) { RegisterPrefetch(pos, len, false); FinalizeRegistration(); PrefetchRegistered(); } // void RegisterPrefetch(idx_t pos, uint64_t len, bool can_merge = true) { ra_buffer.AddReadHead(pos, len, can_merge); } // void FinalizeRegistration() { ra_buffer.merge_set.clear(); } // void PrefetchRegistered() { ra_buffer.Prefetch(); } // void ClearPrefetch() { ra_buffer.read_heads.clear(); ra_buffer.merge_set.clear(); } uint32_t read(uint8_t *buf, uint32_t len) { auto prefetch_buffer = ra_buffer.GetReadHead(location); if (prefetch_buffer != nullptr &amp;&amp; location - prefetch_buffer-&gt;location + len &lt;= prefetch_buffer-&gt;size) { // 如果有预读数据，直接使用 if (!prefetch_buffer-&gt;data_isset) { prefetch_buffer-&gt;Allocate(allocator); handle.Read(prefetch_buffer-&gt;data.get(), prefetch_buffer-&gt;size, prefetch_buffer-&gt;location); prefetch_buffer-&gt;data_isset = true; } memcpy(buf, prefetch_buffer-&gt;data.get() + location - prefetch_buffer-&gt;location, len); } else { // 如果没有预读数据，直接从文件中读取 if (prefetch_mode &amp;&amp; len &lt; PREFETCH_FALLBACK_BUFFERSIZE &amp;&amp; len &gt; 0) { Prefetch(location, MinValue(handle.GetFileSize() - location, PREFETCH_FALLBACK_BUFFERSIZE)); auto prefetch_buffer_fallback = ra_buffer.GetReadHead(location); memcpy(buf, prefetch_buffer_fallback-&gt;data.get() + location - prefetch_buffer_fallback-&gt;location, len); } else { handle.Read(buf, len, location); } } location += len; return len; }private: FileHandle &amp;handle; idx_t location; Allocator &amp;allocator; ReadAheadBuffer ra_buffer; bool prefetch_mode;} ThriftFileTransport重载read方法，优先使用预读数据块，如果数据块未加载则实时读取，并且提供方法Prefetch、RegisterPrefetch、FinalizeRegistration和PrefetchRegistered，实现预读策略： 先注册所有需要预读的范围； 清理合并操作以固定范围； 执行预读操作将数据加载到内存。 不同预取策略： 策略类型 触发条件 优势 实现位置 主动批量预取 已知读取模式时 最大化顺序读取性能 ReadAheadBuffer::Prefetch 被动按需预取 未预取或预取未覆盖时 避免小数据预取开销 ThriftFileTransport::read 读取流程图 graph TD A[打开文件] --> B{加密检查} B -- 是 --> C[解密元数据] B -- 否 --> D[直接读取元数据] C --> E[解析元数据] D --> E E --> F[构建Schema树] F --> G[递归创建列读取器] G --> H[Struct/List列处理] G --> I[基本类型列处理] subgraph 扫描循环 J[选择行组] --> K{行组过滤?} K -- 跳过 --> M[下一行组] K -- 读取 --> L[初始化行组读取] L --> N[预取数据] N --> O[列并行读取] O --> P[应用谓词下推] P --> Q[向量化过滤] Q --> R[输出DataChunk] R --> S{更多行组?} S -- 是 --> J S -- 否 --> T[结束扫描] end E --> J I --> O style A fill:#f9f,stroke:#333 style E fill:#bbf,stroke:#333 style G fill:#9f9,stroke:#333 style J fill:#f96,stroke:#333 style O fill:#6f9,stroke:#333 总结DuckDB 读取 Parquet 文件的设计通过文件系统接口打开文件，借助 Thrift 协议解析元数据，构建列读取器树并初始化 Schema。在扫描过程中，通过行组过滤、列并行读取和谓词下推等优化策略，高效地预取数据并将其转化为向量化数据块，同时利用元数据缓存、延迟加载和内存预分配等机制提升性能，实现对 Parquet 文件的快速读取和处理。设计充分考虑了现代硬件特性，通过向量化处理、数据局部性优化和多级并行，实现了高效的列式数据读取。同时，灵活的过滤机制使得在复杂查询场景下能显著减少不必要的IO和计算开销。","link":"/2025/02/16/DuckDB-2/"},{"title":"多线程共享与伪共享检测","text":"多线程/进程之间使用缓存一致性协议来保证每个包含独立缓存对象的核心在共享使用内存数据时的一致性，缓存一致性协议保证了缓存实体中的任何更新都会对相同位置的其他缓存进行全部更新。MESI协议是最著名的一致性协议，支持现代CPU中缓存回写。通过监控内存事务保持一致性，一致性问题可以得到缓解，但是是有代价的，导致CPU访存空转，浪费系统带宽，两种具有代表性的内存一致性问题是“真共享”和“伪共享”。 “真共享”多线程同时访问相同变量时，为真共享，下列代码简单说明了该问题： 123456let mut sum: u64 = 0;{ // parallel section for i in 0..N { sum += i; // sum is shared between all threads } } 数据竞争是未定义行为，C++下Clang的Thread sanitizer和helgrind可以一定程度检查出数据竞争。而Rust中的数据竞争检测在编译阶段就通过其所有权机制和借用检查器实现，具体的，Rust利用RAII机制在作用域结束时释放对象持有的内存，该内存有且只有一个对象持有，借用检查器是Rust在编译期预防数据竞争的主要手段，确保了在任何时候只有一个线程可以“借用”内存区域。 而在C++中，将sum变量变为原子变量有助于解决真共享发生的数据竞争问题。但是在进行内存顺序的排序、串行化的内存操作之后，会极为影响性能，详细见Perfbook关于多线程Counter的相关设计。 而另外一个解决真共享的方法是使用TLS（Thread Local Storage，TLS）。通过TLS在给定的多线程下，每个线程都可以分配内存来存储该线程内的独占数据，线程之间便可不竞争全局变量。在C++使用thread_local关键字修饰某个变量为线程独占，而在Rust下使用 thread_local 宏可以初始化线程局部变量，然后在线程内部使用该变量的 with 方法获取变量值，或者使用attribute的的#[thread_local]进行标记，或者，可以使用thread_local第三方库，具体例子见下面代码： 123456789101112131415thread_local! { static MACRO_TLS: std::cell::RefCell&lt;Foo&gt; = std::cell::RefCell::new(Foo(0));}#[thread_local]static mut ATTR_TLS: Foo = Foo(0);fn main() { let _ = std::thread::spawn(|| unsafe { MACRO_TLS.with(|f| { println!(&quot;foo: {}&quot;, f.borrow_mut().0); }); }) .join();} 123456789101112131415161718192021222324252627#include &lt;iostream&gt;#include &lt;mutex&gt;#include &lt;string&gt;#include &lt;thread&gt; thread_local unsigned int rage = 1; std::mutex cout_mutex; void increase_rage(const std::string&amp; thread_name){ ++rage; // modifying outside a lock is okay; this is a thread-local variable std::lock_guard&lt;std::mutex&gt; lock(cout_mutex); std::cout &lt;&lt; &quot;Rage counter for &quot; &lt;&lt; thread_name &lt;&lt; &quot;: &quot; &lt;&lt; rage &lt;&lt; '\\n';} int main(){ std::thread a(increase_rage, &quot;a&quot;), b(increase_rage, &quot;b&quot;); { std::lock_guard&lt;std::mutex&gt; lock(cout_mutex); std::cout &lt;&lt; &quot;Rage counter for main: &quot; &lt;&lt; rage &lt;&lt; '\\n'; } a.join(); b.join();} C++的例子来自于cppreference, 当使用该关键字声明时，每个线程都有自己的副本，当通过名称引用时，将使用与当前线程关联的副本。而如果在类的成员上使用thread_local关键字修饰，则在同一个线程内该类的多个成员都会共享该变量，这一点和static关键字一致。 “伪共享”当多个不同的线程修改恰巧位于同一缓存行的不同变量时，称为“伪共享”。下面使用一段代码来说明该问题： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226#define _MULTI_THREADED#define _GNU_SOURCE#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;stdlib.h&gt;#include &lt;stdint.h&gt;#include &lt;unistd.h&gt;#include &lt;sched.h&gt;#include &lt;pthread.h&gt;#include &lt;numa.h&gt;#include &lt;sys/types.h&gt;/* * A thread on each numa node seems to provoke cache misses */#define LOOP_CNT (5 * 1024 * 1024)#if defined(__x86_64__) || defined(__i386__)static __inline__ uint64_t rdtsc() { unsigned hi, lo; __asm__ __volatile__ ( &quot;rdtsc&quot; : &quot;=a&quot;(lo), &quot;=d&quot;(hi)); return ( (uint64_t)lo) | ( ((uint64_t)hi) &lt;&lt; 32);}#elif defined(__aarch64__)static __inline__ uint64_t rdtsc(void){ uint64_t val; /* * According to ARM DDI 0487F.c, from Armv8.0 to Armv8.5 inclusive, the * system counter is at least 56 bits wide; from Armv8.6, the counter * must be 64 bits wide. So the system counter could be less than 64 * bits wide and it is attributed with the flag 'cap_user_time_short' * is true. */ asm volatile(&quot;mrs %0, cntvct_el0&quot; : &quot;=r&quot; (val)); return val;}#endif/* * Create a struct where reader fields share a cacheline with the hot lock field. * Compiling with -DNO_FALSE_SHARING inserts padding to avoid that sharing. */typedef struct _buf { long lock0; long lock1; long reserved1;#if defined(NO_FALSE_SHARING) long pad[5]; // to keep the 'lock*' fields on their own cacheline.#else long pad[1]; // to provoke false sharing.#endif long reader1; long reader2; long reader3; long reader4;} buf __attribute__((aligned (64)));buf buf1;buf buf2;volatile int wait_to_begin = 1;struct thread_data *thread;int max_node_num;int num_threads;char * lock_thd_name = &quot;lock_th&quot;;char * reader_thd_name = &quot;reader_thd&quot;;#define checkResults(string, val) { \\ if (val) { \\ printf(&quot;Failed with %d at %s&quot;, val, string); \\ exit(1); \\ } \\}struct thread_data { pthread_t tid; long tix; long node; char *name;};/* * Bind a thread to the specified numa node.*/void setAffinity(void *parm) { volatile uint64_t rc, j; int node = ((struct thread_data *)parm)-&gt;node; char *func_name = ((struct thread_data *)parm)-&gt;name; numa_run_on_node(node); pthread_setname_np(pthread_self(),func_name);}/* * Thread function to simulate the false sharing. * The &quot;lock&quot; threads will test-n-set the lock field, * while the reader threads will just read the other fields * in the struct. */extern void *read_write_func(void *parm) { int tix = ((struct thread_data *)parm)-&gt;tix; uint64_t start, stop, j; char *thd_name = ((struct thread_data *)parm)-&gt;name; // Pin each thread to a numa node. setAffinity(parm); // Wait for all threads to get created before starting. while(wait_to_begin) ; start = rdtsc(); for(j=0; j&lt;LOOP_CNT; j++) { // Check for lock thread. if (*thd_name == *lock_thd_name) { __sync_lock_test_and_set(&amp;buf1.lock0, 1 ); buf1.lock0 += 1; buf2.lock1 = 1; } else { // Reader threads. switch(tix % max_node_num) { volatile long var; case 0: var = *(volatile uint64_t *)&amp;buf1.reader1; var = *(volatile uint64_t *)&amp;buf2.reader1; break; case 1: var = *(volatile uint64_t *)&amp;buf1.reader2; var = *(volatile uint64_t *)&amp;buf2.reader2; break; case 2: var = *(volatile uint64_t *)&amp;buf1.reader3; var = *(volatile uint64_t *)&amp;buf2.reader3; break; case 3: var = *(volatile uint64_t *)&amp;buf1.reader4; var = *(volatile uint64_t *)&amp;buf2.reader4; break; }; }; } // End of for LOOP_CNT loop // Print out stats // stop = rdtsc(); int cpu = sched_getcpu(); int node = numa_node_of_cpu(cpu); printf(&quot;%ld mticks, %s (thread %d), on node %d (cpu %d).\\n&quot;, (stop-start)/1000000, thd_name, tix, node, cpu); return NULL;}int main ( int argc, char *argv[] ){ int i, n, rc=0; if ( argc != 2 ) /* argc should be 2 for correct execution */ { printf( &quot;usage: %s &lt;n&gt;\\n&quot;, argv[0] ); printf( &quot;where \\&quot;n\\&quot; is the number of threads per node\\n&quot;); exit(1); } if ( numa_available() &lt; 0 ) { printf( &quot;NUMA not available\\n&quot; ); exit(1); } int thread_cnt = atoi(argv[1]); max_node_num = numa_max_node(); if ( max_node_num == 0 ) max_node_num = 1; int node_cnt = max_node_num + 1; // Use &quot;thread_cnt&quot; threads per node. num_threads = (max_node_num +1) * thread_cnt; thread = malloc( sizeof(struct thread_data) * num_threads); // Create the first half of threads as lock threads. // Assign each thread a successive round robin node to // be pinned to (later after it gets created.) // for (i=0; i&lt;=(num_threads/2 - 1); i++) { thread[i].tix = i; thread[i].node = i%node_cnt; thread[i].name = lock_thd_name; rc = pthread_create(&amp;thread[i].tid, NULL, read_write_func, &amp;thread[i]); checkResults(&quot;pthread_create()\\n&quot;, rc); usleep(500); } // Create the second half of threads as reader threads. // Assign each thread a successive round robin node to // be pinned to (later after it gets created.) // for (i=((num_threads/2)); i&lt;(num_threads); i++) { thread[i].tix = i; thread[i].node = i%node_cnt; thread[i].name = reader_thd_name; rc = pthread_create(&amp;thread[i].tid, NULL, read_write_func, &amp;thread[i]); checkResults(&quot;pthread_create()\\n&quot;, rc); usleep(500); } // Sync to let threads start together usleep(500); wait_to_begin = 0; for (i=0; i &lt;num_threads; i++) { rc = pthread_join(thread[i].tid, NULL); checkResults(&quot;pthread_join()\\n&quot;, rc); } return 0;} 这段代码是一个多线程的C代码。具体地，通过创建不同的线程并将它们绑定到特定的NUMA节点，伪造伪共享的效果： NUMA将每个线程绑定到特定的NUMA节点，确保线程在特定物理内存位置上运行； 高精度计时使用rdtsc指令测量代码段的执行时间； 伪共享和缓存行对齐结构体buf中成员可能会因为缓存行对齐而共享相同的缓存行。 这里有两种编译模式： 默认模式下只有一个长整型的填充，用于触发伪共享； 另一模式通过定义 NO_FALSE_SHARING 预处理宏，增加额外的填充以避免伪共享。 下面是一些代码的详细解释和补充说明，可以按需跳过： 结构体 buf 的设计：这个结构体有两个锁变量 lock0 和 lock1，以及四个变量 reader1 到 reader4。根据编译选项，这些变量可能位于同一缓存行（为了故意制造伪共享），或者通过插入填充以分开到不同的缓存行（为了避免伪共享）。 线程功能 read_write_func：每个线程都会执行这个函数。如果是锁线程（通过名称判断），它会连续设置并增加锁变量；如果是读取线程，它则会读取指定的读变量。这样设计是为了在锁和读取之间制造大量的内存访问和潜在的竞争。 主函数：首先检查 NUMA 是否可用，然后解析命令行参数来确定每个 NUMA 节点应创建多少线程。之后，为每个线程分配一个结构体 thread_data，用于存储线程的元数据，如线程 ID、所属 NUMA 节点和线程名称。然后创建线程，先是锁线程后是读取线程，每个线程都通过 pthread_create 调用启动。 下面介绍perf工具如何检查伪共享，以及如何解读结果。Linux perf工具和Intel VTune Profiler都支持检测伪共享。这里仅对perf工具进行展开，perf c2c会匹配不同线程的内存保存和和加载地址，并查看是否有在被修改的缓存行的命中。这里先对上述C代码进行编译和运行，并在同时收集perf data： 12345clang -g false_sharing_example.c -pthread -lnuma -o false_sharing./false_sharing.exe &lt;number of threads per node&gt;sudo perf c2c record -F 60000 -a -u --ldlat 50 sleep 3 结果： 结果1：HITM代表的是在修改后的cacheline中命中的负载，这是伪共享发生的关键指标。 Remote HITM是跨NUMA节点，是最贵的操作； 结果2：第二个结果是个排序的热度，按cacheline的远程HITM或本地HITM进行排序，Rmt LLC Load Hitm标记了跨NUMA节点； 结果3：访问cache line的平均延迟、CPU时钟周期、线程pid、函数之类的信息，具体的： Shared Data Cache Line Table：每一行代表一个缓存行，列出内存地址、所属NUMA节点、页面访问计数（PA cnt）、命中次数（Hitm）、以及其他的一些命中和失效统计数据。可以看出缓存行地址分别有超过50,000次的总访问（Total records），它们在本地节点（LclHitm）和远程节点（RmtHitm）上的命中次数，这可以显示出在不同节点间的数据共享模式。 Shared Cache Line Distribution Pareto：展示缓存行命中的分布情况。可以看到远程命中（RmtHitm）和本地命中（LclHitm）的百分比，这是衡量NUMA节点之间内存访问效率的重要指标。比如，对于第一行（#0）的缓存行，有大约30%的访问是本地命中，也有相似的百分比是远程命中，有相当比例的内存访问需要跨越 NUMA 节点，这通常会导致更高的延迟。 Code address, cycles, and symbols：列出了代码地址与其对应的 CPU 周期数，这有助于识别哪些函数可能对缓存行产生影响，并估计它们的影响大小。例如，对于不同的代码地址，有不同的本地和远程命中次数（lcl hitm 和 rmt hitm），可以识别出哪些代码路径可能导致不必要的远程内存访问。","link":"/2024/04/19/False-share/"},{"title":"现代 C++ 分享（1）—— 生命周期、所有权和资源管理","text":"这是整个现代 C++ 分享系列第一篇，关于生命周期、所有权和资源管理。主题包括但不限于指针、智能指针、引用、类型系统、移动语义以及完美转发和引用折叠相关的主题。 C++ 在演进过程中逐渐增强和扩展了对类型处理的能力： C++11 中引入右值引用，通过重新定义值类别对表达式进行分类，右值引用能表达移动语义，解决了 C++11 之前产生的中间临时对象需要多次拷贝的问题； C++11 中引入 auto 关键字，对初始化变量进行推导，并且引入 decltype 关键字，通过已有对象、变量获得类型； C++17 引入 optional 类型表达对象是否存在，并且引入 variant 作为类型安全的 union，类型表达更灵活。 C++20 中引入 concept 特性对类型在编译期做约束，增强类型的表达和检查能力。 在 C++ 中，容器和指针抽象后想要被正确且高效的使用，在工程中通常需要封装一组数据及函数来访问和操作。举例来说，指针是通用和有效抽象的机器地址，但正确使用指针来表示资源的所有权是非常困难的，因此标准库提供了智能指针类管理资源和生命周期。指针的更泛化的概念是，任何允许我们引用对象并根据其类型访问。 指针和引用指针是从 C 语言中延续下来的概念，而引用是在指针基础上结合资源管理进一步在编译器层做的管理手段，其本质在汇编层面并无区别。但是在使用时，需要注意以下几点： 指针是一个变量，它保存了另一个变量的内存地址；引用是另一个变量的别名，与原变量共享内存地址； 指针可以被重新赋值，指向不同的变量；引用在初始化后不能更改，始终指向同一个变量； 指针可以为 nullptr，表示不指向任何变量；引用必须绑定到一个变量，不能为 nullptr； 指针需要对其进行解引用以获取或修改其指向的变量的值；引用可以直接使用，无需解引用。 1234567891011121314151617181920212223#include &lt;iostream&gt;int main() { int a = 10; int b = 20; // 指针 int *p = &amp;a; std::cout &lt;&lt; &quot;Pointer value: &quot; &lt;&lt; *p &lt;&lt; std::endl; // 输出：Pointer value: 10 std::cout &lt;&lt; &quot;Address of value: &quot; &lt;&lt; p &lt;&lt; std::endl; // a的地址 std::cout &lt;&lt; &quot;Address of p: &quot; &lt;&lt; &amp;p &lt;&lt; std::endl; // p的地址 p = &amp;b; std::cout &lt;&lt; &quot;Pointer value: &quot; &lt;&lt; *p &lt;&lt; std::endl; // 输出：Pointer value: 20 // 引用 int &amp;r = a; std::cout &lt;&lt; &quot;Reference value: &quot; &lt;&lt; r &lt;&lt; std::endl; // 输出：Reference value: 10 std::cout &lt;&lt; &quot;Address of a: &quot; &lt;&lt; &amp;a &lt;&lt; std::endl; std::cout &lt;&lt; &quot;Address of r: &quot; &lt;&lt; &amp;r &lt;&lt; std::endl; // r = &amp;b; // 错误：引用不能被重新绑定 int &amp;r2 = b; r = r2; // 将 b 的值赋给 a，r 仍然引用 a std::cout &lt;&lt; &quot;Reference value: &quot; &lt;&lt; r &lt;&lt; std::endl; // 输出：Reference value: 20 return 0;} 引用是 C++ 编译器的约定，那和指针到底有什么区别？ 答：本质上没有区别 堆栈变量生命周期管理在 C++ 中创建变量一般有两种形式： 12A a;A a = new A(); // delete a; 1、静态建立类对象：编译器为对象在栈空间中分配内存，是通过直接计算 A 的空间，移动栈顶指针，挪出适当的空间，然后在这片内存空间上直接调用构造函数形成一个栈对象，在局部作用域退出时，自动调用类的析构函数； 2、动态建立类对象，使用 new 运算符将对象建立在堆空间。先在堆内存中搜索合适的内存并进行分配，再是调用构造函数构造对象，初始化堆内存，生命周期需要手动管理（delete）。 RAIIRAII 是 Resource Acquisition Is Initialization 的简称，其翻译过来就是“资源获取即初始化”，即在构造函数中申请分配资源，在析构函数中释放资源，它是 C++ 语言中的一种管理资源、避免泄漏的良好的设计方法。 C++ 语言的机制保证了，当创建一个类对象时，会自动调用构造函数，当对象超出作用域时会自动调用析构函数。RAII 正是利用这种机制，利用类来管理资源，将资源与类对象的生命周期绑定，即在对象创建时获取对应的资源，在对象生命周期内控制对资源的访问，使之始终保持有效，最后在对象析构时，释放所获取的资源。 12345678std::mutex mut;int write(std::string content) { mut.lock(); // critical area below (might throw exception) // Writing content to a file descriptor... // Critical areas above mut.unlock();} 上述展示了一个写函数，而在多线程并行调用下，内部可能共用的文件描述符必须用一个互斥锁保护，否则不同线程的字符串会混在一起。这段代码看起来没有问题，但是如果当写线程抛出了异常，调用栈会被直接释放，unlock 方法永远不会执行，造成死锁，其他的线程再也拿不到资源。 1234567std::mutex mut;int write(std::string content) { std::lock_guard&lt;std::mutex&gt; lock(mut); // critical area below (might throw exception) // Writing content to a file descriptor... // Critical areas above} lock_guard 保证在函数返回之后或者异常后释放互斥锁，因此无需担心异常情况下手动释放锁的问题。而 lock_guard 的实现，就是使用了“资源在构造函数中定义，在析构中释放的原则”。 123456789101112template &lt;typename T&gt;class lock_guard { public: explicit lock_guard(T &amp;mutex) : _mutex(mutex) { _mutex.lock(); } ~lock_guard() { _mutex.unlock(); } private: T _mutex;}; lock_guard 在构造函数中锁住了引用传入的资源（mutex），并且在析构函数中释放锁。其异常安全的保障就是析构函数一定会在对象归属的 scope 退出时自动被调用。 拷贝赋值&amp;构造拷贝构造函数是一种特殊构造函数，具有单个形参，该形参（常用 const 修饰）是对该类类型的引用。 拷贝赋值函数，也叫赋值操作符重载，因为赋值必须作为类成员，那么它的第一个操作数隐式绑定到 this 指针，赋值操作符接受单个形参，且该形参是同一类类型的对象。右操作数一般作为 const 引用传递。 问：为啥拷贝构造函数的形参必须是引用？ 1234567891011121314151617class Person { public: Person() {} // 默认构造函数 ~Person() {} // 析构函数 Person(const Person&amp; p) { // 拷贝构造函数 cout &lt;&lt; &quot;Copy Constructor&quot; &lt;&lt; endl; } // Persion(const Person&amp; p) = delete; Person&amp; operator=(const Person&amp; p) { // 拷贝赋值函数 cout &lt;&lt; &quot;Assign&quot; &lt;&lt; endl; return *this; } // Person&amp; operator=(const Person&amp; p) = delete; private: int age; string name;}; 拷贝构造函数和赋值运算符的行为比较相似，都是将一个对象的值复制给另一个对象，但是其结果却有些不同。拷贝构造函数使用传入对象的值生成一个新的对象的实例，而赋值运算符是将对象的值复制给一个已经存在的实例。具体来说，拷贝构造函数是构造函数，其功能就是创建一个新的对象实例；赋值运算符是执行某种运算，将一个对象的值复制给另一个对象（已经存在的）。 调用拷贝构造函数主要有以下场景： 对象作为函数的参数，以值传递的方式传给函数； 对象作为函数的返回值，以值的方式从函数返回； 使用一个对象给另一个对象初始化。 深拷贝&amp;浅拷贝 拷贝构造函数和赋值函数并非每个对象都会使用，另外如果不主动编写的话，编译器将以“位拷贝”的方式自动生成缺省的函数。在类的设计当中，“位拷贝”是应当防止的，可以通过“= delete”删除拷贝构造和拷贝复制函数。倘若类中含有指针变量, 并且这两个指针指向重叠的位置，那么这两个缺省的函数就会发生错误。 深拷贝和浅拷贝区分主要是针对类中的指针成员变量，因为对于指针只是简单的值复制并不能分割开两个对象的关联，任何一个对象对该指针的操作都会影响到另一个对象。这时候就需要提供自定义的深拷贝的拷贝构造函数，消除这种影响。通常的原则是： 指针成员应该提供自定义的拷贝构造函数； 在提供拷贝构造函数的同时实现自定义的赋值运算符。 对于拷贝构造函数的实现要确保以下几点： 对于值类型的成员进行值复制； 对于指针和动态分配的空间，在拷贝中应重新分配分配； 对于基类，要调用基类合适的拷贝方法，完成基类的拷贝。 移动赋值&amp;构造（见下文移动语义） 智能指针标准库中指针在经过多年发展后，不仅有耳熟能详的 share_ptr 和 unique_ptr 还多了一些其他的类型，为了对比，我这里也把普通指针和引用放在一起： T*: 内置指针类型，指向类型 T 的对象，或者指向类型 T 的连续内存空间序列； T&amp;：内置引用类型，引用类型 T 的对象（为别名），是一个隐式解引用的指针； unique_ptr：拥有 T 的所有权，指向 T 的独占指针，在离开作用域时，unique_ptr 的析构会销毁其指向的对象； shared_ptr：指向类型 T 对象的共享指针，所有权在所有指向类型 T 对象的 shared_ptr 之间共享，最后一个共享 T 对象的 shared_ptr 离开最后的作用域时负责析构销毁 T 对象； weak_ptr：指向 shared_ptr 拥有的对象，但是不引用计数，需要用 weak 访问对象时，需要提升为 shared_ptr 才可以； span：指向连续序列的 T 元素的指针，为 std::vector 等容器的“视图”； string_view：指向字符串子串的视图； X_iterator: 来自 C 容器的对象序列，“X”代表具体的迭代器类型（map、set…）。 unique_ptrC++ 智能指针详解（一）——unique_ptr shared_ptrC++ 智能指针详解（二）——shared_ptr 与 weak_ptr 值类型在 C++11 之前，很多 C++ 程序里存在大量的临时对象，又称无名对象。主要出现在如下场景： 函数的返回值 用户自定义类型经过一些计算后产生的临时对象 值传递的形参 C++11 之后，左值和右值分为了三个具体的子值类型，和两个混合类型，这里清晰起见，不对泛左值和右值展开。 左值具有以下特征： 可通过取地址运算符获取其地址； 可修改； 可用作内建赋值和内建符合赋值运算符的左操作数； 可以用来初始化左值引用。 举例： 12345678910111213int a = 1; // a是左值T&amp; f(); // 左值f();//左值++a;//左值--a;//左值int b = a;//a和b都是左值struct S* ptr = &amp;obj; // ptr为左值arr[1] = 2; // 左值int *p = &amp;a; // p为左值*p = 10; // *p为左值class MyClass{};MyClass c; // c为左值&quot;abc&quot; // 左值 将亡值可以理解为通过“盗取”其他变量内存空间的方式获取到的值。在确保其他变量不再被使用、或即将被销毁时，通过“盗取”的方式可以避免内存空间的释放和分配，能够延长变量值的生命期。 将亡值只能通过两种方式来获得，这两种方式都涉及到将一个左值赋给(转化为)一个右值引用： 返回右值引用的函数的调用表达式，static_cast&lt;T&amp;&amp;&gt;(T); 转换为右值引用的转换函数的调用表达式，std::move(t)。 12345std::string fun() { std::string str = &quot;test&quot;; return str;}std::string s = fun(); C++11 之前，s = fun()会调用拷贝构造函数，会将整个 str 复制一份，然后再把 str 销毁。str 比较大则会造成巨大开销。一旦 str 被 s 复制后，将被销毁，无法获取和修改。 C++11 后，引入了 move 语义，编译器会将这部分优化成 move 操作，str 会被进行隐式右值转换，等价于 static_caststd::string&amp;&amp;(str)，进而此处的 s 会将 func 局部返回的值进行移动。 纯右值本身就是纯粹的字面值，如 3，false，12.13，或者求值结果相当于字面值或是一个不具名的临时对象。 具体来说： 纯右值不具有多态：它所标识的对象的动态类型始终是该表达式的类型； 纯右值不能具有不完整类型； 纯右值不能具有抽象类类型或它的数组类型。 左值引用和右值引用12345int a = 1;int&amp; b = a; // before c++11// int&amp;&amp; b = a; // error 不能引用左值int&amp;&amp; b = std::move(a); // c++11 左值引用使用“&amp;”标记，右值引用使用“&amp;&amp;”标记。具体来说，在 C++11 之前的引用，都是左值引用，而： 在语句执行完毕之后被销毁的临时对象； std::move()后的非 const 对象 考虑下列代码，应该使用哪个 foo 函数的重载版本： 12345void foo(int&amp;); // 1void foo(int&amp;&amp;); // 2int&amp;&amp; value = 42;foo(value); 答案是使用“1”的函数。虽然这里的变量定义的是右值引用类型，然后 foo(value)中的表达式 value 确是一个左值，而不是由定义 value 时的类型来决定值类型。简单来说，如果表达式能取地址，则为左值表达式，否则为右值表达式。 根据引用和常量性质进行组合，有几种情况： 左值引用 Value&amp;，只能绑定左值表达式，如 1 中形参； 右值引用 Value&amp;&amp;，只能绑定右值表达式，如 2 中形参； 左值常引用 const Value&amp;，可以绑定左、右值表达式，但是后续无法修改值； 右值常引用 const Value&amp;&amp;，只能绑定常量右值表达式，实际中不使用。 问：如何将上述代码中 value 以右值引用的形式传递给 foo，从而调用“2”的函数，两个方法：一是直接 foo(42)，二是通过 foo(static_cast&lt;int&amp;&amp;&gt;(value))。 编译器会匹配为右值引用。 让编译器将对象匹配为右值引用，是移动语义的基础！！！ 移动语义先来说明为什么需要移动语义： case1： 1234567891011class Stuff { public: Stuff(const std::string &amp;s) : str{s} {}; private: std::string str;};std::vector&lt;Stuff&gt; stuff;Stuff tmp{&quot;hello&quot;};stuff.push_back(tmp);stuff.push_back(tmp); 这是一个较为常见的开发 case，创建了一个容器 std::vector 以及自定义类 Stuff，并且添加到容器中两次，注意我们这里并没有对 Stuff 类自定义拷贝构造和拷贝赋值，使用编译器默认实现。tmp 添加到容器中两次，每次添加时都会发生一次拷贝操作，最终内存结构可能是： tmp 对象在添加到容器中两次后，生命周期随之结束。 case2： 123456789101112std::string process1(const std::string&amp; str) { return process2(str);}std::string process2(const std::string&amp; str) { return process3(str);}std::string process3(const std::string&amp; str) { return process4(str);}// ... 回到 case1，现在修改容器的操作为下面的代码： 1234std::vector&lt;Stuff&gt; stuff;Stuff tmp{ &quot;hello&quot; };stuff.push_back(tmp); stuff.push_back(std::move(tmp); 现在可以明确，移动操作执行对象数据转移，拷贝操作复制对象数据。为了能够将拷贝操作与移动操作区分执行，需要不同于拷贝的标记，“&amp;&amp;”应运而生。 在不考虑模板的情况下，针对两种 push_back 函数需要有两种重载实现： 12345678class vector { public: // 上文说到const T&amp; 也是可以绑定右值引用， // 但是因为有限制，只能后续无法修改， // 所以提供也需要提供&amp;&amp;的实现 void push_back(const Stuff&amp; value){} void push_back(Stuff&amp;&amp; value) {}}; 通过传递左值引用或右值引用，根据需要调用不同的 push_back 重载函数。","link":"/2024/03/10/Modern-cpp-1/"},{"title":"NVIDIA PTX 简单入门","text":"先看代码： 123456789101112131415161718192021222324target datalayout = &quot;e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64&quot;target triple = &quot;nvptx64-nvidia-cuda&quot;declare i32 @llvm.nvvm.read.ptx.sreg.tid.x() nounwind readnonedefine void @kernel(ptr addrspace(1) %A, ptr addrspace(1) %B, ptr addrspace(1) %C) {entry: %id = call i32 @llvm.nvvm.read.ptx.sreg.tid.x() %ptrA = getelementptr inbounds float, ptr addrspace(1) %A, i32 %id %ptrB = getelementptr inbounds float, ptr addrspace(1) %B, i32 %id %ptrC = getelementptr inbounds float, ptr addrspace(1) %C, i32 %id %valA = load float, ptr addrspace(1) %ptrA, align 4 %valB = load float, ptr addrspace(1) %ptrB, align 4 %valC = fadd float %valA, %valB store float %valC, ptr addrspace(1) %ptrC, align 4 ret void}!nvvm.annotations = !{!0}!0 = !{ptr @kernel, !&quot;kernel&quot;, i32 1} 这个kernel.ll文件是LLVM IR（Intermediate Representation）表示的CUDA核函数，专为NVIDIA PTX（Parallel Thread Execution）架构生成。以下是逐行解释： 逐行解释 目标架构定义 1target datalayout = &quot;e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64&quot; 作用：定义数据的内存布局规则。 关键参数： e: 小端字节序 p:64:64:64: 指针占64位，对齐为64位 i1:8:8: 1位整数对齐为8位 f32:32:32: 单精度浮点数对齐为32位 v16:16:16: 16位向量对齐为16位 n16:32:64: 本地指针大小（用于GPU架构） 1target triple = &quot;nvptx64-nvidia-cuda&quot; 作用：指定目标架构为NVIDIA PTX 64位架构，用于CUDA设备。 内置函数声明 1declare i32 @llvm.nvvm.read.ptx.sreg.tid.x() nounwind readnone 作用：声明PTX内置函数，用于读取线程在X维度的ID（类似CUDA的threadIdx.x）。 特性： nounwind: 保证不会抛出异常; readnone: 函数无副作用，输出仅依赖输入; 核函数定义 123define void @kernel(ptr addrspace(1) %A, ptr addrspace(1) %B, ptr addrspace(1) %C) 作用：定义名为kernel的核函数，接受三个全局内存指针参数。 关键点： addrspace(1): 表示指针指向全局内存（GPU显存） 对比CUDA中的参数类型：float* A → ptr addrspace(1) %A 入口基本块（Entry Block） 12entry: %id = call i32 @llvm.nvvm.read.ptx.sreg.tid.x() 作用：获取当前线程在X维度的ID，存入%id寄存器。 123%ptrA = getelementptr inbounds float, ptr addrspace(1) %A, i32 %id%ptrB = getelementptr inbounds float, ptr addrspace(1) %B, i32 %id%ptrC = getelementptr inbounds float, ptr addrspace(1) %C, i32 %id 作用：计算每个线程访问的全局内存地址。 语法： getelementptr inbounds: 计算数组元素地址（类似&amp;A[threadIdx.x]） float: 元素类型 ptr addrspace(1) %A: 基地址 i32 %id: 偏移量 12%valA = load float, ptr addrspace(1) %ptrA, align 4%valB = load float, ptr addrspace(1) %ptrB, align 4 作用：从全局内存加载数据到寄存器。 参数： align 4: 确保4字节对齐访问（优化内存访问） 1%valC = fadd float %valA, %valB 作用：执行浮点数加法运算。 12store float %valC, ptr addrspace(1) %ptrC, align 4ret void 作用：将结果写回全局内存，然后返回。 关键点： store指令的内存地址必须指定正确的地址空间（addrspace(1)） 元数据注解 12!nvvm.annotations = !{!0}!0 = !{ptr @kernel, !&quot;kernel&quot;, i32 1} 作用：标记@kernel函数为CUDA核函数。 参数： ptr @kernel: 函数指针; !”kernel”: 标记类型为核函数; i32 1: 参数（通常表示核函数版本或特性）; 这个核函数的行为等价于以下CUDA代码： 1234__global__ void kernel(float*A, float* B, float* C) { int id = threadIdx.x; C[id] = A[id] + B[id];} 每个线程负责将全局内存中A和B对应位置的值相加，结果写入C的相同位置。 关键概念补充 地址空间： addrspace(1): 全局内存（显存） addrspace(3): 共享内存 addrspace(5): 常量内存 PTX寄存器访问： tid.x对应threadIdx.x 类似还有ctaid.x（blockIdx.x）、ntid.x（blockDim.x）等 LLVM IR特性： 静态单赋值（SSA）形式 强类型系统 显式内存地址空间管理 再看用法将上文的LLVM IR代码编译为NVIDIA PTX 后端代码，命令为： 1llc-15 -march=nvptx64 -mcpu=sm_80 kernel.ll -o kernel.ptx llc-15作用：LLVM 静态编译器工具,用于将 LLVM IR 代码编译为目标平台的汇编代码或二进制格式。此处用于生成 NVIDIA PTX 代码。 -march=nvptx64作用：指定目标架构为 NVIDIA PTX 64 位，nvptx64 是 NVIDIA 的 Parallel Thread Execution (PTX) 虚拟指令集架构，专为 64 位 GPU 设计。PTX 代码可在支持该架构的 NVIDIA GPU 上运行（需进一步编译为实际 GPU 指令）。 -mcpu=sm_80作用：指定目标 GPU 的计算能力版本。 sm_80 对应 NVIDIA Ampere 架构（如 A100、RTX 3090 等）。 sm 表示 “Streaming Multiprocessor”，数字 80 代表计算能力 8.0。此选项确保生成的 PTX 代码针对该架构优化。 kernel.ll作用: 此文件通常由 Clang 或其他 LLVM 前端生成，包含高级语言（如 C/C++、CUDA 等）编译后的中间代码。 -o kernel.ptx作用：输出的 PTX 文件是 NVIDIA GPU 可读的中间代码，后续可通过 NVIDIA 驱动或工具（如 nvcc）进一步编译为实际 GPU 指令（SASS，见下文）。 PTX 生成代码123456789101112131415161718192021222324252627282930313233343536//// Generated by LLVM NVPTX Back-End//.version 7.0.target sm_80.address_size 64 // .globl kernel // -- Begin function kernel // @kernel.visible .entry kernel( .param .u64 kernel_param_0, .param .u64 kernel_param_1, .param .u64 kernel_param_2){ .reg .b32 %r&lt;2&gt;; .reg .f32 %f&lt;4&gt;; .reg .b64 %rd&lt;8&gt;;// %bb.0: // %entry ld.param.u64 %rd1, [kernel_param_0]; ld.param.u64 %rd2, [kernel_param_1]; mov.u32 %r1, %tid.x; ld.param.u64 %rd3, [kernel_param_2]; mul.wide.s32 %rd4, %r1, 4; add.s64 %rd5, %rd1, %rd4; add.s64 %rd6, %rd2, %rd4; add.s64 %rd7, %rd3, %rd4; ld.global.f32 %f1, [%rd5]; ld.global.f32 %f2, [%rd6]; add.rn.f32 %f3, %f1, %f2; st.global.f32 [%rd7], %f3; ret; // -- End function} SASSSASS（Streaming ASSembly）是 NVIDIA GPU 的实际底层机器码（二进制指令集），直接由 GPU 硬件执行。它是 PTX 代码经过进一步编译后的最终产物，与 PTX 的关系类似于 CPU 汇编代码和中间语言（如 Java 字节码）的关系。 SASS 的关键特性 二进制格式： SASS 是 GPU 硬件直接执行的二进制指令，不可读 PTX 是文本格式的中间代码，人类可读（但一般由编译器生成） 硬件绑定： SASS 直接对应具体 GPU 架构（如 Ampere、Ada Lovelace、Hopper）。 不同架构的 SASS 不兼容（例如 sm_80 的 SASS 无法在 sm_70 的 GPU 上运行）。 性能优化 SASS 经过 NVIDIA 驱动或工具（如 nvcc）的优化，包含特定 GPU 的指令调度、寄存器分配等。 PTX 是通用中间表示，需进一步编译为 SASS 才能高效执行。 隐蔽性： NVIDIA 未公开 SASS 的完整指令集和编码规范，普通开发者通常无需直接操作 SASS。 PTX 与 SASS 的编译流程1234567高级语言（如 CUDA C++） ↓ 编译（nvcc/clang）LLVM IR（.ll 文件） ↓ 编译（llc）PTX 代码（.ptx 文件） ↓ 运行时编译（NVIDIA 驱动）或离线编译（nvcc）SASS 机器码（二进制，GPU 直接执行） PTX 是跨 GPU 架构的中间层代码（类似虚拟指令集）; SASS 是最终在 GPU 上运行的机器码（绑定具体架构）; 如何查看 SASS 代码？ 使用 cuobjdump： 1cuobjdump -sass compiled_gpu_binary.cubin 输出 SASS 指令的文本表示（例如指令操作码、寄存器分配）。 使用 nvcc 生成： 1nvcc --keep --gpu-architecture=sm_80 -c code.cu 保留中间文件（如 .sass 或 .cubin）。 Nsight Compute： NVIDIA 官方工具，可分析 SASS 指令的执行效率和资源使用。 假设一段简单的加法操作，PTX 代码： 123456789101112.version 7.8.target sm_80.global .func(.param.b64 %out), add( .param.b64 %a, .param.b64 %b) { ld.param.u64 %r1, [%a]; ld.param.u64 %r2, [%b]; add.u64 %r3, %r1, %r2; st.param.b64 [%out], %r3; ret;} 对应的SASS 代码为： 12IADD R1, R2, R3;MOV R4, R1; PTX 怎么用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119#include &lt;iostream&gt;#include &lt;fstream&gt;#include &lt;cassert&gt;#include &lt;cuda.h&gt;void checkCudaErrors(CUresult err) { assert(err == CUDA_SUCCESS);}/// main - Program entry pointint main(int argc, char **argv) { CUdevice device; CUmodule cudaModule; CUcontext context; CUfunction function; CUlinkState linker; int devCount; // CUDA initialization checkCudaErrors(cuInit(0)); checkCudaErrors(cuDeviceGetCount(&amp;devCount)); checkCudaErrors(cuDeviceGet(&amp;device, 0)); char name[128]; checkCudaErrors(cuDeviceGetName(name, 128, device)); std::cout &lt;&lt; &quot;Using CUDA Device [0]: &quot; &lt;&lt; name &lt;&lt; &quot;\\n&quot;; int devMajor, devMinor; checkCudaErrors(cuDeviceComputeCapability(&amp;devMajor, &amp;devMinor, device)); std::cout &lt;&lt; &quot;Device Compute Capability: &quot; &lt;&lt; devMajor &lt;&lt; &quot;.&quot; &lt;&lt; devMinor &lt;&lt; &quot;\\n&quot;; if (devMajor &lt; 2) { std::cerr &lt;&lt; &quot;ERROR: Device 0 is not SM 2.0 or greater\\n&quot;; return 1; } std::ifstream t(&quot;kernel.ptx&quot;); if (!t.is_open()) { std::cerr &lt;&lt; &quot;kernel.ptx not found\\n&quot;; return 1; } std::string str((std::istreambuf_iterator&lt;char&gt;(t)), std::istreambuf_iterator&lt;char&gt;()); // Create driver context checkCudaErrors(cuCtxCreate(&amp;context, 0, device)); // Create module for object checkCudaErrors(cuModuleLoadDataEx(&amp;cudaModule, str.c_str(), 0, 0, 0)); // Get kernel function checkCudaErrors(cuModuleGetFunction(&amp;function, cudaModule, &quot;kernel&quot;)); // Device data CUdeviceptr devBufferA; CUdeviceptr devBufferB; CUdeviceptr devBufferC; checkCudaErrors(cuMemAlloc(&amp;devBufferA, sizeof(float)*16)); checkCudaErrors(cuMemAlloc(&amp;devBufferB, sizeof(float)*16)); checkCudaErrors(cuMemAlloc(&amp;devBufferC, sizeof(float)*16)); float* hostA = new float[16]; float* hostB = new float[16]; float* hostC = new float[16]; // Populate input for (unsigned i = 0; i != 16; ++i) { hostA[i] = (float)i; hostB[i] = (float)(2*i); hostC[i] = 0.0f; } checkCudaErrors(cuMemcpyHtoD(devBufferA, &amp;hostA[0], sizeof(float)*16)); checkCudaErrors(cuMemcpyHtoD(devBufferB, &amp;hostB[0], sizeof(float)*16)); unsigned blockSizeX = 16; unsigned blockSizeY = 1; unsigned blockSizeZ = 1; unsigned gridSizeX = 1; unsigned gridSizeY = 1; unsigned gridSizeZ = 1; // Kernel parameters void *KernelParams[] = { &amp;devBufferA, &amp;devBufferB, &amp;devBufferC }; std::cout &lt;&lt; &quot;Launching kernel\\n&quot;; // Kernel launch checkCudaErrors(cuLaunchKernel(function, gridSizeX, gridSizeY, gridSizeZ, blockSizeX, blockSizeY, blockSizeZ, 0, NULL, KernelParams, NULL)); // Retrieve device data checkCudaErrors(cuMemcpyDtoH(&amp;hostC[0], devBufferC, sizeof(float)*16)); std::cout &lt;&lt; &quot;Results:\\n&quot;; for (unsigned i = 0; i != 16; ++i) { std::cout &lt;&lt; hostA[i] &lt;&lt; &quot; + &quot; &lt;&lt; hostB[i] &lt;&lt; &quot; = &quot; &lt;&lt; hostC[i] &lt;&lt; &quot;\\n&quot;; } // Clean up after ourselves delete [] hostA; delete [] hostB; delete [] hostC; // Clean-up checkCudaErrors(cuMemFree(devBufferA)); checkCudaErrors(cuMemFree(devBufferB)); checkCudaErrors(cuMemFree(devBufferC)); checkCudaErrors(cuModuleUnload(cudaModule)); checkCudaErrors(cuCtxDestroy(context)); return 0;} 这里是代码的简单行为： 初始化 CUDA 环境： 检测 GPU 设备，验证计算能力； 加载 kernel.ptx 文件。 准备数据： 在 GPU 上分配内存（devBufferA, devBufferB, devBufferC）； 在 CPU 上初始化输入数据 hostA 和 hostB（值分别为 0, 1, 2, …, 15 和 0, 2, 4, …, 30）。 将数据从 CPU 拷贝到 GPU。 启动 GPU 核函数： 调用 kernel 函数，执行 A + B = C 的加法操作； 核函数使用 1 个线程块，包含 16 个线程（每个线程处理一个元素）。 验证结果： 将 GPU 计算结果 devBufferC 拷贝回 CPU 的 hostC； 打印 hostA[i] + hostB[i] = hostC[i]，验证结果是否正确。 清理资源 相关输出为： 1234567891011121314151617181920Using CUDA Device [0]: NVIDIA H100 80GB HBM3Device Compute Capability: 9.0Launching kernelResults:0 + 0 = 01 + 2 = 32 + 4 = 63 + 6 = 94 + 8 = 125 + 10 = 156 + 12 = 187 + 14 = 218 + 16 = 249 + 18 = 2710 + 20 = 3011 + 22 = 3312 + 24 = 3613 + 26 = 3914 + 28 = 4215 + 30 = 45","link":"/2025/02/01/PTX-1/"},{"title":"数据库事务模型分析（1）—— 计算模型","text":"事务服务的计算模型可以通过多种方法精确创建，和抽象程度有关，而建模数据库事务服务的系统需要遵循一些基本的方法论以及相关实际因素，本系列所作的建模一般分为以下5个步骤： 首先定义数据对象的基础操作，这些操作被认为是原子的，并且独立于其他操作，这一步并没有明确定义什么是数据对象以及需要考虑什么类型数据的基础操作。因此本系列定义两种不同的计算模型：页模型和对象模型； 对事务以及事务执行顺序建模，将其视为在数据对象上基础操作的一个（全/偏）序列； 本系列使用调度或者历史的概念作为对事务执行并发的抽象； 在所有句法正确的调度中，必须从中识别出保证ACID“正确”的调度； 需要“算法”或者“协议”来即时地创建事务正确的调度，当事务提交之后，可以被动态执行； 上述的“页模型”是一个非常简单的模型，可以观察在存储层中，数据页是如何被事务优雅的读写的。接下来可以看到这个模型以一种非常优雅的方式建模了并发控制和事务恢复的实现，并且可以描述非常多（但不是全部）系统实现中的重要语义。“对象模型”是预留给数据库的上层操作，比如访问层或者查询处理层的操作，很容易想到把上层数据应用的业务数据对象考虑在内，可以推导出更复杂的对抽象数据结构ADT的语义，借由ADT来执行下层数据对象的操作。 页模型定义页模型基于一个基本假设： 所有对数据的高层操作都会转化为对数据页的读写操作，并且假设每个页的操作都是不可分的，无论是内存还是在硬盘上。 在用页模型给出事务的一般定义之前，先看一个简化的版本，把事务看作具有全序关系的一组操作步骤，而一般页模型允许偏序关系：一个事务$t$(在页模型中)是形如$r(x)$或者$w(x)$的操作步骤的有限序列，记作： $$t = p_1…p_n$$其中，$n &lt; \\infty$, 对于$1 \\leq i \\leq n$，$x \\in D$，有$p_i \\in {r(x), w(x)}$，这样就从事务执行的细节中抽象出事务执行的读写序列了。 在同一个事务中，操作步骤由$p_i$表示，即事务中第$i$步操作，存在多个事务的情况下，可以给每个事务添加一个唯一的事务id，作为步骤的另外一个下标，$p_{ij}$就代表了第$i$个事务的第$j$步，但是在页模型中为了简化问题，可以简单的从上下文推断出第几步，因此不显式给出步数。只考虑一个事务，可以表示为：$$p_j = r/w(x)$$ 理解为事务第$j$步读/写页数据。实际上，无需把一个事务的所有步骤排成一个严格先后执行的序列，可以放松对事务执行步骤全序的要求，接下来定义事务的偏序：设$A$为任意一个集合，$R \\subseteq A \\times A$，如果对于任意的元素$a, b, c \\in A$，下面三个关系都满足，则称关系$R$是集合$A$上的一个偏序： $(a, a) \\in R$ 自反性 $(a, b) \\in R \\wedge (b,a) \\in R \\Rightarrow a = b$ 反对称性 $(a, b) \\in R \\wedge (b,c) \\in R \\Rightarrow (a,c) \\in R$ 传递性 偏序是全序的一个特例，全序在偏序的定义上又增强了约束，即对于任意两个不同的元素$a, b \\in A$，或者$(a,b) \\in R$或者$(b,a) \\in R$，也就是说对两个不同的数据操作在事务集合中的顺序满足唯一约束。 下面根据偏序定义可以给出在页模型下的事务定义：一个事务$t$是具体有$r(x)$或$w(x)$形式的多个操作步骤的偏序集合，其中$x \\in D$，对同一个页数据的读写操作和写写操作是有序的，正式的，事务可以表示为一个二元组：$$t=(op, &lt;)$$ 其中，$op$是具有$r(x)$或者$w(x)$的有限集合，$x \\in D$且$x&lt; \\subseteq op \\times op$是在$op$集合上的偏序关系，如果${p,q} \\subseteq op$且$p, q$都对同一个页数据项进行访问，且$p,q$是少有一个是写操作，则有$p &lt; q \\wedge q &lt; p$。 根据页事务模型的数学定义可推理出，偏序关系下，不允许同一数据项的读写操作或者写写操作乱序。 对象模型对象模型可以认为是页模型的替代或者推广，实际上，页模型是隐含在对象模型中。对象模型提供了一个表达在任意类型对象上进行的操作框架，而处于提高性能的需要，可能还需要利用被调用操作的语义。实际上，在一个对象及其操作的实现过程中，是需要调用底层对象的操作。 例如在数据库访问层中，比如索引查找，需要调用存储层面向页的操作，类似的调用层次还在业务对象集合中。上图中描述了一个事务，标记为$t_1$，执行了一个SQL的Select语句，从数据库中获取所有名叫“tom”的人员信息，在获取结果后，执行一个SQL语句插入一个新信息。 SQL语句在SQL引擎侧已经被转化为查询处理层的操作。在这个例子中，假设包含两个记录$x$和$y$，查询操作调用存储层的读写页操作。这里假设要读B+树的根节点，并且简单起见，只有两层。叶子节点标记为$l$，其中包含所有符合“tom”的列表，通过主键获取满足条件的记录$x$和$y$，访问每个记录各需要一次页读取，页分别记为$p$和$q$，最后还需要调用一个SQL的Insert语句插入新的信息：首先读取存储层的元数据页，找到有足够空闲的页$p$，读取页$p$，把要插入的记录写入，然后写回该页，最后还要在索引上把新记录的主键加入到B+树上名字叫“tom”的列表中。 上述例子中，实际上是把事务看作一棵树，而被调用的操作作为树节点，为了保证事务树是独立的并且允许严格推理，要求事务树的叶节点必须是页模型的基本读写操作。如果一个给定事务树的叶节点全部或者部分与页读写不对应，唯一能做的就是扩展这棵树把叶子节点全部变成页读写操作。 而这种扩展是动态的，跟踪在执行过程中一个高级事务中的所有低级数据操作，而不是从操作的静态结构生成的一个调用层次图。而跟踪一个事务内部操作的顺序，一种简单的方法就是给所有的叶节点确定一个顺序，和上述图中从左到右的事务顺序一致。所以像页模型一样，也同样要求事务树的所有叶节点保持偏序关系。 现在可以正式给出对象模型事务的定义：一个事务$t$是一棵有限树，节点进行如下标记： 事务标识符来标记根节点； 被调用操作的名称和参数标记内部非叶非根节点； 页模型的读写操作标记叶子节点；在叶子节点上定义了偏序关系“$&lt;$”，使得所有的叶子节点操作$p,q$，其中$p$形如$w(x)$, q形如$r(x) \\wedge w(x)$，或相反，都满足$p &lt; q \\vee q &lt; p$。 对象模型中事务对应的树不一定是平衡的，也就是说从根节点到所有叶子节点的路径不一定是等长的，而为了清晰起见，讨论问题时把属于同样对象操作类型或者接口的操作放在事务树的同层，所以当后续利用对象模型考虑并发执行时，经常放在完全平衡的事务树中。 这种事务树所有叶节点到根节点的路径是等长的，成为分层事务或者多级事务。 在上述的讨论中，有一个问题被忽略了，偏序只在树的叶子节点被提及和定义。内部节点的顺序实际上在叶子节点被约束后已经被隐式定义了。例如，对两个内部节点操作$a$和$b$，如果在叶子节点偏序“&lt;”下，$a$节点下的所有后代叶子节点先于$b$节点下的所有后代叶子节点处理，则称$a$先于$b$，这需要后代叶子节点集合之间有序，否则称$a$和$b$是并发操作。如果想找到中间节点对存储引擎的数据影响，可以检查其子女节点以及更远后代之间的交叉情况，直到叶子节点，其中的偏序关系保证了数据解释的明确性。 最后简要的阐述后续系列如何使用引入的对象模型作为高级并发控制算法的基础来结束本节。当考虑多个事务以并发或者并行的方式执行时，要把所有的事务树组合，形成一个操作的森林，然后检查叶子节点之间的偏序关系，以及隐含推导出来的高层操作顺序。与单层事务不同的是，需要在所有事务树的所有叶子节点集合上来定义偏序关系。而由于内部节点之间的顺序可以由叶子节点的偏序关系推导，可以研究高层节点之间的并发或者并行。而增强事务性能的关键，是高层内部节点操作的语义特性纳入事务调度的考虑范围。","link":"/2024/05/09/Transaction-1/"},{"title":"数据库事务模型分析（2）—— 等价关系","text":"在本节中，基于上一节的页模型，提出并发执行正确性的概念，并建立多个事务在时间上交叉执行的模型，提出有关并发事务调度的概念。首先要明确的是，事务的执行是高度动态的，完全建模会带来巨大的理解成本，实际上，当事务到来，并发控制需要对该事务做出执行决定，并使能其和系统中正在运行的事务正确同步。其次，需要考虑执行失败而被终止的事务，在正常执行的路径中，包含提交终止。这些操作的处理方式和直觉很不一样。 并发执行的事务之间的数据访问操作存在潜在的冲突，比如“修改后丢失”、“读不一致”、“脏读”等问题，作为事务执行顺序的保证来说，这些异常都是要避免的，给外部执行一致的印象。 为了判断事务执行时，何种情况是期望的，何种情况是应该避免的，调度器需要在线的应用这些规则。首先，本文假定调度中包含了事务结束标志成功或者不成功，具体的，一个事务成功的结束使用$c$表示，即事务在没有被中断的情况下完成了内部所有的数据操作，而失败事务的结束使用$a$表示，一个被终止的事务不应该对底层数据产生任何影响，这由恢复过程保证。 调度和历史设$T={t_1,…,t_n}$是一个有限事务集合，对每一个$t_i \\in T$，都有$t_i=(op_i,&lt;_i)$，其中$op_i$是$t_i$的操作集合，$&lt;_i$表示操作的顺序，$i \\in [1, n]$。$T$中的一个历史$s=(op(s),&lt;_s)$定义如下： $op(s) \\subseteq \\bigcup_{i=1}^{n}op_i \\bigcup_{i=1}^{n}{a_i,c_i}$，并且$\\bigcup_{i=1}^{n}op_i \\subseteq op(s)$，即事务的历史$s$是由事务所有操作的并集和每个事务的终止操作组成； $ \\forall i \\in [1,n], c_i \\in op(s) \\Leftrightarrow a_i \\notin op(s)$，也即每个事务都有结束，成功或者失败，但不能同时存在； $ \\bigcup_{i=1}^{n}&lt;_i \\subseteq &lt;_s$，也即所有事务的顺序都包含在由$s$给出的偏序中； $ \\forall i \\in [1,n], \\forall p \\in op_i, p &lt;_s a_i | p &lt;_s c_i$，也即成功或者失败总是作为事务最后一步出现； 对每一个来自不同事务的一对操作$p,q \\in op(s)$，如果访问同一个数据页并且其中至少一个为写操作，满足$p&lt;_s q|q &lt;_s p$。 因此一个历史，或者满足偏序的事务来说，必须：包含所有事务的所有操作（1），使每个事务都包含唯一的结束符（2），保持每个事务中操作的顺序（3），以结束符作为每个事务的最后一步（4），安排冲突操作的顺序（5）。由于（1）和（2），字面上历史也被称作完整调度。 同样的，也可以给出串行历史的调度：一个事务序列历史$s$为串行的，当且仅当在$s$中对历史中的任意两个事务$t_i$和$t_j, i \\neq j$，$t_j$的所有操作都在$t_i$之后，或者相反。 调度正确性判定准则这一部分的目标是给出调度的正确性准则，如果$S$是所有调度的集合，正确性准则可以在形式上看作一个映射：$$\\sigma.S \\rightarrow {0,1}$$这个映射对每个调度$s \\in S$返回一个布尔值，因此，对于调度$s \\in S$，如果$s$是正确的，那么$\\sigma(s)=1$，也即：$$correct(S):={ s \\in S | \\sigma(s) = 1}$$ 针对一个调度，判断其正确性的准则至少需要满足以下要求： $S$ 中至少存在某些正确的调度； $s \\in correct(S)$是可以有效判定的，不存在停机问题； 对于给定的事务集合，可以找到调度最优解。 本系列中要保持底层数据的完整性，并且每个事务都能保持数据的完整性。而串行事务执行总是正确的，在合适的选择事务调度的等价关系后，使用串行历史作为正确性的度量标准，因此可以给出在所有调度的集合$S$上的等价关系：$$([S]\\approx) ={[s] \\approx | s \\in S }$$其中，$[S]\\approx$是通过$\\approx$得到的所有等价类的集合，显然，一个调度集合中的正确调度都是两两等价的，因此任意一个调度$s$都可以代表这个类。而对可以选出串行调度的调度集合，这种调度集合中的调度被成为可串行化的。因此在接下来的分析中，本文会做以下两件事： 定义调度的等价概念； 通过和串行历史的等价定义“可串行化”。 调度Herbrand等价语义接下来本文将通过一种语义的概念在调度之间建立等价关系。精确地描述未知的事务语义是困难的，因此为了明确事务调度在事务上下文的语义，先明确在调度中出现的操作的语义，然后再定义事务调度本身。 本文在考虑调度等价的问题时，先对事务失败回滚的情况做出忽略，稍后在后面的章节对该问题进行展开。 对一个任意的调度$s$，有如下假设： 一个事务$t_i \\in trans(s)$的一个操作$r_i(x) \\in s$读取在$r_i(x)$之前出现的最后一个$w_j(x), j \\neq i$写入的值； 一个操作$w_j(x) \\in s$写入的新值潜在依赖于$t_i$中$w_j(x)$之前的属于$active(s) \\cup committed(s)$的事务中读取的所有数据的值。 对于第一个假设，可能存在一个问题是，并不是调度中每个读操作前都有一个写操作，比如：$$s=r_1(x)r_2(x)w_1(x)r_2(x)…$$为了统一不同的情况，假设在每个调度的头部都有一个假想的初始事务$t_0$，$t_0$写入调度中提及的所有数据项后提交，这种情况下，上面提到的调度变成：$$s=w_0(x)w_0(y)c_0r_1(x)r_2(x)w_1(x)r_2(x)…$$简单来说，初始事务为一个给定的调度定义了初始状态。 设$s$是一个调度，操作$r_i(x), w_i(x) \\in op(x)$的Herbrand语义$H_s$如下递归定义： $H_s(r_i(x)) := H_s(w_j(x))$, 其中$w_j(x),j \\neq i$，是$s$中在$r_i(x)$之前的对$x$的最后一个写操作； $H_s(w_i(x)) := f_{ix}(H_s(r_i(y_1)),…,H_s(r_i(y_m)))$，其中$r_i(y_j), 1 \\leq j \\leq m$，是事务$t_i$在调度$s$里发生在$w_i(x)$之前所有读操作，而$f_{ix}$是一个未知m元函数。假设在每一个事务中，对每一个数据项至多只有一个写操作，易验证对每个操作$p \\in op(s)$，$H_s(p)$的定义都是合理的。其次，初始事务$t_0$对数据的写操作和一个0元函数$f_{0x}$联系起来。 而在此基础上可以进一步推广“事务的Herbrand空间”，也即Herbrand全域。设$D={x,y,z…}$是一个数据的有限集合，对于一个事务$t$，设$op(t)$是事务$t$对数据的操作集合，事务$t_i(i&gt;0)$的Herbrand空间$HU$是满足下面条件的最小符合集合： 对每个$x \\in D$，有$f_{0x}() \\in HU$； 若$w_j(x) \\in op(t_i), |{r_i(y)|(y \\in D)r_i(y) &lt; t_i w_i(x)}|=m$，并且如果$v_1,…,v_m \\ in HU$，那么$f_{ix}(v_1,…v_m) \\in HU$。 因此调度中对数据操作的语义范围是Herbrand空间值的集合，但Herbrand空间纯粹是基于数学符号的语义构造，没有真实事务读写真实数据的任何信息。 最后根据Herbrand全域定义可以推出调度的Herbrand语义，所以调度$s$的语义是映射：$$H[s]:D \\rightarrow HU$$进一步的，$$H[s] (s) := H_s(w_i(s))$$其中对于每个$x \\in D$，$w_i(x)$是$s$对$x$的最后一个写操作。也就是说，调度$s$的Herbrand语义是$s$中最终写入值得集合。而再次重申暂时不对异常终止做考虑。上述得定义虽然普通，但有趣得是，能适用于任意复杂具体事务的解释，但同样的，Herbrand一般性的代价是具体处理起来异常复杂。有了以上定义，可以依据此定义不同可串行的事务调度形式，或者说定义了和串行调度等价的联系，而从根本上说，这样的等价关系定义只对历史（也就是完整调度）有意义，后面本系列会讨论如何定义任意调度的正确性问题，在这种情况下，调度也不必是历史。","link":"/2024/05/12/Transaction-2/"}],"tags":[{"name":"debug","slug":"debug","link":"/tags/debug/"},{"name":"DuckDB","slug":"DuckDB","link":"/tags/DuckDB/"},{"name":"Performance","slug":"Performance","link":"/tags/Performance/"},{"name":"cpp2x","slug":"cpp2x","link":"/tags/cpp2x/"},{"name":"CUDA, PTX","slug":"CUDA-PTX","link":"/tags/CUDA-PTX/"},{"name":"transaction","slug":"transaction","link":"/tags/transaction/"}],"categories":[],"pages":[{"title":"Whoami","text":"Hi 👋 My name is HuanbingDistributed System &amp; Database Dev 🌍 I’m based in Shanghai 🖥️ See my portfolio at Blog ✉️ You can contact me at luhuanbing084@gmail.com 🚀 I’m currently working on TemplateDB 🧠 I’m learning TLA+ &amp; TSDB 🤝 I’m open to collaborating on TiKV &amp; Databend &amp; RisingWave ⚡ I am Iron man Skills Socials","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}]}