{"posts":[{"title":"C++调试（1）—— 认识Dwarf格式","text":"DWARF全称为“Debugging With Attributed Record Formats”，其设计初衷是为了配合ELF格式进行UNIX可执行文件的调试信息生成。DWARF调试信息主要面向开发者用以指导如何生成调试信息以及如何使用调试信息。比如编译器、链接器开发者需要参考DWARF来生成调试信息，而调试器开发者需要参考DWARF来使用调试信息。DWARF开始时主要是为 UNIX 下的调试器提供必要的调试信息，例如内存地址对应的文件名以及代码行号等信息。GCC和Clang以及Go和Rust都使用该格式生成调试信息，与该格式相对的，Windows平台使用PDB（Program Database）作为调试信息的主要格式。2017年，DWARF v5发布，提供了更好的数据压缩能力，调试信息与可执行程序的分离，对macro宏和源代码文件的更好的描述以及更快速的符号搜索、还有对编译器优化后代码的更好描述等等。 DWARF调试信息根据描述对象的不同，在最终存储到不同的section，section名称均以前缀.debug_开头。为了提升效率，对DWARF数据的大多数引用都是通过相对于当前编译单元的偏移量引用。 常见的ELF sections及其存储的内容如下: .debug_abbrev, 存储.debug_info中使用的缩写信息； .debug_arranges, 存储一个加速访问的查询表，通过内存地址查询对应编译单元信息； .debug_frame, 存储调用栈帧信息； .debug_info, 存储核心DWARF数据，包含了描述变量、代码等的DIEs； .debug_line, 存储行号表程序 (程序指令由行号表状态机执行，执行后构建出完整的行号表) .debug_loc, 存储location描述信息； .debug_macinfo, 存储宏相关描述信息； .debug_pubnames, 存储一个加速访问的查询表，通过名称查询全局对象和函数； .debug_pubtypes, 存储一个加速访问的查询表，通过名称查询全局类型； .debug_ranges, 存储DIEs中引用的address ranges； .debug_str, 存储.debug_info中引用的字符串表，也是通过偏移量来引用； .debug_types, 存储描述数据类型相关的DIEs； 符号级调试器需要两张大表，一个是行号表（Line Number Table），一个是调用栈信息表（Call Frame Information）。 行号表, 将程序代码段的指令地址映射为源文件中的地址，如“源文件名:行号”。当然如果指定了源文件中的位置，也可以将其映射为程序代码段中的指令地址。 调用栈信息表, 它允许调试器根据指令地址来定位其在调用栈上的栈帧。 DWARF使用调试信息条目DIE（Debugging Information Entry）来表示每一个编译单元，也即变量、函数、指针等。每个DIE都包含一个tag（如DW_TAG_variable表示变量，DW_TAG_pointer_type表示指针类型，DW_TAG_subprogram表示一个函数等）以及一系列的attributes。每个DIE还可以包含子DIE，结合构成树结构共同描述一个变量、数据类型、函数、编译单元等不同的程序构造，DIE中的每个attribute可以引用另一个DIE（类似指针），例如一个描述变量的DIE，它会包含一个属性DW_AT_type来指向一个描述变量数据类型的DIE。 生成时机 dSYM 文件和 DWARF 文件在编译时生成是根据链接动作中链接脚本下的符号解析与重定位构建。 DIE具体的，每个DIE都包含一个标签（tag）以及一系列的属性（attributes），存储在.debug_info和.debug_types中： tag指明了当前调试信息条目描述的程序构造属于哪种类型，如类型、变量、函数、编译单元等； attribute定义了调试信息条目的一些特征，如函数的返回值类型是int类型。 基本类型描述DW_TAG_base_type，用来描述多种基本类型，包括：整数，地址，字符和浮点数。早期 DWARF 和其他调试信息格式，都假设编译器和调试器对基本类型的大小达成共识，例如int是8位，16位还是32位。但是对于不同的硬件平台和编程语言位宽不一致， DWARF v2以后提供了基础类型和具体硬件上实现的映射解决该问题。 复合类型描述DWARF支持通过组合或者链接其他基本数据类型来定义新的数据类型。TAG为DW_TAG_base_type，表示它是一个基本数据类型，具体为4字节有符号整数。 数组DW_TAG_array_type，结合一些相关attributes共同来描述数组，数组对应的DIE，该DIE包含了这样的一些属性来描述数组元素： DW_AT_ordering：描述数组是按照“行主序”还是按照“列主序”存储，如Fortran是按照列主序存储，C和C++是按照行主序存储。如果未指定该属性值，则使用DW_AT_language指定编程语言的默认数组排列规则； DW_AT_type：描述数组中各个元素的类型信息； DW_AT_byte_stride/DW_AT_bit_stride：如果数组中每个元素的实际大小和分配的空间大小不同的话，可以通过这两个属性来说明； 数组的索引值范围，DIE中也需要通过指定最小、最大索引值来给出一个有效的索引值区间。这样DWARF就可以既能够描述C风格的数组（用0作为数组起始索引），也能够描述Pascal和Ada的数组（其数组最小索引值、最大索引值是可以变化的）。数组维度一般是通过换一个TAG为DW_TAG_subrange_type或者DW_TAG_enumeration_type的DIE来描述。 类组合多种不同的数据类型来定义一个新的数据类型是编程语言的基本功能，DWARF中分别使用下述tag来描述不同类型： DW_TAG_structure_type，描述结构体struct； DW_TAG_class_type，描述类class； DW_TAG_union_type，描述联合union； DW_TAG_interface_type，描述interface。 如果class实例的大小在编译时可以确定，比如都是基础类型或者没有堆上指针，描述class的DIE就会多一个属性DW_AT_byte_size以描述类的大小。 变量DW_TAG_variable，用来描述变量，变量名代指存储变量值的内存位置，变量的类型描述了包含的值及其是否可以修改的修饰（例如const）。对变量进行区分的关键是变量的存储位置和作用域，变量可以被存储在全局数据区（.data section）、栈、堆或者寄存器中，变量的作用域，描述了它在程序中什么时候是可见的，某种程度上，变量作用域是由其声明时的位置确定的，在DWARF中通过三元组（文件名，行号，列号）对变量声明位置进行描述。 位置信息DWARF提供了一种非常通用的机制描述如何确定变量的数据位置，就是通过属性DW_AT_location，该属性允许指定一个操作序列，来告知调试器如何确定数据的位置。调试信息必须为调试器提供一种方法，使其能够查找程序变量的位置、确定动态数组和字符串的范围，以及能找到函数栈帧的基地址或函数返回地址的方法。 而位置信息描述可以分为两类： 位置表达式，是与语言无关的寻址规则表示形式，它是由一些基本构建块、操作序列组合而成的任意复杂度的寻址规则。 只要对象的生命周期是静态或与拥有它的词法块相同，并且在整个生命周期内都不会移动，它们就足以描述任何对象的位置。 位置列表，用于描述生命周期有限的对象或在整个生命周期内可能会更改位置的对象。 位置表达式由零个或多个位置操作组成。 如果没有位置运算表达式，则表示该对象在源代码中存在，但是在目标代码中不存在，可能是由于编译器优化导致的。 可执行描述DW_TAG_subprogram用来描述函数: 函数DIE具有属性 DW_AT_low_pc、DW_AT_high_pc，以给出函数占用的内存地址空间的上下界。 函数的内存地址可能是连续的，也可能不是连续的。如果不连续，则会有一个内存范围列表。一般DW_AT_low_pc的值为函数入口点地址，除非明确指定了另一个地址； 函数的返回值类型由属性 DW_AT_type 描述。 如果没有返回值，则此属性不存在。如果在此函数的相同范围内定义了返回类型，则返回类型DIE将作为此函数DIE的兄弟DIE； 函数可能具有零个或多个形式参数，这些参数由DIE DW_TAG_formal_parameter 描述，这些形参DIE的位置被安排在函数DIE之后，并且各形参DIE的顺序按照形参列表中出现的顺序； 函数主体可能包含局部变量，这些变量由DIE DW_TAG_variables 在形参DIE之后列出。 编译单元在生成程序时，多个源文件都被视为一个独立的编译单元，并被编译为独立的*.o文件（例如C），然后链接器会将这些目标文件、系统特定的启动代码、系统库链接在一起以生成完整的可执行程序。DWARF中采用了C语言中的术语编译单元（compilation unit）作为DIE的名称 DW_TAG_compilation_unit。DIE包含有关编译的常规信息，包括源文件对应的目录和文件名、使用的编程语言、DWARF信息的生产者，以及有助于定位行号和宏信息的偏移量等等。 如果编译单元占用了连续的内存（即，它会被装入一个连续的内存区域），那么该单元的低内存地址和高内存地址将有值，即低pc和高pc属性。 这有助于调试器更轻松地确定特定地址处的指令是由哪个编译单元生成的。 如果编译单元占用的内存不连续，则编译器和链接器将提供代码占用的内存地址列表，每个编译单元都由一个“公共信息条目CIE（Common Information Entry）”表示，编译单元中除了CIE以外，还包含了一系列的帧描述条目FDE（Frame Description Entrie）。 其他信息除了这些内容以外，DWARF调试信息中还有几种非常重要的信息需要描述，符号级调试器非常依赖这些数据。这几种重要的调试信息主要包括： 加速访问 调试器经常需要根据符号名、类型名、指令地址，快速定位到对应的源代码行。DWARF为了加速查询，在DWARF信息生成的时候允许编译器额外创建3张表用来加速查询，加速符号名查询的.debug_pubnames，加速类型名查询的.debug_pubtypes（查询类型），加速指令地址查询的.debug_aranges。 行号表 DWARF行号表，包含了可执行程序机器指令的内存地址和对应的源代码行之间的映射关系。 宏信息 DWARF调试信息中包含了对程序中定义的宏的描述。这是非常基本的信息，但是调试器可以使用它来显示宏的值或将宏翻译成相应的源语言。 调用栈信息 DWARF中的调用栈信息（Call Frame Information，CFI）为调试器提供了如下信息，函数是如何被调用的，如何找到函数参数，如何找到调用函数（caller）的栈帧信息。调试器借助CFI可以展开调用栈、查找上一个函数、确定当前函数的被调用位置以及传递的参数值。 变长数据 DWARF定义了一种可变长度的整数，称为Little Endian Base 128（带符号整数为LEB128或无符号整数为ULEB128），LEB128可以压缩占用的字节来表示整数值以节省存储空间。 压缩DWARF数据 简单玩法为了不编译和链接C++标准库依赖，简化示例，给出下列代码： 12345678int foo(int x, int y) { return x + y;}int main() { int ans = foo(1, 2); return 0;} 执行命令： 12clang -O0 -gdwarf-5 test.cpp -o test # 生成dwarf和调试信息objdump --dwarf=info test 打出来的结果应该如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364test: file format elf64-x86-64Contents of the .debug_info section: Compilation Unit @ offset 0: Length: 0x66 (32-bit) Version: 5 Unit Type: DW_UT_compile (1) Abbrev Offset: 0 Pointer Size: 8 &lt;0&gt;&lt;c&gt;: Abbrev Number: 1 (DW_TAG_compile_unit) &lt;d&gt; DW_AT_producer : (indexed string: 0): Debian clang version 16.0.6 (27) &lt;e&gt; DW_AT_language : 33 (C++14) &lt;10&gt; DW_AT_name : (indexed string: 0x1): test.cpp &lt;11&gt; DW_AT_str_offsets_base: 0x8 &lt;15&gt; DW_AT_stmt_list : 0 &lt;19&gt; DW_AT_comp_dir : (indexed string: 0x2): /home/luhuanbing &lt;1a&gt; DW_AT_low_pc : (index: 0): 0x1130 &lt;1b&gt; DW_AT_high_pc : 0x49 &lt;1f&gt; DW_AT_addr_base : 0x8 &lt;1&gt;&lt;23&gt;: Abbrev Number: 2 (DW_TAG_subprogram) &lt;24&gt; DW_AT_low_pc : (index: 0): 0x1130 &lt;25&gt; DW_AT_high_pc : 0x12 &lt;29&gt; DW_AT_frame_base : 1 byte block: 56 (DW_OP_reg6 (rbp)) &lt;2b&gt; DW_AT_linkage_name: (indexed string: 0x3): _Z3fooii &lt;2c&gt; DW_AT_name : (indexed string: 0x4): foo &lt;2d&gt; DW_AT_decl_file : 0 &lt;2e&gt; DW_AT_decl_line : 1 &lt;2f&gt; DW_AT_type : &lt;0x65&gt; &lt;33&gt; DW_AT_external : 1 &lt;2&gt;&lt;33&gt;: Abbrev Number: 3 (DW_TAG_formal_parameter) &lt;34&gt; DW_AT_location : 2 byte block: 91 7c (DW_OP_fbreg: -4) &lt;37&gt; DW_AT_name : (indexed string: 0x7): x &lt;38&gt; DW_AT_decl_file : 0 &lt;39&gt; DW_AT_decl_line : 1 &lt;3a&gt; DW_AT_type : &lt;0x65&gt; &lt;2&gt;&lt;3e&gt;: Abbrev Number: 3 (DW_TAG_formal_parameter) &lt;3f&gt; DW_AT_location : 2 byte block: 91 78 (DW_OP_fbreg: -8) &lt;42&gt; DW_AT_name : (indexed string: 0x8): y &lt;43&gt; DW_AT_decl_file : 0 &lt;44&gt; DW_AT_decl_line : 1 &lt;45&gt; DW_AT_type : &lt;0x65&gt; &lt;2&gt;&lt;49&gt;: Abbrev Number: 0 &lt;1&gt;&lt;4a&gt;: Abbrev Number: 4 (DW_TAG_subprogram) &lt;4b&gt; DW_AT_low_pc : (index: 0x1): 0x1150 &lt;4c&gt; DW_AT_high_pc : 0x29 &lt;50&gt; DW_AT_frame_base : 1 byte block: 56 (DW_OP_reg6 (rbp)) &lt;52&gt; DW_AT_name : (indexed string: 0x6): main &lt;53&gt; DW_AT_decl_file : 0 &lt;54&gt; DW_AT_decl_line : 5 &lt;55&gt; DW_AT_type : &lt;0x65&gt; &lt;59&gt; DW_AT_external : 1 &lt;2&gt;&lt;59&gt;: Abbrev Number: 5 (DW_TAG_variable) &lt;5a&gt; DW_AT_location : 2 byte block: 91 78 (DW_OP_fbreg: -8) &lt;5d&gt; DW_AT_name : (indexed string: 0x9): ans &lt;5e&gt; DW_AT_decl_file : 0 &lt;5f&gt; DW_AT_decl_line : 6 &lt;60&gt; DW_AT_type : &lt;0x65&gt; &lt;2&gt;&lt;64&gt;: Abbrev Number: 0 &lt;1&gt;&lt;65&gt;: Abbrev Number: 6 (DW_TAG_base_type) &lt;66&gt; DW_AT_name : (indexed string: 0x5): int &lt;67&gt; DW_AT_encoding : 5 (signed) &lt;68&gt; DW_AT_byte_size : 4 &lt;1&gt;&lt;69&gt;: Abbrev Number: 0 总结DWARF的基本概念为在程序编译的链接的符号重定向阶段对符号进行解析并生成对应的信息： 程序被描述为“DIE节点构成的树”，抽象表示源码中的各种函数、数据和类型； 行号表提供了可执行指令地址和生成它们的源码之间的映射关系； CFI描述了如何虚拟地展开堆栈（unwind的用处）。","link":"/2024/06/15/Debug-1/"},{"title":"DuckDB-源码分析（1）背景与应用","text":"2019年，CWI发表了一篇关于DuckDB的论文：《DuckDB: an Embeddable Analytical Database》，旨在OLAP领域构建一个嵌入式的数据库，解决单点交互式数据分析的问题，并给边缘计算提供除SQLite之外的更优选择。在论文中，作者们总结DuckDB为在一个进程内的SQL OLAP DBMS，这句话至少有两个解读点： DuckDB的工作方式应该和SQLite一致，即在进程内部运行，并不需要类似MySQL或者PG的等单独起一个数据库服务； 作为AP系统，可以支撑一定复杂的数据分析和查询任务。 SQLite是在全球运行最多的关系型数据库管理系统，每个浏览器和操作系统以及各种嵌入式设备都能找到该数据库使用的痕迹。但是SQLite更多为TP任务服务，很难利用向量化、内存加速来提高数据分析的速度。所以DuckDB弥补了SQLite这方面的空白。 DuckDB 支持多种数据格式： CSV: 批量加载 CSV 文件并自动映射列内容； DataFrames: DuckDB 可以直接处理同一个 Python 进程中的 DataFrame 内存内容；JSON: DuckDB可以直接将JSON转换为关系型表格。也提供 JSON 数据类型； Parquet: DuckDB 可以查询 Parquet 文件及其架构元数据。查询中使用的谓词会下推到 Parquet 存储层进行计算，以减少加载的数据量。Parquet 是数据湖泊理想的列式格式，可用于读写数据； Apache Arrow: DuckDB 可以通过ADBC直接访问 Apache Arrow 列式数据，无需复制和转换数据； 云存储: DuckDB 可以访问云存储桶（例如 S3 或 GCP）中的数据，减少传输和复制基础设施，并允许廉价处理大量数据。 DuckDB官方提供了一个简单的WASM版本的在线环境：https://shell.duckdb.org/，可以直接在内部执行SQL，简单如下所示： 1234567891011121314151617181920212223242526272829duckdb&gt; SELECT count(*) FROM 'https://shell.duckdb.org/data/tpch/0_01/parquet/lineitem.parquet';┌──────────────┐│ count_star() │╞══════════════╡│ 60175 │└──────────────┘duckdb&gt; SELECT count(*) FROM 'https://shell.duckdb.org/data/tpch/0_01/parquet/customer.parquet';┌──────────────┐│ count_star() │╞══════════════╡│ 1500 │└──────────────┘duckdb&gt; SELECT * FROM 'https://shell.duckdb.org/data/tpch/0_01/parquet/orders.parquet' LIMIT 10;┌────────────┬───────────┬───────────────┬──────────────┬─────────────┬─────────────────┬─────────────────┬────────────────┬───────────────────────────────────────────────────────────────────────────┐│ o_orderkey ┆ o_custkey ┆ o_orderstatus ┆ o_totalprice ┆ o_orderdate ┆ o_orderpriority ┆ o_clerk ┆ o_shippriority ┆ o_comment │╞════════════╪═══════════╪═══════════════╪══════════════╪═════════════╪═════════════════╪═════════════════╪════════════════╪═══════════════════════════════════════════════════════════════════════════╡│ 1 ┆ 370 ┆ O ┆ 172799.49 ┆ 1996-01-02 ┆ 5-LOW ┆ Clerk#000000951 ┆ 0 ┆ nstructions sleep furiously among ││ 2 ┆ 781 ┆ O ┆ 38426.09 ┆ 1996-12-01 ┆ 1-URGENT ┆ Clerk#000000880 ┆ 0 ┆ foxes. pending accounts at the pending, silent asymptot ││ 3 ┆ 1234 ┆ F ┆ 205654.3 ┆ 1993-10-14 ┆ 5-LOW ┆ Clerk#000000955 ┆ 0 ┆ sly final accounts boost. carefully regular ideas cajole carefully. depos ││ 4 ┆ 1369 ┆ O ┆ 56000.91 ┆ 1995-10-11 ┆ 5-LOW ┆ Clerk#000000124 ┆ 0 ┆ sits. slyly regular warthogs cajole. regular, regular theodolites acro ││ 5 ┆ 445 ┆ F ┆ 105367.67 ┆ 1994-07-30 ┆ 5-LOW ┆ Clerk#000000925 ┆ 0 ┆ quickly. bold deposits sleep slyly. packages use slyly ││ 6 ┆ 557 ┆ F ┆ 45523.1 ┆ 1992-02-21 ┆ 4-NOT SPECIFIED ┆ Clerk#000000058 ┆ 0 ┆ ggle. special, final requests are against the furiously specia ││ 7 ┆ 392 ┆ O ┆ 271885.66 ┆ 1996-01-10 ┆ 2-HIGH ┆ Clerk#000000470 ┆ 0 ┆ ly special requests ││ 32 ┆ 1301 ┆ O ┆ 198665.57 ┆ 1995-07-16 ┆ 2-HIGH ┆ Clerk#000000616 ┆ 0 ┆ ise blithely bold, regular requests. quickly unusual dep ││ 33 ┆ 670 ┆ F ┆ 146567.24 ┆ 1993-10-27 ┆ 3-MEDIUM ┆ Clerk#000000409 ┆ 0 ┆ uriously. furiously final request ││ 34 ┆ 611 ┆ O ┆ 73315.48 ┆ 1998-07-21 ┆ 3-MEDIUM ┆ Clerk#000000223 ┆ 0 ┆ ly final packages. fluffily final deposits wake blithely ideas. spe │└────────────┴───────────┴───────────────┴──────────────┴─────────────┴─────────────────┴─────────────────┴────────────────┴───────────────────────────────────────────────────────────────────────────┘ 再测试一下语言的API使用，也非常简单，这里以Python为例： 12345678910111213141516171819202122import duckdbcon = duckdb.connect()con.sql(&quot;SELECT * FROM 'orders.parquet' LIMIT 10&quot;).show()con.close()┌────────────┬───────────┬───────────────┬──────────────┬─────────────┬─────────────────┬─────────────────┬────────────────┬───────────────────────────────────────────────────────────────────────────┐│ o_orderkey │ o_custkey │ o_orderstatus │ o_totalprice │ o_orderdate │ o_orderpriority │ o_clerk │ o_shippriority │ o_comment ││ int32 │ int32 │ varchar │ double │ date │ varchar │ varchar │ int32 │ varchar │├────────────┼───────────┼───────────────┼──────────────┼─────────────┼─────────────────┼─────────────────┼────────────────┼───────────────────────────────────────────────────────────────────────────┤│ 1 │ 370 │ O │ 172799.49 │ 1996-01-02 │ 5-LOW │ Clerk#000000951 │ 0 │ nstructions sleep furiously among ││ 2 │ 781 │ O │ 38426.09 │ 1996-12-01 │ 1-URGENT │ Clerk#000000880 │ 0 │ foxes. pending accounts at the pending, silent asymptot ││ 3 │ 1234 │ F │ 205654.3 │ 1993-10-14 │ 5-LOW │ Clerk#000000955 │ 0 │ sly final accounts boost. carefully regular ideas cajole carefully. depos ││ 4 │ 1369 │ O │ 56000.91 │ 1995-10-11 │ 5-LOW │ Clerk#000000124 │ 0 │ sits. slyly regular warthogs cajole. regular, regular theodolites acro ││ 5 │ 445 │ F │ 105367.67 │ 1994-07-30 │ 5-LOW │ Clerk#000000925 │ 0 │ quickly. bold deposits sleep slyly. packages use slyly ││ 6 │ 557 │ F │ 45523.1 │ 1992-02-21 │ 4-NOT SPECIFIED │ Clerk#000000058 │ 0 │ ggle. special, final requests are against the furiously specia ││ 7 │ 392 │ O │ 271885.66 │ 1996-01-10 │ 2-HIGH │ Clerk#000000470 │ 0 │ ly special requests ││ 32 │ 1301 │ O │ 198665.57 │ 1995-07-16 │ 2-HIGH │ Clerk#000000616 │ 0 │ ise blithely bold, regular requests. quickly unusual dep ││ 33 │ 670 │ F │ 146567.24 │ 1993-10-27 │ 3-MEDIUM │ Clerk#000000409 │ 0 │ uriously. furiously final request ││ 34 │ 611 │ O │ 73315.48 │ 1998-07-21 │ 3-MEDIUM │ Clerk#000000223 │ 0 │ ly final packages. fluffily final deposits wake blithely ideas. spe │├────────────┴───────────┴───────────────┴──────────────┴─────────────┴─────────────────┴─────────────────┴────────────────┴───────────────────────────────────────────────────────────────────────────┤│ 10 rows 9 columns │└──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘ DuckDB的实现主要分为几个部分，parser、logical planner、optimizer、physical planner、execution engine和storage engine。 ParserDuckDB的SQL parser直接使用Postgres的libpg_query，可以直接得到一个C结构体表示的parse tree。DuckDB将其转换成自己内部的C++对象，无缝接入。 Logical PlannerDuckDB的Logical planner由binder和plan generator两部分组成，binder将parse tree与schema的信息（列名、类型等）绑定。plan generator将parse tree转换成一棵逻辑查询算子（scan, filter, project等）结构树。经过planning阶段后生成logical plan，DuckDB同时保存所存储数据的统计信息，这些数据在logical plan生成阶段通过不同的expression trees向下传播，用于optimizer阶段优化查询计划。 OptimizerDuckDB实现了RBO和CBO的优化器。Optimizer将前面logical planner生成的logical plan转换成一个等价但执行代价更小的计划。常见的优化方式有predicate pushdown、expression rewriting、join ordering等。 Physical PannerPhysical planner将logical plan转换成physical plan。在生成physical plan过程中，会选择合适的实现。 Execution EngineDuckDB实现了一个向量化执行引擎，execution engine以火山的模式开始执行查询计划的。Query execution以从物理执行计划的root节点拉取chunk data开始。chunk data是结果集中间表或者基表的水平子集，递归的从子节点拉取数据，到scan operator为止。scan operator从persistent tables中读取chunk data。回溯到root节点的chunk是空时则代表该计划已经执行完。 可移植性问题没有选择JIT实现，因为JIT依赖编译组件，比如LLVM。 TransactionDuckDB使用MVCC提供了ACID，DuckDB实现了HyPer的MVCC变体，就地更新数据，并将previous states保存在一个单独的undo buffer，用于并发事务或者中止。 StorageDuckDB使用列式存储，并使用了读优化的数据存储布局。逻辑表水平划分为chunks of columns，这些chunk of columns压缩成physical block。physical block中保存min/max索引，用于在查询时判断该Block是否相关，裁剪block读的大小。也为每个column保留索引，优化列读。 作为新手学习OLAP，DuckDB是一个非常好的开始，相比于Clickhouse或者其他大型AP实现，其抽象和代码都相对清晰和完善，但是因为使用场景有限，面向云上和多核以及分布式下的设计就需要参考其他的数据库，接下来笔者会从不同部分对DuckDB进行展开和详细剖析。","link":"/2024/05/04/DuckDB-1/"},{"title":"DuckDB-源码分析（2）Parquet读设计","text":"在ParquetReader的构造函数中，会打开文件句柄，加载元数据，并初始化。Scan函数负责实际的扫描过程，处理行组，应用过滤器，读取数据到DataChunk中。首先，在元数据加载部分，parquet文件的元数据通常位于文件末尾，包含schema、行组信息、统计信息等。LoadMetadata函数通过读取文件末尾的元数据部分，解析出FileMetaData结构。 DeriveLogicalType函数根据Parquet的SchemaElement中的类型信息（如Type、converted_type、logicalType）转换为DuckDB的LogicalType。例如，Parquet的INT32可能对应DuckDB的INTEGER，而带有converted_type为DATE的INT32会被转换为DATE类型。这里还处理了时间戳、UUID等复杂类型的转换。 创建列读取器的过程在CreateReaderRecursive中完成，这个函数递归地遍历SchemaElement树，根据每个元素的类型和子元素创建相应的ColumnReader。例如，遇到STRUCT类型时，会创建StructColumnReader，并为每个子列创建对应的读取器，对于重复的字段（如LIST或MAP），会使用ListColumnReader来处理嵌套结构。 初始化时，InitializeSchema调用CreateReader创建根读取器，通常是StructColumnReader，因为它对应Parquet文件的根结构。然后根据读取到的列信息填充columns向量，记录每个列的名称和类型。 扫描数据的过程由Scan函数处理。ParquetReaderScanState用于跟踪扫描的状态，包括当前处理的行组、文件句柄、过滤器等。PrepareRowGroupBuffer函数准备当前行组的数据，可能应用统计信息进行过滤，跳过不需要读取的行组。然后通过ColumnReader的Read方法将数据读取到DataChunk中，应用过滤器（如果有的话），最终将结果返回。 过滤器处理部分，ApplyFilter函数根据过滤条件对读取的数据进行过滤。例如，等值比较、范围比较、IS NULL等条件会被应用到数据上，减少需要处理的数据量，提升查询性能。 初始化12345678910111213141516171819202122ParquetReader::ParquetReader(ClientContext &amp;context, string file_name, ParquetOptions parquet_options, shared_ptr&lt;ParquetFileMetadataCache&gt; metadata) : fs(FileSystem::GetFileSystem(context)), allocator(BufferAllocator::Get(context)), parquet_options(std::move(parquet_options)) { // 1. 打开文件 file_handle = fs.OpenFile(file_name, FileFlags::FILE_FLAGS_READ); // 2. 加载/获取元数据缓存 if (!metadata) { this-&gt;metadata = LoadMetadata(context, allocator, *file_handle, parquet_options.encryption_config, *encryption_util); } else { this-&gt;metadata = std::move(metadata); } // 3. 初始化Schema InitializeSchema(context);} 关键步骤： 文件打开：使用DuckDB统一文件系统接口 元数据加载： 优先使用传入的缓存元数据; 无缓存时调用LoadMetadata解析 Schema初始化：构建列读取器的树 MetaData加载12345678910111213141516171819202122shared_ptr&lt;ParquetFileMetadataCache&gt; LoadMetadata(...) { // 创建Thrift协议解析器 auto file_proto = CreateThriftFileProtocol(allocator, file_handle, false); // 读取文件尾部8字节 transport.read(buf.ptr, 8); if (memcmp(buf.ptr+4, &quot;PAR1&quot;,4)==0) { footer_encrypted = false; // 标准Parquet文件 } else if (memcmp(..., &quot;PARE&quot;, ...)) { footer_encrypted = true; // 加密文件 ParquetCrypto::Read(...); // AES-GCM解密 } // 反序列化元数据 auto metadata = make_uniq&lt;FileMetaData&gt;(); metadata-&gt;read(file_proto.get()); // 处理GeoParquet扩展 auto geo_metadata = GeoParquetFileMetadata::TryRead(*metadata, context); return make_shared_ptr&lt;ParquetFileMetadataCache&gt;(...);} 元数据结构： 12345struct ParquetFileMetadataCache { unique_ptr&lt;FileMetaData&gt; metadata; // Thrift生成的元数据 time_t read_time; // 缓存时间戳 unique_ptr&lt;GeoParquetFileMetadata&gt; geo_metadata; // 地理扩展}; Schema 初始化12345678910111213141516171819202122void ParquetReader::InitializeSchema(ClientContext &amp;context) { auto file_meta_data = GetFileMetadata(); // 1. 创建根读取器 root_reader = CreateReader(context); // 2. 构建列定义 auto &amp;struct_reader = root_reader-&gt;Cast&lt;StructColumnReader&gt;(); for (idx_t i = 0; i &lt; child_types.size(); i++) { columns.emplace_back( MultiFileReaderColumnDefinition( child_types[i].first, // 列名 child_types[i].second // 逻辑类型 ) ); } // 3. 处理生成列（如file_row_number） if (parquet_options.file_row_number) { columns.emplace_back(&quot;file_row_number&quot;, LogicalType::BIGINT); }} 结构说明graph TD A[ParquetReader] --> B[FileHandle] A --> C[ParquetFileMetadataCache] C --> D[FileMetaData] C --> E[GeoParquetMetadata] A --> F[ColumnReader树] subgraph Metadata D --> G[SchemaElements] D --> H[RowGroups] H --> I[ColumnChunks] I --> J[Statistics] I --> K[EncryptionInfo] end subgraph Reader F --> L[StructColumnReader] L --> M[ColumnReader...] M --> N[ListColumnReader] M --> O[TemplatedColumnReader] end B -->|read| P[Parquet file] 关键数据结构说明 FileMetaData 元数据类结构 123456789struct FileMetaData { 1: required i32 version 2: required list&lt;SchemaElement&gt; schema 3: required i64 num_rows 4: required list&lt;RowGroup&gt; row_groups 5: optional list&lt;KeyValue&gt; key_value_metadata 6: optional string created_by 7: optional list&lt;ColumnOrder&gt; column_orders} ColumnReader 类层次 12345678910111213141516171819class ColumnReader {public: virtual void Read(...) = 0; virtual void Skip(...) = 0; virtual unique_ptr&lt;BaseStatistics&gt; Stats(...) = 0;};class StructColumnReader : public ColumnReader { vector&lt;unique_ptr&lt;ColumnReader&gt;&gt; child_readers;};class ListColumnReader : public ColumnReader { unique_ptr&lt;ColumnReader&gt; child_reader;};template &lt;class T&gt;class TemplatedColumnReader : public ColumnReader { // 物理类型特化实现}; 初始化阶段性能优化 元数据缓存 12345metadata = ObjectCache::Get(file_name);if (!metadata || expired) { metadata = LoadMetadata(...); ObjectCache::Put(file_name, metadata);} 延迟加载 123456void StructColumnReader::InitializeRead(...) { // 按需初始化子读取器 for (auto &amp;child : child_readers) { if (child) child-&gt;InitializeRead(...); }} 内存预分配 12ResizeableBuffer define_buf;define_buf.resize(allocator, STANDARD_VECTOR_SIZE); // 2048 初始化时序图sequenceDiagram participant Client participant ParquetReader participant FileSystem participant ThriftParser participant ColumnReader Client->>ParquetReader: Create Reader(file_name) ParquetReader->>FileSystem: OpenFile() FileSystem-->>ParquetReader: Return FileHandle alt non-cache ParquetReader->>ThriftParser: LoadMetadata() ThriftParser->>FileHandle: Read file footer ThriftParser-->>ParquetReader: FileMetaData else cache ParquetReader->>ObjectCache: GetMetadata() end ParquetReader->>ColumnReader: CreateReaderRecursive() loop create ColumnReader->>ColumnReader: Create sub reader end ParquetReader-->>Client: Init done 扫描数据过程123456789101112131415161718192021222324252627void ParquetReader::InitializeScan(ClientContext &amp;context, ParquetReaderScanState &amp;state, vector&lt;idx_t&gt; groups_to_read) { // 重置扫描状态 state.current_group = -1; state.group_offset = 0; state.finished = false; // 独立文件句柄（支持多线程） state.file_handle = fs.OpenFile(file_name, FileFlags::FILE_FLAGS_READ | (prefetch ? FILE_FLAGS_DIRECT_IO : 0)); // 创建Thrift协议解析器（每个扫描状态独立） state.thrift_file_proto = CreateThriftFileProtocol(allocator, *state.file_handle, state.prefetch_mode); // 创建列读取器树（可能带谓词过滤） state.root_reader = CreateReader(context); // 初始化缓冲区 state.define_buf.resize(allocator, STANDARD_VECTOR_SIZE); state.repeat_buf.resize(allocator, STANDARD_VECTOR_SIZE); // 配置预取模式 Value prefetch_all; context.TryGetCurrentSetting(&quot;prefetch_all_parquet_files&quot;, prefetch_all); state.prefetch_mode = prefetch_all.GetValue&lt;bool&gt;();} 关键点： 每个扫描状态维护独立的文件句柄和Thrift解析器，支持并发扫描 根据配置选择直接IO模式（减少内核缓存开销） 列读取器树可能根据谓词进行剪枝（减少不必要的列读取） RowGroup处理123456789101112131415161718192021void ParquetReader::PrepareRowGroupBuffer(ParquetReaderScanState &amp;state, idx_t out_col_idx) { auto &amp;group = GetGroup(state); auto column_id = reader_data.column_ids[out_col_idx]; auto &amp;column_reader = state.root_reader-&gt;Cast&lt;StructColumnReader&gt;().GetChildReader(column_id); // 1. 检查行组统计信息 auto stats = column_reader.Stats(state.current_group, group.columns); if (stats &amp;&amp; reader_data.filters) { auto global_id = reader_data.column_mapping[out_col_idx]; auto filter = reader_data.filters-&gt;filters[global_id]; // 2. 应用统计过滤 if (stats-&gt;CheckFilter(*filter) == FilterPropagateResult::FILTER_ALWAYS_FALSE) { state.group_offset = group.num_rows; // 跳过整个行组 return; } } // 3. 初始化该RowGroup列读取器 column_reader.InitializeRead(state.current_group, group.columns, *state.thrift_file_proto);} 过滤逻辑： 使用行组的min/max值快速判断是否跳过; 对字符串列使用更精确的字典过滤; ScanInternal 函数ScanInternal函数负责从Parquet文件中读取数据到DataChunk中，处理行组切换、预取、数据解码和过滤等步骤。函数开始检查是否完成扫描，然后处理行组切换。在切换行组时，会进行预取操作，包括整个行组的预取或按列预取。之后，函数计算需要读取的行数，初始化过滤掩码，然后按列读取数据，应用过滤器，最后更新状态。在预取部分，代码根据是否启用预取模式（prefetch_mode）以及扫描的列比例（scan_percentage）来决定是预取整个行组还是按列预取。 行组切换与预取处理 123456789101112131415161718192021222324252627282930313233343536373839if (state.current_group &lt; 0 || (int64_t)state.group_offset &gt;= GetGroup(state).num_rows) { // 切换到下一个行组 state.current_group++; state.group_offset = 0; auto &amp;trans = reinterpret_cast&lt;ThriftFileTransport &amp;&gt;(*state.thrift_file_proto-&gt;getTransport()); trans.ClearPrefetch(); // 清空之前的预取 // 检查行组边界 if ((idx_t)state.current_group == state.group_idx_list.size()) { state.finished = true; return false; } // 计算需要扫描的压缩字节数 uint64_t to_scan_compressed_bytes = 0; for (所有列) { PrepareRowGroupBuffer(...); // 准备列读取器 to_scan_compressed_bytes += 列压缩大小; } // 预取决策逻辑 if (state.prefetch_mode) { double scan_percentage = 扫描字节数 / 行组总跨度; if (scan_percentage &gt; 0.95) { // 全行组预取 trans.Prefetch(起始偏移, 行组总跨度); } else { // 按列预取 for (所有列) { if (无过滤 || 列有过滤条件) { 注册列预取; } } trans.PrefetchRegistered(); // 触发预取 } }} 预取策略： 条件 策略 适用场景 scan_percentage &gt; 95% 预取整个行组 全表扫描 存在过滤器 仅预取过滤列 条件查询 无过滤器 预取所有列 投影查询 读取逻辑 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748// 计算本次读取行数idx_t this_output_chunk_rows = Min(STANDARD_VECTOR_SIZE, GetGroup().num_rows - state.group_offset);result.SetCardinality(this_output_chunk_rows);// 初始化过滤掩码parquet_filter_t filter_mask;filter_mask.set(); // 初始全有效for (i &gt;= this_output_chunk_rows) filter_mask.reset(i); // 屏蔽超出部分// 列处理流程if (存在过滤器) { // 第一阶段：处理过滤列 for (每个过滤条件) { if (过滤列是常量) { ApplyFilter(常量向量, 过滤条件); } else { 读取过滤列数据到result_vector; ApplyFilter(result_vector, 过滤条件); } } // 第二阶段：处理其他列 for (剩余列) { if (过滤结果全无效) { Skip(); // 跳过解码 } else { 读取列数据到result_vector; } } // 生成选择向量 SelectionVector sel; for (i=0 到 this_output_chunk_rows) { if (filter_mask.test(i)) sel.append(i); } result.Slice(sel); // 压缩结果} else { // 无过滤直接读取 for (所有列) { 读取列数据到result_vector; }}// 更新偏移量state.group_offset += this_output_chunk_rows; ThriftFileTransport 数据结构123456789struct ReadHead { ReadHead(idx_t location, uint64_t size) : location(location), size(size) {}; idx_t location; // 数据块起始位置 uint64_t size; // 数据块大小 AllocatedData data; // 分配的存储空间 bool data_isset = false; // 数据是否已加载标记 idx_t GetEnd() const { return size + location; } // 获取数据块结束位置 void Allocate(Allocator &amp;allocator) { data = allocator.Allocate(size); } // 分配内存空间}; ReadHead表示一个数据块的基本信息，包括起始位置、大小、存储空间等信息。初始化时传入起始位置和大小，并且提供获取数据块结束位置的方法，以及提供内存分配方法，将数据加载到data缓冲区。 1234567891011121314struct ReadHeadComparator { static constexpr uint64_t ALLOW_GAP = 1 &lt;&lt; 14; // 16 KiB bool operator()(const ReadHead *a, const ReadHead *b) const { auto a_start = a-&gt;location; auto a_end = a-&gt;location + a-&gt;size; auto b_start = b-&gt;location; if (a_end &lt;= NumericLimits&lt;idx_t&gt;::Maximum() - ALLOW_GAP) { a_end += ALLOW_GAP; } return a_start &lt; b_start &amp;&amp; a_end &lt; b_start; }}; ReadHeadComparator用于比较两个ReadHead对象的位置关系，判断是否允许合并或重叠。具体的，类中定义了一个允许的隔距ALLOW_GAP（16KiB），用来比较两个数据块的起始位置和结束位置：如果a的结束位置（加上允许隔距）在b的起始位置之前，则认为两者不重叠。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465struct ReadAheadBuffer { ReadAheadBuffer(Allocator &amp;allocator, FileHandle &amp;handle) : allocator(allocator), handle(handle) {} std::list&lt;ReadHead&gt; read_heads; // 存储所有预读数据块 std::set&lt;ReadHead *, ReadHeadComparator&gt; merge_set; // 用于合并数据块 Allocator &amp;allocator; // 内存分配器 FileHandle &amp;handle; // 文件句柄 idx_t total_size = 0; // 总预读大小 // 添加一个预读数据块 void AddReadHead(idx_t pos, uint64_t len, bool merge_buffers = true) { // 尝试合并现有数据块 if (merge_buffers) { ReadHead new_read_head {pos, len}; auto lookup_set = merge_set.find(&amp;new_read_head); if (lookup_set != merge_set.end()) { auto existing_head = *lookup_set; auto new_start = MinValue(existing_head-&gt;location, new_read_head.location); auto new_length = MaxValue(existing_head-&gt;GetEnd(), new_read_head.GetEnd()) - new_start; existing_head-&gt;location = new_start; existing_head-&gt;size = new_length; return; } } read_heads.emplace_front(ReadHead(pos, len)); total_size += len; auto &amp;read_head = read_heads.front(); if (merge_buffers) { merge_set.insert(&amp;read_head); } // 检查数据块是否超出文件范围 if (read_head.GetEnd() &gt; handle.GetFileSize()) { throw std::runtime_error(&quot;Prefetch registered for bytes outside file&quot;); } } // 获取相关数据块 ReadHead *GetReadHead(idx_t pos) { for (auto &amp;read_head : read_heads) { if (pos &gt;= read_head.location &amp;&amp; pos &lt; read_head.GetEnd()) { return &amp;read_head; } } return nullptr; } // 预读所有数据块 void Prefetch() { for (auto &amp;read_head : read_heads) { read_head.Allocate(allocator); if (read_head.GetEnd() &gt; handle.GetFileSize()) { throw std::runtime_error(&quot;Prefetch requested for bytes outside file&quot;); } handle.Read(read_head.data.get(), read_head.size, read_head.location); read_head.data_isset = true; } }}; ReadAheadBuffer 结构用来管理预读数据块的队列和合并操作，list存储所有未读取的数据块，set配合ReadHeadComparator合并相邻或重叠的数据块，减少存储和合并IO，具体的： AddReadHead添加预读数据块，并尝试与现有数据块合并； GetReadHead根据给定位置返回对应的数据块； Prefetch将所有预读数据块从文件中读取并加载到内存。 12345678910111213141516171819202122232425262728293031323334353637class ThriftFileTransport { // void Prefetch(idx_t pos, uint64_t len) { RegisterPrefetch(pos, len, false); FinalizeRegistration(); PrefetchRegistered(); } // void RegisterPrefetch(idx_t pos, uint64_t len, bool can_merge = true) { ra_buffer.AddReadHead(pos, len, can_merge); } // void FinalizeRegistration() { ra_buffer.merge_set.clear(); } // void PrefetchRegistered() { ra_buffer.Prefetch(); } // void ClearPrefetch() { ra_buffer.read_heads.clear(); ra_buffer.merge_set.clear(); } uint32_t read(uint8_t *buf, uint32_t len) { auto prefetch_buffer = ra_buffer.GetReadHead(location); if (prefetch_buffer != nullptr &amp;&amp; location - prefetch_buffer-&gt;location + len &lt;= prefetch_buffer-&gt;size) { // 如果有预读数据，直接使用 if (!prefetch_buffer-&gt;data_isset) { prefetch_buffer-&gt;Allocate(allocator); handle.Read(prefetch_buffer-&gt;data.get(), prefetch_buffer-&gt;size, prefetch_buffer-&gt;location); prefetch_buffer-&gt;data_isset = true; } memcpy(buf, prefetch_buffer-&gt;data.get() + location - prefetch_buffer-&gt;location, len); } else { // 如果没有预读数据，直接从文件中读取 if (prefetch_mode &amp;&amp; len &lt; PREFETCH_FALLBACK_BUFFERSIZE &amp;&amp; len &gt; 0) { Prefetch(location, MinValue(handle.GetFileSize() - location, PREFETCH_FALLBACK_BUFFERSIZE)); auto prefetch_buffer_fallback = ra_buffer.GetReadHead(location); memcpy(buf, prefetch_buffer_fallback-&gt;data.get() + location - prefetch_buffer_fallback-&gt;location, len); } else { handle.Read(buf, len, location); } } location += len; return len; }private: FileHandle &amp;handle; idx_t location; Allocator &amp;allocator; ReadAheadBuffer ra_buffer; bool prefetch_mode;} ThriftFileTransport重载read方法，优先使用预读数据块，如果数据块未加载则实时读取，并且提供方法Prefetch、RegisterPrefetch、FinalizeRegistration和PrefetchRegistered，实现预读策略： 先注册所有需要预读的范围； 清理合并操作以固定范围； 执行预读操作将数据加载到内存。 不同预取策略： 策略类型 触发条件 优势 实现位置 主动批量预取 已知读取模式时 最大化顺序读取性能 ReadAheadBuffer::Prefetch 被动按需预取 未预取或预取未覆盖时 避免小数据预取开销 ThriftFileTransport::read 读取流程图 graph TD A[Open file] --> B{Crypt check} B -- Y --> C[Decrypt metadata] B -- N --> D[Read metadata] C --> E[Decode metadata] D --> E E --> F[Create schema tree] F --> G[Create column reader] G --> H[Struct/List column process] G --> I[Basic type column process] subgraph scan loop J[Choose row group] --> K{Row group filter?} K -- Jump --> M[Next row group] K -- Read --> L[Init row group reader] L --> N[Prefetch] N --> O[Column read] O --> P[Apply filter] P --> Q[Vector filter column] Q --> R[Output DataChunk] R --> S{More row group?} S -- Y --> J S -- N --> T[Scan done] end E --> J I --> O 总结DuckDB 读取 Parquet 文件的设计通过文件系统接口打开文件，借助 Thrift 协议解析元数据，构建列读取器树并初始化 Schema。在扫描过程中，通过行组过滤、列并行读取和谓词下推等优化策略，高效地预取数据并将其转化为向量化数据块，同时利用元数据缓存、延迟加载和内存预分配等机制提升性能，实现对 Parquet 文件的快速读取和处理。设计充分考虑了现代硬件特性，通过向量化处理、数据局部性优化和多级并行，实现了高效的列式数据读取。同时，灵活的过滤机制使得在复杂查询场景下能显著减少不必要的IO和计算开销。","link":"/2025/02/16/DuckDB-2/"},{"title":"多线程共享与伪共享检测","text":"多线程/进程之间使用缓存一致性协议来保证每个包含独立缓存对象的核心在共享使用内存数据时的一致性，缓存一致性协议保证了缓存实体中的任何更新都会对相同位置的其他缓存进行全部更新。MESI协议是最著名的一致性协议，支持现代CPU中缓存回写。通过监控内存事务保持一致性，一致性问题可以得到缓解，但是是有代价的，导致CPU访存空转，浪费系统带宽，两种具有代表性的内存一致性问题是“真共享”和“伪共享”。 “真共享”多线程同时访问相同变量时，为真共享，下列代码简单说明了该问题： 123456let mut sum: u64 = 0;{ // parallel section for i in 0..N { sum += i; // sum is shared between all threads } } 数据竞争是未定义行为，C++下Clang的Thread sanitizer和helgrind可以一定程度检查出数据竞争。而Rust中的数据竞争检测在编译阶段就通过其所有权机制和借用检查器实现，具体的，Rust利用RAII机制在作用域结束时释放对象持有的内存，该内存有且只有一个对象持有，借用检查器是Rust在编译期预防数据竞争的主要手段，确保了在任何时候只有一个线程可以“借用”内存区域。 而在C++中，将sum变量变为原子变量有助于解决真共享发生的数据竞争问题。但是在进行内存顺序的排序、串行化的内存操作之后，会极为影响性能，详细见Perfbook关于多线程Counter的相关设计。 而另外一个解决真共享的方法是使用TLS（Thread Local Storage，TLS）。通过TLS在给定的多线程下，每个线程都可以分配内存来存储该线程内的独占数据，线程之间便可不竞争全局变量。在C++使用thread_local关键字修饰某个变量为线程独占，而在Rust下使用 thread_local 宏可以初始化线程局部变量，然后在线程内部使用该变量的 with 方法获取变量值，或者使用attribute的的#[thread_local]进行标记，或者，可以使用thread_local第三方库，具体例子见下面代码： 123456789101112131415thread_local! { static MACRO_TLS: std::cell::RefCell&lt;Foo&gt; = std::cell::RefCell::new(Foo(0));}#[thread_local]static mut ATTR_TLS: Foo = Foo(0);fn main() { let _ = std::thread::spawn(|| unsafe { MACRO_TLS.with(|f| { println!(&quot;foo: {}&quot;, f.borrow_mut().0); }); }) .join();} 123456789101112131415161718192021222324252627#include &lt;iostream&gt;#include &lt;mutex&gt;#include &lt;string&gt;#include &lt;thread&gt; thread_local unsigned int rage = 1; std::mutex cout_mutex; void increase_rage(const std::string&amp; thread_name){ ++rage; // modifying outside a lock is okay; this is a thread-local variable std::lock_guard&lt;std::mutex&gt; lock(cout_mutex); std::cout &lt;&lt; &quot;Rage counter for &quot; &lt;&lt; thread_name &lt;&lt; &quot;: &quot; &lt;&lt; rage &lt;&lt; '\\n';} int main(){ std::thread a(increase_rage, &quot;a&quot;), b(increase_rage, &quot;b&quot;); { std::lock_guard&lt;std::mutex&gt; lock(cout_mutex); std::cout &lt;&lt; &quot;Rage counter for main: &quot; &lt;&lt; rage &lt;&lt; '\\n'; } a.join(); b.join();} C++的例子来自于cppreference, 当使用该关键字声明时，每个线程都有自己的副本，当通过名称引用时，将使用与当前线程关联的副本。而如果在类的成员上使用thread_local关键字修饰，则在同一个线程内该类的多个成员都会共享该变量，这一点和static关键字一致。 “伪共享”当多个不同的线程修改恰巧位于同一缓存行的不同变量时，称为“伪共享”。下面使用一段代码来说明该问题： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226#define _MULTI_THREADED#define _GNU_SOURCE#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;stdlib.h&gt;#include &lt;stdint.h&gt;#include &lt;unistd.h&gt;#include &lt;sched.h&gt;#include &lt;pthread.h&gt;#include &lt;numa.h&gt;#include &lt;sys/types.h&gt;/* * A thread on each numa node seems to provoke cache misses */#define LOOP_CNT (5 * 1024 * 1024)#if defined(__x86_64__) || defined(__i386__)static __inline__ uint64_t rdtsc() { unsigned hi, lo; __asm__ __volatile__ ( &quot;rdtsc&quot; : &quot;=a&quot;(lo), &quot;=d&quot;(hi)); return ( (uint64_t)lo) | ( ((uint64_t)hi) &lt;&lt; 32);}#elif defined(__aarch64__)static __inline__ uint64_t rdtsc(void){ uint64_t val; /* * According to ARM DDI 0487F.c, from Armv8.0 to Armv8.5 inclusive, the * system counter is at least 56 bits wide; from Armv8.6, the counter * must be 64 bits wide. So the system counter could be less than 64 * bits wide and it is attributed with the flag 'cap_user_time_short' * is true. */ asm volatile(&quot;mrs %0, cntvct_el0&quot; : &quot;=r&quot; (val)); return val;}#endif/* * Create a struct where reader fields share a cacheline with the hot lock field. * Compiling with -DNO_FALSE_SHARING inserts padding to avoid that sharing. */typedef struct _buf { long lock0; long lock1; long reserved1;#if defined(NO_FALSE_SHARING) long pad[5]; // to keep the 'lock*' fields on their own cacheline.#else long pad[1]; // to provoke false sharing.#endif long reader1; long reader2; long reader3; long reader4;} buf __attribute__((aligned (64)));buf buf1;buf buf2;volatile int wait_to_begin = 1;struct thread_data *thread;int max_node_num;int num_threads;char * lock_thd_name = &quot;lock_th&quot;;char * reader_thd_name = &quot;reader_thd&quot;;#define checkResults(string, val) { \\ if (val) { \\ printf(&quot;Failed with %d at %s&quot;, val, string); \\ exit(1); \\ } \\}struct thread_data { pthread_t tid; long tix; long node; char *name;};/* * Bind a thread to the specified numa node.*/void setAffinity(void *parm) { volatile uint64_t rc, j; int node = ((struct thread_data *)parm)-&gt;node; char *func_name = ((struct thread_data *)parm)-&gt;name; numa_run_on_node(node); pthread_setname_np(pthread_self(),func_name);}/* * Thread function to simulate the false sharing. * The &quot;lock&quot; threads will test-n-set the lock field, * while the reader threads will just read the other fields * in the struct. */extern void *read_write_func(void *parm) { int tix = ((struct thread_data *)parm)-&gt;tix; uint64_t start, stop, j; char *thd_name = ((struct thread_data *)parm)-&gt;name; // Pin each thread to a numa node. setAffinity(parm); // Wait for all threads to get created before starting. while(wait_to_begin) ; start = rdtsc(); for(j=0; j&lt;LOOP_CNT; j++) { // Check for lock thread. if (*thd_name == *lock_thd_name) { __sync_lock_test_and_set(&amp;buf1.lock0, 1 ); buf1.lock0 += 1; buf2.lock1 = 1; } else { // Reader threads. switch(tix % max_node_num) { volatile long var; case 0: var = *(volatile uint64_t *)&amp;buf1.reader1; var = *(volatile uint64_t *)&amp;buf2.reader1; break; case 1: var = *(volatile uint64_t *)&amp;buf1.reader2; var = *(volatile uint64_t *)&amp;buf2.reader2; break; case 2: var = *(volatile uint64_t *)&amp;buf1.reader3; var = *(volatile uint64_t *)&amp;buf2.reader3; break; case 3: var = *(volatile uint64_t *)&amp;buf1.reader4; var = *(volatile uint64_t *)&amp;buf2.reader4; break; }; }; } // End of for LOOP_CNT loop // Print out stats // stop = rdtsc(); int cpu = sched_getcpu(); int node = numa_node_of_cpu(cpu); printf(&quot;%ld mticks, %s (thread %d), on node %d (cpu %d).\\n&quot;, (stop-start)/1000000, thd_name, tix, node, cpu); return NULL;}int main ( int argc, char *argv[] ){ int i, n, rc=0; if ( argc != 2 ) /* argc should be 2 for correct execution */ { printf( &quot;usage: %s &lt;n&gt;\\n&quot;, argv[0] ); printf( &quot;where \\&quot;n\\&quot; is the number of threads per node\\n&quot;); exit(1); } if ( numa_available() &lt; 0 ) { printf( &quot;NUMA not available\\n&quot; ); exit(1); } int thread_cnt = atoi(argv[1]); max_node_num = numa_max_node(); if ( max_node_num == 0 ) max_node_num = 1; int node_cnt = max_node_num + 1; // Use &quot;thread_cnt&quot; threads per node. num_threads = (max_node_num +1) * thread_cnt; thread = malloc( sizeof(struct thread_data) * num_threads); // Create the first half of threads as lock threads. // Assign each thread a successive round robin node to // be pinned to (later after it gets created.) // for (i=0; i&lt;=(num_threads/2 - 1); i++) { thread[i].tix = i; thread[i].node = i%node_cnt; thread[i].name = lock_thd_name; rc = pthread_create(&amp;thread[i].tid, NULL, read_write_func, &amp;thread[i]); checkResults(&quot;pthread_create()\\n&quot;, rc); usleep(500); } // Create the second half of threads as reader threads. // Assign each thread a successive round robin node to // be pinned to (later after it gets created.) // for (i=((num_threads/2)); i&lt;(num_threads); i++) { thread[i].tix = i; thread[i].node = i%node_cnt; thread[i].name = reader_thd_name; rc = pthread_create(&amp;thread[i].tid, NULL, read_write_func, &amp;thread[i]); checkResults(&quot;pthread_create()\\n&quot;, rc); usleep(500); } // Sync to let threads start together usleep(500); wait_to_begin = 0; for (i=0; i &lt;num_threads; i++) { rc = pthread_join(thread[i].tid, NULL); checkResults(&quot;pthread_join()\\n&quot;, rc); } return 0;} 这段代码是一个多线程的C代码。具体地，通过创建不同的线程并将它们绑定到特定的NUMA节点，伪造伪共享的效果： NUMA将每个线程绑定到特定的NUMA节点，确保线程在特定物理内存位置上运行； 高精度计时使用rdtsc指令测量代码段的执行时间； 伪共享和缓存行对齐结构体buf中成员可能会因为缓存行对齐而共享相同的缓存行。 这里有两种编译模式： 默认模式下只有一个长整型的填充，用于触发伪共享； 另一模式通过定义 NO_FALSE_SHARING 预处理宏，增加额外的填充以避免伪共享。 下面是一些代码的详细解释和补充说明，可以按需跳过： 结构体 buf 的设计：这个结构体有两个锁变量 lock0 和 lock1，以及四个变量 reader1 到 reader4。根据编译选项，这些变量可能位于同一缓存行（为了故意制造伪共享），或者通过插入填充以分开到不同的缓存行（为了避免伪共享）。 线程功能 read_write_func：每个线程都会执行这个函数。如果是锁线程（通过名称判断），它会连续设置并增加锁变量；如果是读取线程，它则会读取指定的读变量。这样设计是为了在锁和读取之间制造大量的内存访问和潜在的竞争。 主函数：首先检查 NUMA 是否可用，然后解析命令行参数来确定每个 NUMA 节点应创建多少线程。之后，为每个线程分配一个结构体 thread_data，用于存储线程的元数据，如线程 ID、所属 NUMA 节点和线程名称。然后创建线程，先是锁线程后是读取线程，每个线程都通过 pthread_create 调用启动。 下面介绍perf工具如何检查伪共享，以及如何解读结果。Linux perf工具和Intel VTune Profiler都支持检测伪共享。这里仅对perf工具进行展开，perf c2c会匹配不同线程的内存保存和和加载地址，并查看是否有在被修改的缓存行的命中。这里先对上述C代码进行编译和运行，并在同时收集perf data： 12345clang -g false_sharing_example.c -pthread -lnuma -o false_sharing./false_sharing.exe &lt;number of threads per node&gt;sudo perf c2c record -F 60000 -a -u --ldlat 50 sleep 3 结果： 结果1：HITM代表的是在修改后的cacheline中命中的负载，这是伪共享发生的关键指标。 Remote HITM是跨NUMA节点，是最贵的操作； 结果2：第二个结果是个排序的热度，按cacheline的远程HITM或本地HITM进行排序，Rmt LLC Load Hitm标记了跨NUMA节点； 结果3：访问cache line的平均延迟、CPU时钟周期、线程pid、函数之类的信息，具体的： Shared Data Cache Line Table：每一行代表一个缓存行，列出内存地址、所属NUMA节点、页面访问计数（PA cnt）、命中次数（Hitm）、以及其他的一些命中和失效统计数据。可以看出缓存行地址分别有超过50,000次的总访问（Total records），它们在本地节点（LclHitm）和远程节点（RmtHitm）上的命中次数，这可以显示出在不同节点间的数据共享模式。 Shared Cache Line Distribution Pareto：展示缓存行命中的分布情况。可以看到远程命中（RmtHitm）和本地命中（LclHitm）的百分比，这是衡量NUMA节点之间内存访问效率的重要指标。比如，对于第一行（#0）的缓存行，有大约30%的访问是本地命中，也有相似的百分比是远程命中，有相当比例的内存访问需要跨越 NUMA 节点，这通常会导致更高的延迟。 Code address, cycles, and symbols：列出了代码地址与其对应的 CPU 周期数，这有助于识别哪些函数可能对缓存行产生影响，并估计它们的影响大小。例如，对于不同的代码地址，有不同的本地和远程命中次数（lcl hitm 和 rmt hitm），可以识别出哪些代码路径可能导致不必要的远程内存访问。","link":"/2024/04/19/False-share/"},{"title":"现代 C++ 分享（1）—— 生命周期、所有权和资源管理","text":"这是整个现代 C++ 分享系列第一篇，关于生命周期、所有权和资源管理。主题包括但不限于指针、智能指针、引用、类型系统、移动语义以及完美转发和引用折叠相关的主题。 C++ 在演进过程中逐渐增强和扩展了对类型处理的能力： C++11 中引入右值引用，通过重新定义值类别对表达式进行分类，右值引用能表达移动语义，解决了 C++11 之前产生的中间临时对象需要多次拷贝的问题； C++11 中引入 auto 关键字，对初始化变量进行推导，并且引入 decltype 关键字，通过已有对象、变量获得类型； C++17 引入 optional 类型表达对象是否存在，并且引入 variant 作为类型安全的 union，类型表达更灵活。 C++20 中引入 concept 特性对类型在编译期做约束，增强类型的表达和检查能力。 在 C++ 中，容器和指针抽象后想要被正确且高效的使用，在工程中通常需要封装一组数据及函数来访问和操作。举例来说，指针是通用和有效抽象的机器地址，但正确使用指针来表示资源的所有权是非常困难的，因此标准库提供了智能指针类管理资源和生命周期。指针的更泛化的概念是，任何允许我们引用对象并根据其类型访问。 指针和引用指针是从 C 语言中延续下来的概念，而引用是在指针基础上结合资源管理进一步在编译器层做的管理手段，其本质在汇编层面并无区别。但是在使用时，需要注意以下几点： 指针是一个变量，它保存了另一个变量的内存地址；引用是另一个变量的别名，与原变量共享内存地址； 指针可以被重新赋值，指向不同的变量；引用在初始化后不能更改，始终指向同一个变量； 指针可以为 nullptr，表示不指向任何变量；引用必须绑定到一个变量，不能为 nullptr； 指针需要对其进行解引用以获取或修改其指向的变量的值；引用可以直接使用，无需解引用。 1234567891011121314151617181920212223#include &lt;iostream&gt;int main() { int a = 10; int b = 20; // 指针 int *p = &amp;a; std::cout &lt;&lt; &quot;Pointer value: &quot; &lt;&lt; *p &lt;&lt; std::endl; // 输出：Pointer value: 10 std::cout &lt;&lt; &quot;Address of value: &quot; &lt;&lt; p &lt;&lt; std::endl; // a的地址 std::cout &lt;&lt; &quot;Address of p: &quot; &lt;&lt; &amp;p &lt;&lt; std::endl; // p的地址 p = &amp;b; std::cout &lt;&lt; &quot;Pointer value: &quot; &lt;&lt; *p &lt;&lt; std::endl; // 输出：Pointer value: 20 // 引用 int &amp;r = a; std::cout &lt;&lt; &quot;Reference value: &quot; &lt;&lt; r &lt;&lt; std::endl; // 输出：Reference value: 10 std::cout &lt;&lt; &quot;Address of a: &quot; &lt;&lt; &amp;a &lt;&lt; std::endl; std::cout &lt;&lt; &quot;Address of r: &quot; &lt;&lt; &amp;r &lt;&lt; std::endl; // r = &amp;b; // 错误：引用不能被重新绑定 int &amp;r2 = b; r = r2; // 将 b 的值赋给 a，r 仍然引用 a std::cout &lt;&lt; &quot;Reference value: &quot; &lt;&lt; r &lt;&lt; std::endl; // 输出：Reference value: 20 return 0;} 引用是 C++ 编译器的约定，那和指针到底有什么区别？ 答：本质上没有区别 堆栈变量生命周期管理在 C++ 中创建变量一般有两种形式： 12A a;A a = new A(); // delete a; 1、静态建立类对象：编译器为对象在栈空间中分配内存，是通过直接计算 A 的空间，移动栈顶指针，挪出适当的空间，然后在这片内存空间上直接调用构造函数形成一个栈对象，在局部作用域退出时，自动调用类的析构函数； 2、动态建立类对象，使用 new 运算符将对象建立在堆空间。先在堆内存中搜索合适的内存并进行分配，再是调用构造函数构造对象，初始化堆内存，生命周期需要手动管理（delete）。 RAIIRAII 是 Resource Acquisition Is Initialization 的简称，其翻译过来就是“资源获取即初始化”，即在构造函数中申请分配资源，在析构函数中释放资源，它是 C++ 语言中的一种管理资源、避免泄漏的良好的设计方法。 C++ 语言的机制保证了，当创建一个类对象时，会自动调用构造函数，当对象超出作用域时会自动调用析构函数。RAII 正是利用这种机制，利用类来管理资源，将资源与类对象的生命周期绑定，即在对象创建时获取对应的资源，在对象生命周期内控制对资源的访问，使之始终保持有效，最后在对象析构时，释放所获取的资源。 12345678std::mutex mut;int write(std::string content) { mut.lock(); // critical area below (might throw exception) // Writing content to a file descriptor... // Critical areas above mut.unlock();} 上述展示了一个写函数，而在多线程并行调用下，内部可能共用的文件描述符必须用一个互斥锁保护，否则不同线程的字符串会混在一起。这段代码看起来没有问题，但是如果当写线程抛出了异常，调用栈会被直接释放，unlock 方法永远不会执行，造成死锁，其他的线程再也拿不到资源。 1234567std::mutex mut;int write(std::string content) { std::lock_guard&lt;std::mutex&gt; lock(mut); // critical area below (might throw exception) // Writing content to a file descriptor... // Critical areas above} lock_guard 保证在函数返回之后或者异常后释放互斥锁，因此无需担心异常情况下手动释放锁的问题。而 lock_guard 的实现，就是使用了“资源在构造函数中定义，在析构中释放的原则”。 123456789101112template &lt;typename T&gt;class lock_guard { public: explicit lock_guard(T &amp;mutex) : _mutex(mutex) { _mutex.lock(); } ~lock_guard() { _mutex.unlock(); } private: T _mutex;}; lock_guard 在构造函数中锁住了引用传入的资源（mutex），并且在析构函数中释放锁。其异常安全的保障就是析构函数一定会在对象归属的 scope 退出时自动被调用。 拷贝赋值&amp;构造拷贝构造函数是一种特殊构造函数，具有单个形参，该形参（常用 const 修饰）是对该类类型的引用。 拷贝赋值函数，也叫赋值操作符重载，因为赋值必须作为类成员，那么它的第一个操作数隐式绑定到 this 指针，赋值操作符接受单个形参，且该形参是同一类类型的对象。右操作数一般作为 const 引用传递。 问：为啥拷贝构造函数的形参必须是引用？ 1234567891011121314151617class Person { public: Person() {} // 默认构造函数 ~Person() {} // 析构函数 Person(const Person&amp; p) { // 拷贝构造函数 cout &lt;&lt; &quot;Copy Constructor&quot; &lt;&lt; endl; } // Persion(const Person&amp; p) = delete; Person&amp; operator=(const Person&amp; p) { // 拷贝赋值函数 cout &lt;&lt; &quot;Assign&quot; &lt;&lt; endl; return *this; } // Person&amp; operator=(const Person&amp; p) = delete; private: int age; string name;}; 拷贝构造函数和赋值运算符的行为比较相似，都是将一个对象的值复制给另一个对象，但是其结果却有些不同。拷贝构造函数使用传入对象的值生成一个新的对象的实例，而赋值运算符是将对象的值复制给一个已经存在的实例。具体来说，拷贝构造函数是构造函数，其功能就是创建一个新的对象实例；赋值运算符是执行某种运算，将一个对象的值复制给另一个对象（已经存在的）。 调用拷贝构造函数主要有以下场景： 对象作为函数的参数，以值传递的方式传给函数； 对象作为函数的返回值，以值的方式从函数返回； 使用一个对象给另一个对象初始化。 深拷贝&amp;浅拷贝 拷贝构造函数和赋值函数并非每个对象都会使用，另外如果不主动编写的话，编译器将以“位拷贝”的方式自动生成缺省的函数。在类的设计当中，“位拷贝”是应当防止的，可以通过“= delete”删除拷贝构造和拷贝复制函数。倘若类中含有指针变量, 并且这两个指针指向重叠的位置，那么这两个缺省的函数就会发生错误。 深拷贝和浅拷贝区分主要是针对类中的指针成员变量，因为对于指针只是简单的值复制并不能分割开两个对象的关联，任何一个对象对该指针的操作都会影响到另一个对象。这时候就需要提供自定义的深拷贝的拷贝构造函数，消除这种影响。通常的原则是： 指针成员应该提供自定义的拷贝构造函数； 在提供拷贝构造函数的同时实现自定义的赋值运算符。 对于拷贝构造函数的实现要确保以下几点： 对于值类型的成员进行值复制； 对于指针和动态分配的空间，在拷贝中应重新分配分配； 对于基类，要调用基类合适的拷贝方法，完成基类的拷贝。 移动赋值&amp;构造（见下文移动语义） 智能指针标准库中指针在经过多年发展后，不仅有耳熟能详的 share_ptr 和 unique_ptr 还多了一些其他的类型，为了对比，我这里也把普通指针和引用放在一起： T*: 内置指针类型，指向类型 T 的对象，或者指向类型 T 的连续内存空间序列； T&amp;：内置引用类型，引用类型 T 的对象（为别名），是一个隐式解引用的指针； unique_ptr：拥有 T 的所有权，指向 T 的独占指针，在离开作用域时，unique_ptr 的析构会销毁其指向的对象； shared_ptr：指向类型 T 对象的共享指针，所有权在所有指向类型 T 对象的 shared_ptr 之间共享，最后一个共享 T 对象的 shared_ptr 离开最后的作用域时负责析构销毁 T 对象； weak_ptr：指向 shared_ptr 拥有的对象，但是不引用计数，需要用 weak 访问对象时，需要提升为 shared_ptr 才可以； span：指向连续序列的 T 元素的指针，为 std::vector 等容器的“视图”； string_view：指向字符串子串的视图； X_iterator: 来自 C 容器的对象序列，“X”代表具体的迭代器类型（map、set…）。 unique_ptrC++ 智能指针详解（一）——unique_ptr shared_ptrC++ 智能指针详解（二）——shared_ptr 与 weak_ptr 值类型在 C++11 之前，很多 C++ 程序里存在大量的临时对象，又称无名对象。主要出现在如下场景： 函数的返回值 用户自定义类型经过一些计算后产生的临时对象 值传递的形参 C++11 之后，左值和右值分为了三个具体的子值类型，和两个混合类型，这里清晰起见，不对泛左值和右值展开。 左值具有以下特征： 可通过取地址运算符获取其地址； 可修改； 可用作内建赋值和内建符合赋值运算符的左操作数； 可以用来初始化左值引用。 举例： 12345678910111213int a = 1; // a是左值T&amp; f(); // 左值f();//左值++a;//左值--a;//左值int b = a;//a和b都是左值struct S* ptr = &amp;obj; // ptr为左值arr[1] = 2; // 左值int *p = &amp;a; // p为左值*p = 10; // *p为左值class MyClass{};MyClass c; // c为左值&quot;abc&quot; // 左值 将亡值可以理解为通过“盗取”其他变量内存空间的方式获取到的值。在确保其他变量不再被使用、或即将被销毁时，通过“盗取”的方式可以避免内存空间的释放和分配，能够延长变量值的生命期。 将亡值只能通过两种方式来获得，这两种方式都涉及到将一个左值赋给(转化为)一个右值引用： 返回右值引用的函数的调用表达式，static_cast&lt;T&amp;&amp;&gt;(T); 转换为右值引用的转换函数的调用表达式，std::move(t)。 12345std::string fun() { std::string str = &quot;test&quot;; return str;}std::string s = fun(); C++11 之前，s = fun()会调用拷贝构造函数，会将整个 str 复制一份，然后再把 str 销毁。str 比较大则会造成巨大开销。一旦 str 被 s 复制后，将被销毁，无法获取和修改。 C++11 后，引入了 move 语义，编译器会将这部分优化成 move 操作，str 会被进行隐式右值转换，等价于 static_caststd::string&amp;&amp;(str)，进而此处的 s 会将 func 局部返回的值进行移动。 纯右值本身就是纯粹的字面值，如 3，false，12.13，或者求值结果相当于字面值或是一个不具名的临时对象。 具体来说： 纯右值不具有多态：它所标识的对象的动态类型始终是该表达式的类型； 纯右值不能具有不完整类型； 纯右值不能具有抽象类类型或它的数组类型。 左值引用和右值引用12345int a = 1;int&amp; b = a; // before c++11// int&amp;&amp; b = a; // error 不能引用左值int&amp;&amp; b = std::move(a); // c++11 左值引用使用“&amp;”标记，右值引用使用“&amp;&amp;”标记。具体来说，在 C++11 之前的引用，都是左值引用，而： 在语句执行完毕之后被销毁的临时对象； std::move()后的非 const 对象 考虑下列代码，应该使用哪个 foo 函数的重载版本： 12345void foo(int&amp;); // 1void foo(int&amp;&amp;); // 2int&amp;&amp; value = 42;foo(value); 答案是使用“1”的函数。虽然这里的变量定义的是右值引用类型，然后 foo(value)中的表达式 value 确是一个左值，而不是由定义 value 时的类型来决定值类型。简单来说，如果表达式能取地址，则为左值表达式，否则为右值表达式。 根据引用和常量性质进行组合，有几种情况： 左值引用 Value&amp;，只能绑定左值表达式，如 1 中形参； 右值引用 Value&amp;&amp;，只能绑定右值表达式，如 2 中形参； 左值常引用 const Value&amp;，可以绑定左、右值表达式，但是后续无法修改值； 右值常引用 const Value&amp;&amp;，只能绑定常量右值表达式，实际中不使用。 问：如何将上述代码中 value 以右值引用的形式传递给 foo，从而调用“2”的函数，两个方法：一是直接 foo(42)，二是通过 foo(static_cast&lt;int&amp;&amp;&gt;(value))。 编译器会匹配为右值引用。 让编译器将对象匹配为右值引用，是移动语义的基础！！！ 移动语义先来说明为什么需要移动语义： case1： 1234567891011class Stuff { public: Stuff(const std::string &amp;s) : str{s} {}; private: std::string str;};std::vector&lt;Stuff&gt; stuff;Stuff tmp{&quot;hello&quot;};stuff.push_back(tmp);stuff.push_back(tmp); 这是一个较为常见的开发 case，创建了一个容器 std::vector 以及自定义类 Stuff，并且添加到容器中两次，注意我们这里并没有对 Stuff 类自定义拷贝构造和拷贝赋值，使用编译器默认实现。tmp 添加到容器中两次，每次添加时都会发生一次拷贝操作，最终内存结构可能是： tmp 对象在添加到容器中两次后，生命周期随之结束。 case2： 123456789101112std::string process1(const std::string&amp; str) { return process2(str);}std::string process2(const std::string&amp; str) { return process3(str);}std::string process3(const std::string&amp; str) { return process4(str);}// ... 回到 case1，现在修改容器的操作为下面的代码： 1234std::vector&lt;Stuff&gt; stuff;Stuff tmp{ &quot;hello&quot; };stuff.push_back(tmp); stuff.push_back(std::move(tmp); 现在可以明确，移动操作执行对象数据转移，拷贝操作复制对象数据。为了能够将拷贝操作与移动操作区分执行，需要不同于拷贝的标记，“&amp;&amp;”应运而生。 在不考虑模板的情况下，针对两种 push_back 函数需要有两种重载实现： 12345678class vector { public: // 上文说到const T&amp; 也是可以绑定右值引用， // 但是因为有限制，只能后续无法修改， // 所以提供也需要提供&amp;&amp;的实现 void push_back(const Stuff&amp; value){} void push_back(Stuff&amp;&amp; value) {}}; 通过传递左值引用或右值引用，根据需要调用不同的 push_back 重载函数。","link":"/2024/03/10/Modern-cpp-1/"},{"title":"NVIDIA PTX 简单入门","text":"先看代码： 123456789101112131415161718192021222324target datalayout = &quot;e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64&quot;target triple = &quot;nvptx64-nvidia-cuda&quot;declare i32 @llvm.nvvm.read.ptx.sreg.tid.x() nounwind readnonedefine void @kernel(ptr addrspace(1) %A, ptr addrspace(1) %B, ptr addrspace(1) %C) {entry: %id = call i32 @llvm.nvvm.read.ptx.sreg.tid.x() %ptrA = getelementptr inbounds float, ptr addrspace(1) %A, i32 %id %ptrB = getelementptr inbounds float, ptr addrspace(1) %B, i32 %id %ptrC = getelementptr inbounds float, ptr addrspace(1) %C, i32 %id %valA = load float, ptr addrspace(1) %ptrA, align 4 %valB = load float, ptr addrspace(1) %ptrB, align 4 %valC = fadd float %valA, %valB store float %valC, ptr addrspace(1) %ptrC, align 4 ret void}!nvvm.annotations = !{!0}!0 = !{ptr @kernel, !&quot;kernel&quot;, i32 1} 这个kernel.ll文件是LLVM IR（Intermediate Representation）表示的CUDA核函数，专为NVIDIA PTX（Parallel Thread Execution）架构生成。以下是逐行解释： 逐行解释 目标架构定义 1target datalayout = &quot;e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64&quot; 作用：定义数据的内存布局规则。 关键参数： e: 小端字节序 p:64:64:64: 指针占64位，对齐为64位 i1:8:8: 1位整数对齐为8位 f32:32:32: 单精度浮点数对齐为32位 v16:16:16: 16位向量对齐为16位 n16:32:64: 本地指针大小（用于GPU架构） 1target triple = &quot;nvptx64-nvidia-cuda&quot; 作用：指定目标架构为NVIDIA PTX 64位架构，用于CUDA设备。 内置函数声明 1declare i32 @llvm.nvvm.read.ptx.sreg.tid.x() nounwind readnone 作用：声明PTX内置函数，用于读取线程在X维度的ID（类似CUDA的threadIdx.x）。 特性： nounwind: 保证不会抛出异常; readnone: 函数无副作用，输出仅依赖输入; 核函数定义 123define void @kernel(ptr addrspace(1) %A, ptr addrspace(1) %B, ptr addrspace(1) %C) 作用：定义名为kernel的核函数，接受三个全局内存指针参数。 关键点： addrspace(1): 表示指针指向全局内存（GPU显存） 对比CUDA中的参数类型：float* A → ptr addrspace(1) %A 入口基本块（Entry Block） 12entry: %id = call i32 @llvm.nvvm.read.ptx.sreg.tid.x() 作用：获取当前线程在X维度的ID，存入%id寄存器。 123%ptrA = getelementptr inbounds float, ptr addrspace(1) %A, i32 %id%ptrB = getelementptr inbounds float, ptr addrspace(1) %B, i32 %id%ptrC = getelementptr inbounds float, ptr addrspace(1) %C, i32 %id 作用：计算每个线程访问的全局内存地址。 语法： getelementptr inbounds: 计算数组元素地址（类似&amp;A[threadIdx.x]） float: 元素类型 ptr addrspace(1) %A: 基地址 i32 %id: 偏移量 12%valA = load float, ptr addrspace(1) %ptrA, align 4%valB = load float, ptr addrspace(1) %ptrB, align 4 作用：从全局内存加载数据到寄存器。 参数： align 4: 确保4字节对齐访问（优化内存访问） 1%valC = fadd float %valA, %valB 作用：执行浮点数加法运算。 12store float %valC, ptr addrspace(1) %ptrC, align 4ret void 作用：将结果写回全局内存，然后返回。 关键点： store指令的内存地址必须指定正确的地址空间（addrspace(1)） 元数据注解 12!nvvm.annotations = !{!0}!0 = !{ptr @kernel, !&quot;kernel&quot;, i32 1} 作用：标记@kernel函数为CUDA核函数。 参数： ptr @kernel: 函数指针; !”kernel”: 标记类型为核函数; i32 1: 参数（通常表示核函数版本或特性）; 这个核函数的行为等价于以下CUDA代码： 1234__global__ void kernel(float*A, float* B, float* C) { int id = threadIdx.x; C[id] = A[id] + B[id];} 每个线程负责将全局内存中A和B对应位置的值相加，结果写入C的相同位置。 关键概念补充 地址空间： addrspace(1): 全局内存（显存） addrspace(3): 共享内存 addrspace(5): 常量内存 PTX寄存器访问： tid.x对应threadIdx.x 类似还有ctaid.x（blockIdx.x）、ntid.x（blockDim.x）等 LLVM IR特性： 静态单赋值（SSA）形式 强类型系统 显式内存地址空间管理 再看用法将上文的LLVM IR代码编译为NVIDIA PTX 后端代码，命令为： 1llc-15 -march=nvptx64 -mcpu=sm_80 kernel.ll -o kernel.ptx llc-15作用：LLVM 静态编译器工具,用于将 LLVM IR 代码编译为目标平台的汇编代码或二进制格式。此处用于生成 NVIDIA PTX 代码。 -march=nvptx64作用：指定目标架构为 NVIDIA PTX 64 位，nvptx64 是 NVIDIA 的 Parallel Thread Execution (PTX) 虚拟指令集架构，专为 64 位 GPU 设计。PTX 代码可在支持该架构的 NVIDIA GPU 上运行（需进一步编译为实际 GPU 指令）。 -mcpu=sm_80作用：指定目标 GPU 的计算能力版本。 sm_80 对应 NVIDIA Ampere 架构（如 A100、RTX 3090 等）。 sm 表示 “Streaming Multiprocessor”，数字 80 代表计算能力 8.0。此选项确保生成的 PTX 代码针对该架构优化。 kernel.ll作用: 此文件通常由 Clang 或其他 LLVM 前端生成，包含高级语言（如 C/C++、CUDA 等）编译后的中间代码。 -o kernel.ptx作用：输出的 PTX 文件是 NVIDIA GPU 可读的中间代码，后续可通过 NVIDIA 驱动或工具（如 nvcc）进一步编译为实际 GPU 指令（SASS，见下文）。 PTX 生成代码123456789101112131415161718192021222324252627282930313233343536//// Generated by LLVM NVPTX Back-End//.version 7.0.target sm_80.address_size 64 // .globl kernel // -- Begin function kernel // @kernel.visible .entry kernel( .param .u64 kernel_param_0, .param .u64 kernel_param_1, .param .u64 kernel_param_2){ .reg .b32 %r&lt;2&gt;; .reg .f32 %f&lt;4&gt;; .reg .b64 %rd&lt;8&gt;;// %bb.0: // %entry ld.param.u64 %rd1, [kernel_param_0]; ld.param.u64 %rd2, [kernel_param_1]; mov.u32 %r1, %tid.x; ld.param.u64 %rd3, [kernel_param_2]; mul.wide.s32 %rd4, %r1, 4; add.s64 %rd5, %rd1, %rd4; add.s64 %rd6, %rd2, %rd4; add.s64 %rd7, %rd3, %rd4; ld.global.f32 %f1, [%rd5]; ld.global.f32 %f2, [%rd6]; add.rn.f32 %f3, %f1, %f2; st.global.f32 [%rd7], %f3; ret; // -- End function} SASSSASS（Streaming ASSembly）是 NVIDIA GPU 的实际底层机器码（二进制指令集），直接由 GPU 硬件执行。它是 PTX 代码经过进一步编译后的最终产物，与 PTX 的关系类似于 CPU 汇编代码和中间语言（如 Java 字节码）的关系。 SASS 的关键特性 二进制格式： SASS 是 GPU 硬件直接执行的二进制指令，不可读 PTX 是文本格式的中间代码，人类可读（但一般由编译器生成） 硬件绑定： SASS 直接对应具体 GPU 架构（如 Ampere、Ada Lovelace、Hopper）。 不同架构的 SASS 不兼容（例如 sm_80 的 SASS 无法在 sm_70 的 GPU 上运行）。 性能优化 SASS 经过 NVIDIA 驱动或工具（如 nvcc）的优化，包含特定 GPU 的指令调度、寄存器分配等。 PTX 是通用中间表示，需进一步编译为 SASS 才能高效执行。 隐蔽性： NVIDIA 未公开 SASS 的完整指令集和编码规范，普通开发者通常无需直接操作 SASS。 PTX 与 SASS 的编译流程1234567高级语言（如 CUDA C++） ↓ 编译（nvcc/clang）LLVM IR（.ll 文件） ↓ 编译（llc）PTX 代码（.ptx 文件） ↓ 运行时编译（NVIDIA 驱动）或离线编译（nvcc）SASS 机器码（二进制，GPU 直接执行） PTX 是跨 GPU 架构的中间层代码（类似虚拟指令集）; SASS 是最终在 GPU 上运行的机器码（绑定具体架构）; 如何查看 SASS 代码？ 使用 cuobjdump： 1cuobjdump -sass compiled_gpu_binary.cubin 输出 SASS 指令的文本表示（例如指令操作码、寄存器分配）。 使用 nvcc 生成： 1nvcc --keep --gpu-architecture=sm_80 -c code.cu 保留中间文件（如 .sass 或 .cubin）。 Nsight Compute： NVIDIA 官方工具，可分析 SASS 指令的执行效率和资源使用。 假设一段简单的加法操作，PTX 代码： 123456789101112.version 7.8.target sm_80.global .func(.param.b64 %out), add( .param.b64 %a, .param.b64 %b) { ld.param.u64 %r1, [%a]; ld.param.u64 %r2, [%b]; add.u64 %r3, %r1, %r2; st.param.b64 [%out], %r3; ret;} 对应的SASS 代码为： 12IADD R1, R2, R3;MOV R4, R1; PTX 怎么用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119#include &lt;iostream&gt;#include &lt;fstream&gt;#include &lt;cassert&gt;#include &lt;cuda.h&gt;void checkCudaErrors(CUresult err) { assert(err == CUDA_SUCCESS);}/// main - Program entry pointint main(int argc, char **argv) { CUdevice device; CUmodule cudaModule; CUcontext context; CUfunction function; CUlinkState linker; int devCount; // CUDA initialization checkCudaErrors(cuInit(0)); checkCudaErrors(cuDeviceGetCount(&amp;devCount)); checkCudaErrors(cuDeviceGet(&amp;device, 0)); char name[128]; checkCudaErrors(cuDeviceGetName(name, 128, device)); std::cout &lt;&lt; &quot;Using CUDA Device [0]: &quot; &lt;&lt; name &lt;&lt; &quot;\\n&quot;; int devMajor, devMinor; checkCudaErrors(cuDeviceComputeCapability(&amp;devMajor, &amp;devMinor, device)); std::cout &lt;&lt; &quot;Device Compute Capability: &quot; &lt;&lt; devMajor &lt;&lt; &quot;.&quot; &lt;&lt; devMinor &lt;&lt; &quot;\\n&quot;; if (devMajor &lt; 2) { std::cerr &lt;&lt; &quot;ERROR: Device 0 is not SM 2.0 or greater\\n&quot;; return 1; } std::ifstream t(&quot;kernel.ptx&quot;); if (!t.is_open()) { std::cerr &lt;&lt; &quot;kernel.ptx not found\\n&quot;; return 1; } std::string str((std::istreambuf_iterator&lt;char&gt;(t)), std::istreambuf_iterator&lt;char&gt;()); // Create driver context checkCudaErrors(cuCtxCreate(&amp;context, 0, device)); // Create module for object checkCudaErrors(cuModuleLoadDataEx(&amp;cudaModule, str.c_str(), 0, 0, 0)); // Get kernel function checkCudaErrors(cuModuleGetFunction(&amp;function, cudaModule, &quot;kernel&quot;)); // Device data CUdeviceptr devBufferA; CUdeviceptr devBufferB; CUdeviceptr devBufferC; checkCudaErrors(cuMemAlloc(&amp;devBufferA, sizeof(float)*16)); checkCudaErrors(cuMemAlloc(&amp;devBufferB, sizeof(float)*16)); checkCudaErrors(cuMemAlloc(&amp;devBufferC, sizeof(float)*16)); float* hostA = new float[16]; float* hostB = new float[16]; float* hostC = new float[16]; // Populate input for (unsigned i = 0; i != 16; ++i) { hostA[i] = (float)i; hostB[i] = (float)(2*i); hostC[i] = 0.0f; } checkCudaErrors(cuMemcpyHtoD(devBufferA, &amp;hostA[0], sizeof(float)*16)); checkCudaErrors(cuMemcpyHtoD(devBufferB, &amp;hostB[0], sizeof(float)*16)); unsigned blockSizeX = 16; unsigned blockSizeY = 1; unsigned blockSizeZ = 1; unsigned gridSizeX = 1; unsigned gridSizeY = 1; unsigned gridSizeZ = 1; // Kernel parameters void *KernelParams[] = { &amp;devBufferA, &amp;devBufferB, &amp;devBufferC }; std::cout &lt;&lt; &quot;Launching kernel\\n&quot;; // Kernel launch checkCudaErrors(cuLaunchKernel(function, gridSizeX, gridSizeY, gridSizeZ, blockSizeX, blockSizeY, blockSizeZ, 0, NULL, KernelParams, NULL)); // Retrieve device data checkCudaErrors(cuMemcpyDtoH(&amp;hostC[0], devBufferC, sizeof(float)*16)); std::cout &lt;&lt; &quot;Results:\\n&quot;; for (unsigned i = 0; i != 16; ++i) { std::cout &lt;&lt; hostA[i] &lt;&lt; &quot; + &quot; &lt;&lt; hostB[i] &lt;&lt; &quot; = &quot; &lt;&lt; hostC[i] &lt;&lt; &quot;\\n&quot;; } // Clean up after ourselves delete [] hostA; delete [] hostB; delete [] hostC; // Clean-up checkCudaErrors(cuMemFree(devBufferA)); checkCudaErrors(cuMemFree(devBufferB)); checkCudaErrors(cuMemFree(devBufferC)); checkCudaErrors(cuModuleUnload(cudaModule)); checkCudaErrors(cuCtxDestroy(context)); return 0;} 这里是代码的简单行为： 初始化 CUDA 环境： 检测 GPU 设备，验证计算能力； 加载 kernel.ptx 文件。 准备数据： 在 GPU 上分配内存（devBufferA, devBufferB, devBufferC）； 在 CPU 上初始化输入数据 hostA 和 hostB（值分别为 0, 1, 2, …, 15 和 0, 2, 4, …, 30）。 将数据从 CPU 拷贝到 GPU。 启动 GPU 核函数： 调用 kernel 函数，执行 A + B = C 的加法操作； 核函数使用 1 个线程块，包含 16 个线程（每个线程处理一个元素）。 验证结果： 将 GPU 计算结果 devBufferC 拷贝回 CPU 的 hostC； 打印 hostA[i] + hostB[i] = hostC[i]，验证结果是否正确。 清理资源 相关输出为： 1234567891011121314151617181920Using CUDA Device [0]: NVIDIA H100 80GB HBM3Device Compute Capability: 9.0Launching kernelResults:0 + 0 = 01 + 2 = 32 + 4 = 63 + 6 = 94 + 8 = 125 + 10 = 156 + 12 = 187 + 14 = 218 + 16 = 249 + 18 = 2710 + 20 = 3011 + 22 = 3312 + 24 = 3613 + 26 = 3914 + 28 = 4215 + 30 = 45","link":"/2025/02/01/PTX-1/"},{"title":"Sigmod 论文《Revisiting the Design of In-Memory Dynamic Graph Storages》阅读","text":"本论文来自上海交大，论文主要关于内存中动态图存储（In-Memory Dynamic Graph Storage, DGS）设计的研究论文，论文核心是通过系统评估现有 DGS 方法，揭示其性能瓶颈并指明未来优化方向。动态图存储需支持并发读写，以满足实时图分析和更新需求。现有方法（如 LLAMA、Aspen、LiveGraph、Teseo、Sortledton 等）在读写支持、空间开销和并发控制上差异显著，但缺乏对这些维度权衡的系统研究。作者主要分析了现有系统的设计，以及讨论了如何针对 DGS 算法进行系统评估。 本文阅读该论文的重点一是仔细学习作者类比、分析、抽象系统的做法，二是借作者之手，对现在的 DGS 系统的发展做一个简单的了解。论文链接 Revisiting the Design of In-Memory Dynamic Graph Storages 现有 DGS 方法的核心差异体现在图容器（存储结构）和并发控制（版本管理与协议）两方面： Method Version Management Protocol Vertex Index Container Neighbor Index Container Additional Optimization LiveGraph Fine-Grained with Continuous Version S2PL Dynamic Array Dynamic Array Bloom Filter Sortledton Fine-Grained with Version Chain G2PL Dynamic Array Segmented Skip List Adaptive Indexing Teseo Fine-Grained with Version Chain OCC Hash Table PMA Write-Optimized Segment Aspen Coarse-Grained Single Writer AVL Tree Segmented PAM Vertex Index Flatten &amp; Data Encoding LLAMA Coarse-Grained Single Writer Dynamic Array Dynamic Array - 省流结论作者通过测试图容器效率、并发控制粒度及内存消耗，得出以下结论： 空间开销大：现有 DGS 方法内存消耗显著高于静态存储（如 CSR）。例如，Aspen 比 CSR 高 3.3-10.8 倍，最优细粒度锁方法高 4.1-8.9 倍； 硬件利用不足：现有方法忽略现代架构的内存访问特性，分段存储导致缓存 Cache misses 增加、分支预测错误率上升，性能低于连续存储； 细粒度锁并发控制的问题：能提升写吞吐量，但因维护每个邻接的版本和锁检查，导致效率和空间开销大，且高 degree 顶点存在严重锁竞争； CSR 仍占优：静态存储 CSR 在查询性能（高 2.4-11.0 倍）和内存消耗上显著优于现有 DGS 方法； 所以作者认为接下来的发展方向应该是： 优化版本管理：减少细粒度锁版本的空间和效率开销，探索更粗粒度锁的版本控制； 提升硬件利用率：优化内存布局以减少缓存 misses 和分支错误，结合向量化等硬件特性； 改进并发控制：降低读写干扰，缓解高 degree 顶点的锁竞争，提升插入可扩展性； 图表示静态图与动态图：静态图可表示为邻接表（AdjLst）或压缩稀疏行（CSR）格式，其中 CSR 因紧凑的内存布局和高效的访问性能，成为静态图最常用的存储方式。动态图则通过初始图$G_0$和一系列更新$\\Delta G$（包括顶点 / 边的插入或删除）来记录图的演化，每次更新后形成新的图版本$G_i$。 顶点与边的表示：顶点 ID 被限定在$[0, |V|)$的整数范围内，以提高计算和存储效率，若输入 ID 不符合该范围，可通过字典编码映射。 CSR 格式CSR需要三个数组表示矩阵结构 V = [] # 存储非零元素的值: COL_INDEX = [] # 存储每个非零元素所在的列号 ROW_INDEX = [] # 存储每行首个非零元素在V中的位置 要获取第row行的非零元素： 计算起始位置：a = ROW_INDEX[row] 计算结束位置：b = ROW_INDEX[row+1] 该行元素在V中的区间为：V[a : b] 对应的列号在：COL_INDEX[a : b] 📌 重要特性：元素V[k]的行号=row，列号=COL_INDEX[k]其中 k ∈ [ROW_INDEX[row], ROW_INDEX[row+1]) 顶点集的数据结构 Data Structure INSERT SEARCH SCAN RESIZE Space Dynamic Array $O(1)$ $O(n)$ $O(n)$ $Θ(n)$ $O(n)$ Sorted Dynamic Array $O(n)$ $O(log n)$ $O(n)$ $Θ(n)$ $O(n)$ Packed Memory Array $O(log² n)$ $O(log n)$ $O(n)$ $Θ(n)$ $O(n)$ Skip List $O(log n)$ $O(log n)$ $O(n)$ - $O(n log n)$ Hash Table $O(1)$ $O(1)$ $O(n)$ - $O(n)$ AVL Tree $O(log n)$ $O(log n)$ $O(n)$ - $O(n)$ Parallel Augmented Map $O(log n)$ $O(log n)$ $O(n)$ - $O(n)$ 这里再来复习一下上述数据结构中的常见CURD时间复杂度，其中： 动态数组：支持快速追加插入$O(1)$和连续扫描，但搜索需遍历(O(n))，扩容时需复制元素(\\Theta(n))； 排序动态数组：通过二分查找优化搜索(O(\\log n))，但插入需移动元素(O(n))； 其他结构：PMA平衡了插入和搜索效率；跳表和 AVL 树在插入、搜索上均为(O(\\log n))，但跳表空间开销较高(O(n \\log n))；哈希表支持常数级插入和搜索，但扫描效率低(O(n))。 动态图抽象模型抽象模型作者从 “图查询与数据抽象” 和 “图操作抽象” 两个维度，揭示了动态图存储的核心逻辑，先来看“图查询和数据抽象”： 这张图展示了 DGS 的多层抽象模型，包括 Global Abstraction和 Local Abstraction，用于统一建模图的查询时序、数据结构及版本管理逻辑： Global Abstraction核心逻辑：通过 “全局时间戳”（(t(G))）协调读（Q）与写（(\\Delta G)）的关系，确保并发操作的可串行化。全局时间戳(t(G))初始为 0，仅在写提交时递增（如(\\Delta G_1)在(t(G)=1)提交，(\\Delta G_2)在(t(G)=2)提交）。读Q的 “本地时间戳”（(t(Q))）等于其开始时的全局时间戳（如(Q_1)在(t(G)=49)开始，(t(Q_1)=49)），仅能访问时间戳≤(t(Q))的版本数据。 Local Abstraction核心逻辑：将图数据拆解为 “顶点表”（(V(G))）和 “邻接表”（(N(u))），每个条目记录版本信息，体现数据的动态演化。具体的： 顶点表：每个顶点条目包含 “顶点 ID”、“时间戳”、“操作类型”（I = 插入 / D = 删除）“属性”“邻接表地址”； 邻接表：每个邻接条目包含 “邻接 ID”、“时间戳”、“操作类型”、“属性”，记录边的版本变化（如(v_3)在(t=1)插入、(t=50)删除、(t=99)再次插入）。 这张图展示了各类图操作（如插入顶点、插入边、查询边、扫描邻接等）的执行流程，揭示了操作之间的依赖关系及底层数据访问路径。核心逻辑所有操作始于 “START” 节点，通过 “顶点表操作”（(P_V)）和 “邻接表操作”（(P_N)）的组合完成，路径中包含 “查找”、“插入”、“扫描” 等基本操作。 比如：插入边（((u, v))）需先 “搜索顶点u是否存在”（((u))），再 “搜索v是否在(N(u))中”（((u, v))），最后 “插入v到(N(u))”。扫描邻接（((u))）需先 “搜索顶点u”（((u))），再 “扫描(N(u))的所有条目”。 LiveGraph具体应用正如作者上述抽象的所述，LiveGraph 每个写（如插入 / 删除边）提交时会更新全局时间戳，并为操作的顶点 / 邻边创建新的版本（标记 begin-ts 为当前全局时间戳）；读（如扫描邻边）仅访问 begin-ts≤自身本地时间戳且 end-ts=INF（未删除）的版本，符合Global Abstraction的时序协调逻辑。 而LiveGraph 为每个顶点和邻边维护 “begin-ts”和 “end-ts”，与Local Abstraction中的 “时间戳” 和 “操作类型” 对应。例如，邻接表中(v_3)的多个版本（(t=1)插入、(t=50)删除、(t=99)插入），正是 LiveGraph 通过连续版本存储实现动态更新的具体表现。 LiveGraph 的操作流程也符合操作的抽象路径： 插入边：依赖动态数组的 “搜索”（遍历邻边表检查v是否存在，辅以布隆过滤器优化）和 “追加插入”（在 DA 末尾添加新版本），对应上图 中 InsEdge 的路径。 扫描邻边：直接遍历动态数组的连续存储，从末尾向开头访问（优先最新版本）； 并发控制：LiveGraph 对每个顶点加锁（S2PL），确保操作的线程安全。 作者从 “时间戳协调” 和 “版本化数据结构” 角度，结合LiveGraph解释了一般动态图如何通过细粒度锁局部时间/版本（begin-ts/end-ts）和全局时间戳实现并发读写；并结合 LiveGraph 的动态数组，解释了如何依赖S2PL支持插入、扫描等操作。 目前主流动态图的做法现有DGS方法主要致力于优化两个问题： 图并发控制，包括版本管理和协调并发图查询执行的并发控制协议，以及最小化每个图操作的开销； 图容器，包括顶点索引、邻边索引和其他内存数据结构优化措施; Live Graph 图并发控制：LiveGraph 为顶点集(V(G))中的每个顶点配备一个锁，并采用S2PL来同步数据访问。对于写(\\Delta G)，LiveGraph 首先获取(\\Delta V)（(\\Delta G)中涉及的顶点）中所有顶点的排他锁，执行图操作后再释放这些锁。为处理死锁，若在时限内无法获取锁，LiveGraph 会中止(\\Delta G)。如上图所示，LiveGraph 为每个邻边或顶点的版本记录开始和结束时间戳（(begin-ts)和(end-ts)），以记录其生命周期。对于在时间戳i提交的(\\Delta G)，插入边（InsEdge((u, v))）会查找(N(u))中v的最新版本。若找到，将其(end-ts)设为i，并创建一个(begin-ts = i)且(end-ts = INF)的v的新版本；若未找到，则直接创建v的新版本。删除边操作（DelEdge((u, v))）会查找v的最新版本并将其(end-ts)设为i。对于读Q中的操作，LiveGraph 会获取顶点的共享锁，并在操作完成后立即释放。例如，遍历邻边操作（((u))）获取u的锁，基于时间戳访问邻边后立即释放锁。因此，Q不会导致死锁，因为它从不同时持有两个锁。 图容器：对于(u \\in V(G))，LiveGraph 使用动态数组作为(N(u))的邻边索引，其中每个元素对应(v \\in N(u))的一个物理版本。由于动态数组的存储是连续的且不存在版本链，遍历操作（Scan）速度很快。此外，LiveGraph 从动态数组的末尾向开头执行遍历，因为最新元素可能比旧元素更频繁地被访问。因为动态数组是未排序的，需要通过遍历执行搜索，所以插入边也较慢，因为它依赖于查找边操作。为缓解这一问题，LiveGraph 为每个(N(u))维护一个布隆过滤器，用于记录元素是否存在于(N(u))中。LiveGraph 的顶点索引(V(G))也使用动态数组。由于顶点 ID 的范围是([0, |V|))，索引u处的元素即为顶点u，因此顶点索引的查找操作时间复杂度为(O(1))。 Sortledton 图并发控制：Sortledton 同样使用S2PL，但对加锁顺序进行了优化: 将(\\Delta V)中的顶点按顶点 ID 升序排序，并按该顺序获取它们的排他锁。这种优化通过避免写之间的循环等待来防止死锁，因此 Sortledton 不需要任何处理死锁的机制。对于在时间戳i提交的(\\Delta G)中的插入边（InsEdge((u, v))）（或删除边操作（DelEdge((u, v))）），Sortledton 会创建一个时间戳为i、操作类型为 I（或 D）的v的新版本，如上图所示。Sortledton 为v的不同版本维护一个版本链，新版本指向旧版本。对于读，Sortledton 采用与 LiveGraph 相同的并发控制策略。 图容器：与 LiveGraph 类似，Sortledton 使用动态数组作为顶点索引。对于邻边索引，Sortledton 将(N(u))划分为块B，并使用跳表作为链接这些块的块索引。每个块的填充率维持在 50% 到 100% 之间。当一个块满时，Sortledton 将其分裂为两个块，平均分配邻边；若填充率低于 50%，则将其与相邻块合并。每个块的第一个元素作为其在跳表中的键，这种结构称为分段跳表。读操作通过遍历版本链查找目标版本。而真实世界的图通常是稀疏的，Sortledton 也给了一种优化手段：自适应邻边索引：若(N(u))的大小低于阈值（如 256），则使用排序动态数组作为邻边索引，而非分段跳表。 Teseo 图并发控制：Teseo 采用与 Sortledton 相同的版本管理方法，但采用OCC来协调并发写。与 LiveGraph 和 Sortledton 为每个顶点配备一个锁不同，Teseo 为每个边分区维护一个锁，以同步并发数据访问。具体的，Teseo 将(E(G))逻辑划分为大小相等的分区，每个分区配备一个锁。写在访问分区中的数据前，会获取该分区的排他锁（读则获取共享锁），并在访问后立即释放。 图容器：如上图所示，Teseo 使用PMA作为邻边索引，但将所有邻接表存储在单个 PMA 中。若(N(u))较大，它会跨越 PMA 中的多个块；而多个小邻边集可以共享一个块。然而，若将(E(G))存储在一起，PMA 的全局重平衡开销会很大。为解决这一问题，Teseo 将单个 PMA 划分为多个大叶子节点（每个几兆字节），并使用ART为这些叶子节点建立索引，将这种数据结构称为 FAT。除此之外，Teseo 使用哈希表作为顶点索引，记录每个顶点的邻边索引在 FAT 中的位置。默认情况下，FAT 中的块是排序的，为读优化段；当插入率较高时，FAT 会切换到写优化段，通过追加到更新日志来处理更新。对于写优化段，Teseo 通过遍历段来查找目标顶点。 Teseo的做法有点类似lucene中的ART状态机的做法，并且切换读优化段和写优化段的做法在很多数据库中都见到过。 PMAPMA 的核心思想是​​在物理连续的内存块中，有意地预留一些“间隙”​​。这些间隙分布在数组元素之间。​​PMA 中的元素并不是像传统数组那样一个紧挨着一个。它们之间散布着一些空的槽位（Gaps）。这些间隙并不是随机分布的。它们通常按照某种策略（例如，基于元素数量或位置）分布在数据块中。一种常见的策略是让间隙的密度随着位置的变化而变化（例如，在数据块的开头和结尾间隙较少，中间较多），或者均匀分布。 ​​插入操作：​​当需要插入一个新元素时，算法会找到一个​​目标附近​​的间隙来放置它。如果附近没有合适的间隙，或者间隙密度低于某个阈值，PMA 不会立即移动所有元素。相反，它会进行一种称为 ​​“重新平衡”​​ 的操作。 ​​重新平衡：​​ 这个操作会扫描数据块中的一个​​局部区域​​（比整个数组小得多），将该区域内的所有元素（包括新插入的）和现有的间隙重新整理，使它们在该区域内均匀分布（或者满足预设的间隙密度规则）。这个操作的成本分摊到多次插入中，使得​​单次插入的均摊时间复杂度可以达到 $O(log n)$​​。 ​​删除操作：​​删除一个元素时，简单地将其标记为“间隙”（或者用一个特殊值标记）。如果某个区域的间隙密度变得过高（超过阈值），也会触发对该区域的​​重新平衡​​操作，将间隙移除或重新分布，使数据更紧凑。 ​查找操作：​​由于数据在物理内存上仍然是​​连续或近似连续​​的（间隙是分布其中的），PMA 支持高效的​​二分查找​​。查找算法需要跳过遇到的间隙（例如，通过记录元素的实际位置索引或使用特殊的“间隙”标记值）。 Aspen 图并发控制：Aspen 采用粗粒度锁策略，为每个图快照维护时间戳。具体的，Aspen 使用单写多读，写串行执行，允许多个读并发执行。写(\\Delta G_i)使用CoW方法在图的副本上应用更新，创建新的快照(G_i)。如上图所示，读Q在最新的图快照上执行，所以读和写从不相互阻塞，多个读可以共享同一个图快照。 图容器：Aspen 将(N(u))划分为一组排序的块，为这些块建立索引。插入边u时会复制块，然后复制从该块到根的路径，目的是创建(N(v))的新快照。给定(N(u))和块大小B，Aspen 选择满足(v \\mod B = 0)的顶点作为头部，即(heads = {v \\in N(u) | v \\mod B = 0})。这种方法确保对一个块的更新不会影响相邻块。Aspen 使用 AVL 树作为顶点索引，并为每个更新操作复制树中的路径。 Aspen 提出了两种优化方法来提升性能：第一，对于长时间运行的读，Aspen 可以创建一个基于 AVL 树的数组，存储每个邻接表的位置，以消除 AVL 树中的查找开销；第二，Aspen 使用差分编码方案压缩块中的数据，并使用字节码进行压缩，以加速集合交集运算。 LLAMALLAMA 于 2015 年提出，采用与 Aspen 类似机制。具体而言，LLAMA 将顶点表划分为多个分区，每个分区存储在一个数据页中，并维护一个Hash表来存储这些页的位置。每个写必须复制Hash表以创建新的图快照，这种复制开销限制了更新性能和图的可扩展性。 对比分析时间方面：鉴于顶点 ID 的范围是([0, |V|))，使用动态数组作为顶点索引时，查找顶点和插入顶点操作的时间复杂度为(O(1))，且内存访问简单。由于存储是连续的，动态数组支持快速的遍历操作。相比之下，尽管哈希表和 AVL 树在某些操作上具有相同的时间复杂度，但它们的开销比动态数组更大。由于动态图存储中插入边依赖于查找边操作，因此尽管动态数组的插入操作（Insert）时间复杂度为(O(1))，因为查找边带来的均摊时间复杂度也难以避免。 空间方面：实际内存消耗受元素大小的影响显著。Sortledton 和 Teseo 中的每个元素消耗(3 \\times w)字节（w为字长），分别用于存储顶点 ID、版本和指针。操作类型可以使用时间戳的高位表示。LiveGraph 中的每个元素也占用(3 \\times w)字节。相比之下，由于采用粗粒度锁，Aspen 中的每个元素仅消耗w字节。此外，Aspen 的邻边索引没有空槽。所以粗粒度锁方法比细粒度锁方法的内存效率更高。 细粒度锁与粗粒度锁的比较：细粒度锁方法需要对每个图操作执行锁操作，这可能导致锁竞争，从而产生高昂的时间成本。高 degree 顶点尤其容易被频繁访问。细粒度锁方法需要对每个元素进行版本检查，这会增加内存数据加载量和版本比较的计算量。相比之下，粗粒度锁方法避免了这些问题。但是细粒度锁方法允许多个写者同时更新图，并通过简单插入新元素原地更新，反而提升整体更新性能。 并且的，细粒度锁锁策略对底层图容器没有特殊要求，因此更具通用性；而粗粒度锁策略需要支持快速创建快照。但由于粗粒度锁策略不为每个元素维护版本或执行版本检查，它可以有效地与数据压缩技术结合，而这对细粒度锁方法来说是不可行的。 总结该论文围绕内存中 DGS 展开系统研究，旨在通过统一的抽象模型和测试框架，评估现有方法的性能瓶颈并指明优化方向。 研究提出了DGS的多层抽象模型（全局、局部及操作抽象），统一建模图查询、数据及访问模式，并基于此构建了包含第三方模块和自定义沙箱的通用测试框架，实现对 LLAMA、Aspen、LiveGraph、Teseo、Sortledton 等现有方法的公平对比。 实验结果表明，现有 DGS 方法存在显著缺陷：空间开销大；硬件利用不足，分段存储导致缓存失效和分支预测错误率上升；细粒度锁并发控制因版本维护和锁竞争效率低下。 最后，论文指出未来需优化版本管理以减少空间和效率开销、提升硬件利用率，并改进并发控制机制以缓解读写干扰和锁竞争问题。","link":"/2025/08/19/Paper-2/"},{"title":"Sigmod 论文《Rethinking The Compaction Policies in LSM-trees》阅读","text":"文章来自清华大学交叉信息学院，论文聚焦于 LSM 树的Compact策略优化，提出了一种名为 EcoTune 的动态规划算法，旨在通过重新思考Compact操作的定位，最大化系统的平均查询吞吐量。LSM 树的核心挑战在于通过将内存中的数据批量Flush到磁盘形成 “sorted string tables”，并定期Compact重叠的待合并SSTable以减少读放大（RA）。 传统Compact策略主要关注写放大（WA）与读放大的权衡，但忽略了现代存储设备的特性。现代 NVMe SSD 具备极高的写入带宽，写入性能不再是瓶颈。Compact与查询会竞争 CPU 和 I/O 资源，因此Compact策略的核心应是优化平均查询吞吐量，而非单纯降低瞬时读放大。 论文链接 Rethinking The Compaction Policies in LSM-trees 研究思路将Compact视为对计算和 I/O 资源的投资，其收益是未来查询性能的提升（减少需要探测的归并待合并SSTable数量），成本是Compact过程中占用的资源，优化目标是最大化Compact的 “累积收益”。 对写入性能的影响传统观点认为，更高的写放大（WA）会降低写入性能，这一结论源于早期HDD的限制：HDD 上的Flush和Compact操作无法并行，Compact会阻塞Flush，可能导致写入停滞和数据丢失，因此需尽可能降低Compact带来的写放大。但现代 NVMe SSD 的高写入带宽改变了这一局面： Flush和Compact可并行处理，且实际工作负载的写入速度通常较低； 只需为Flush预留足够带宽和 CPU 资源，剩余资源可自由分配给Compact和查询，Compact策略不会影响写入性能。 在不同 SSD 和 CPU 核心数下，不同Compact策略的平均写入延迟和尾延迟（P99）几乎无差异，作者认为Compact策略不影响写入性能。 作者认为Compact策略的设计应聚焦于如何利用剩余资源最大化查询性能。 对查询性能的影响Compact策略的核心目标应是利用剩余资源提升平均查询性能，而非仅优化瞬时读放大（RA）。作者给出平均查询性能的定义：以 “Compact周期”（两次全局Compact的间隔）内的平均查询吞吐量为衡量标准。全局Compact会将所有待合并SSTable合并为一个，确保不同周期的策略互不影响，但是Compact对查询的双刃剑： 积极Compact可减少待合并SSTable数量，提升瞬时查询速度；但Compact会占用 CPU 和 I/O 资源，导致查询暂时变慢，甚至阻塞。 保守Compact可能增加单次查询的 I/O 成本，但能为查询保留更多资源，反而可能提升整个周期的平均吞吐量。 作者认为Compact是对资源的 “投资”，成本是Compact的写放大和资源占用，收益是未来查询性能的提升。优化目标是最大化Compact的性能提升与持续时间的乘积。 EcoTune 算法出发点: 作者通过实验分析不同查询类型（Get、Seek、短扫描、长扫描）对Compact策略的敏感度：：合并小SSTable对 Get、Seek 和短扫描的 I/O 性能提升有限（RangeFilter已减少不必要 I/O），但过多小SSTable会增加 CPU 开销（需更多Filter来下探SSTable内容）。合并大SSTable显著提升长范围扫描的吞吐量（因长扫描的目标键多位于大SSTable中，Compact可降低读放大）。 作者认为Compact策略的核心应是优化大SSTable的合并，以提升长范围扫描性能，同时避免小SSTable过多导致的 CPU 开销。 核心设计Compact过程拆分为三个逻辑层级，根据对查询性能的影响制定不同策略： 顶层（Top Level）：作为 SSD 上的写缓冲区，存储新刷入的小SSTable，不进行Compact（作者认为对查询 I/O 影响小）。通过全局索引记录键与运行 ID 的映射，平衡 CPU 开销与内存占用。其容量设为(S = M/K)（M为 LSM 树总大小，K为长扫描的平均键数量），确保长扫描的 I/O 效率。 主层（Main Level）：存储中等大小的SSTable，其Compact策略直接影响长扫描性能。通过动态规划算法优化Compact时机和粒度，每个运行配备范围过滤器加速查询。 最后层（Last Level）：仅保留一个SSTable，限制空间放大。当主层达到容量上限（由主层与最后层的大小比c控制）时，触发全局Compact，合并所有运行至最后层。 其中，Flush和顶层到主层的Compact使用固定线程，主层内部的Compact与查询共享剩余线程，空闲时主层线程可处理查询，提升资源利用率。通过动态规划确定主层的最优Compact策略，动态规划的核心是权衡Compact的 “成本”（资源占用时间）和 “收益”（查询性能提升的持续时间）。 动态规划算法：问题定义：现有$e$个sstable，待处理$c$个新sstable，需确定最优Compact策略以最大化查询吞吐量累积。 状态定义：$f(e,c)$ 表示吞吐量累计的最大值。 递推关系：$$f(e,c)=max_x[f(e,c) + (T_w - x \\cdot T_c) \\cdot q(e + 1) + f(e + 1, c - x)]$$其中： $x$: 首次合并的单位SSTable数量； $T_w$：生成 1 个单位SSTable的时间； $T_c$：合并 1 个单位SSTable的时间； $q(e)$：当前有e个SSTable时的查询速度； 时间复杂度：(O(R^3))（R为Compact周期内的单位SSTable总数） 个人分析适合的场景长范围扫描占比高的工作负载EcoTune 的核心优化目标之一是提升长范围扫描的性能，其三级模型通过合并主层的大SSTable显著降低长扫描的 I/O 放大。 高带宽 SSD 环境该方法的设计前提是现代 SSD 具备远超实际工作负载的写入带宽可支持Compact与查询并行执行。在这类硬件上，EcoTune 能更灵活地分配剩余资源（CPU 和 I/O），平衡Compact与查询的资源竞争。 中等写入速度的稳定工作负载当写入速度远低于 SSD 带宽，EcoTune 保证写入不阻塞的前提下，通过动态规划优化Compact时机，最大化平均查询吞吐量。 对平均吞吐量和延迟稳定性要求高的场景EcoTune 通过平衡Compact与查询的资源占用，减少了因Compact导致的查询排队延迟，尤其在高并发或查询请求突发的场景中，其延迟稳定性优于 Leveling 和 Lazy Leveling。作者在 30K/s 的查询到达速度下，EcoTune 的尾延迟比传统策略低 3-4 个数量级。 不适合的场景写入速度接近或饱和 SSD 带宽的极端场景当写入速度接近 SSD 带宽上限，Flush与Compact的并行性被打破，该方法的优化逻辑可能失效。此时使用传统策略（比如 Lazy Leveling）可能更稳定。 以点查询或短扫描为主的工作负载EcoTune 的优化重心是主层大SSTable的合并，以提升长范围扫描性能，但对以点查询、短扫描为主的场景提升有限。实验显示，合并小SSTable对这类查询的 I/O 性能影响极小，反而可能因动态规划的资源分配逻辑增加额外开销。 硬件资源极度受限的环境若硬件资源限制（比如 CPU 核心少、SSD 带宽低）无法支持Compact与查询并行，EcoTune 的动态规划算法难以平衡资源分配。在作者的实验中仅 4-6 个 CPU 核心的场景中，CPU 竞争和上下文切换开销会削弱其优化效果，导致性能接近甚至低于传统策略。 对瞬时性能而非平均性能要求高的场景EcoTune 通过牺牲部分瞬时性能（如允许短期内更多SSTable存在）来优化Compact周期内的平均吞吐量。若应用场景对 “瞬时最高延迟” 或 “某一时刻的峰值性能” 要求严格（如高频交易、实时查询），其策略可能不符合需求。","link":"/2025/08/12/Paper-1/"},{"title":"RDMA-Toy-0 环境简单搭建","text":"RXE 是 Linux 上的“软件 RoCE v2 驱动”（内核模块 rdma_rxe），让普通以太网接口具备 RDMA 能力（走内核网络栈）。RoCE v2 将 RDMA 负载封装在 UDP/IPv4 或 UDP/IPv6 中，默认目的端口 4791。设备/驱动通常把到 4791 的流量识别为 RoCE。本文旨在用同一台主机、不依赖硬件 RNIC，搭出一套可复现且稳定的 RDMA 实验环境：rping、perftest均可跑通。 实验环境介绍系统：Debian testing（个人喜用testing分支，任一较新内核的 Debian/Ubuntu 均可），软件包（发行版自带，避免混装第三方 OFED）： 12sudo apt updatesudo apt install -y rdma-core ibverbs-providers rdmacm-utils perftest ethtool nftables 实验环境搭建 思路：在宿主机创建一对二层直连的虚拟网卡 vA/vB，分别配置 IPv4，再把它们各自绑定为 rxe0/rxe1。这样路径最短、变量最少，最利于复现与排障。 清理残留（可以反复执行） 12345678910# 停掉可能残留的测试进程sudo pkill -f 'rping|ibv_|ib_write_bw|ib_read_bw|ib_send_bw' 2&gt;/dev/null || true# 删除历史 veth/macvlan/ipvlanfor nic in vA vB mvl0 mvl1 ivl0 ivl1; do sudo ip link del &quot;$nic&quot; 2&gt;/dev/null || true; done# 删除历史 rxe/siw 设备并卸载模块for dev in $(sudo rdma link show 2&gt;/dev/null | awk '/link (rxe|siw)/{print $2}'); do sudo rdma link delete &quot;$dev&quot;; donesudo modprobe -r rdma_rxe 2&gt;/dev/null || true# 清空本机 nftables（避免拦截 UDP/4791）sudo nft flush ruleset 2&gt;/dev/null || true 创建veth对 123456789101112131415161718sudo ip link add vA type veth peer name vBsudo ip addr add 10.10.0.1/24 dev vAsudo ip addr add 10.10.0.2/24 dev vBsudo ip link set vA upsudo ip link set vB up# 关闭聚合，减少后续 RXE 的不确定性sudo ethtool -K vA gro off gso off tso off 2&gt;/dev/null || truesudo ethtool -K vB gro off gso off tso off 2&gt;/dev/null || true# 放宽反向路由校验（仅对这两个接口）sudo sysctl -w net.ipv4.conf.vA.rp_filter=0sudo sysctl -w net.ipv4.conf.vB.rp_filter=0# 基本连通性（预期均为成功）ping -c1 -I vA 10.10.0.2ping -c1 -I vB 10.10.0.1 ▸ 为什么先验证 ping？这一步确保二层（ARP）与三层（ICMP）基本无拦截；若此时 rping 能通而 ping 不通，通常是规则屏蔽了 ICMP 而并未影响 UDP/4791，详见NVIDIA DOC。 ▸ 为什么要关闭聚合，以及为什么？在 veth/macvlan/ipvlan 这类纯软件路径上做 Soft-RoCE 实验时，建议 ethtool -K gro off gso off tso off。这么做的目的有两个： 保证一条 UDP 报文 ≈ 一个内核 skb，不被合并/延后处理； 避免“虚拟网卡 + 分段/聚合”带来的时序抖动与特殊 skb 形态，从而使 rxe 的收发、流控与超时行为更可预测，可复现。 WHY: TSO（TCP Segmentation Offload）/GSO（Generic Segmentation Offload）发送端把一段很大的逻辑报文（大于 MTU）打成一个大 skb下发，交由后续层（通常是网卡或协议子层）再分片成多个 MTU 大小的帧。GSO 是可作用于 UDP 等非 TCP 场景；TSO 是网卡分段的特定形式。 GRO（Generic Receive Offload）/ LRO接收端把多个属于同一流的连续小包在内核合并为一个更大的 skb，以减少上层处理次数。GRO 是内核通用聚合，LRO 是网卡侧的聚合。在物理网卡场景中，这些特性通常有利于吞吐；但在纯软件路径（veth 等）上，它们会引入额外的合并/分段时机与非 MTU 粒度的 skb 形态。 RoCEv2 把 RDMA 负载封进 UDP；rxe 在内核处理 UDP/4791 的收发，将每个RoCE BTH 单元映射为数据平面的动作（QP/PSN、RNR/ACK/NAK、CQE 产生等）。期望是报文到达的时间序列尽量贴近发送端真实节奏。 多个连续 UDP 报文可能被 GRO 合成一个带分段信息的 skb（如 skb 的 fraglist 或 GRO segment list），最后在一次 NAPI 轮询里一次性交给上层。 对 rxe 来说，改变了包到达的时间分布（排队论）：本来应该“均匀到达”的 WQE 相关报文，会成批出现。副作用是对接收侧 CQE 瞬时压力增大，容易触发RNR（Receiver Not Ready），导致发送侧重试；发送侧若启用 GSO/TSO，可能把多 MTU 的数据打成一个大 skb交给 veth；veth 的对端再在某个时刻统一分段。导致不是每次立刻按 MTU 发送，而是攒批分段。副作用是对端看到的包间隔不均匀，ACK/NAK 的时序也随之改变；对 RDMA RC（可靠连接）这种对超时、窗口非常敏感的传输来说，会放大实验不可重复性。 启用Soft-RoCE 1234567891011sudo modprobe rdma_rxesudo rdma link add rxe0 type rxe netdev vAsudo rdma link add rxe1 type rxe netdev vB# 预期：state ACTIVE / LINK_UPsudo rdma link show# 查看 GID（用于后续 perftest 选择）ibv_devinfo -d rxe0 -v | grep -i 'GID\\[\\s*1\\]'ibv_devinfo -d rxe1 -v | grep -i 'GID\\[\\s*1\\]' 这里看到的 GID 是什么？ GID[0] 为 IPv6 链路本地（fe80::…）； GID[1] 为 IPv4-mapped IPv6（形如 ::ffff:10.10.0.x）。RoCE/Soft-RoCE 在小规模实验中经常选 GID[1] 来与 IPv4 地址对应。 链路验证 RDMA_CM（rping）: 1234# 服务端: 10.10.0.1sudo rping -s -a 10.10.0.1 -p 18515 -v -d# 客户端: 10.10.0.2 一侧，指向 .1sudo rping -c -a 10.10.0.1 -p 18515 -v -d 服务端输出: 12345678server posted rdma read reqrdma read completionserver received read completeserver ping data: rdma-ping-5868: KLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyserver posted go aheadsend completionrecv completionReceived rkey 101f addr 5645934421e0 len 64 from peer 客户端输出 1234send completionrecv completionping data: rdma-ping-5868: KLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyRDMA addr 56459344f390 rkey 11f5 len 64 ▸ 为什么推荐先跑 rping？rping 使用 RDMA-CM（RoCE v2 下走 UDP/4791），若能互通，说明 GID/寻址/控制面均已就绪。 perftest 验证 12345# 服务端（rxe0@vA）sudo ib_write_bw -d rxe0 -x 1 -R -F -m 1024 -s 1024 -p 19999# 客户端（rxe1@vB，指向 10.10.0.1）sudo ib_write_bw -d rxe1 -x 1 -R -F -m 1024 -s 1024 -p 19999 10.10.0.1 ▸ perftest 是什么？perftest 是基于 verbs 的带宽/时延微基准工具集，常用于功能与性能验证。 服务端输出 123456789101112131415161718192021222324252627282930************************************* Waiting for client to connect... *************************************--------------------------------------------------------------------------------------- RDMA_Write BW Test Dual-port : OFF Device : rxe0 Number of qps : 1 Transport type : IB Connection type : RC Using SRQ : OFF PCIe relax order: ON Lock-free : OFF ibv_wr* API : OFF Using DDP : OFF CQ Moderation : 100 Mtu : 1024[B] Link type : Ethernet GID index : 1 Max inline data : 0[B] rdma_cm QPs : ON Data ex. method : rdma_cm--------------------------------------------------------------------------------------- Waiting for client rdma_cm QP to connect Please run the same command with the IB/RoCE interface IP--------------------------------------------------------------------------------------- local address: LID 0000 QPN 0x0020 PSN 0xeb02b8 GID: 00:00:00:00:00:00:00:00:00:00:255:255:10:10:00:01 remote address: LID 0000 QPN 0x001f PSN 0xe6da3e GID: 00:00:00:00:00:00:00:00:00:00:255:255:10:10:00:01--------------------------------------------------------------------------------------- #bytes #iterations BW peak[MiB/sec] BW average[MiB/sec] MsgRate[Mpps] 1024 5000 416.70 409.83 0.419671--------------------------------------------------------------------------------------- 客户端输出： 123456789101112131415161718192021222324--------------------------------------------------------------------------------------- RDMA_Write BW Test Dual-port : OFF Device : rxe1 Number of qps : 1 Transport type : IB Connection type : RC Using SRQ : OFF PCIe relax order: ON Lock-free : OFF ibv_wr* API : OFF Using DDP : OFF TX depth : 128 CQ Moderation : 100 Mtu : 1024[B] Link type : Ethernet GID index : 1 Max inline data : 0[B] rdma_cm QPs : ON Data ex. method : rdma_cm--------------------------------------------------------------------------------------- local address: LID 0000 QPN 0x001f PSN 0xe6da3e GID: 00:00:00:00:00:00:00:00:00:00:255:255:10:10:00:01 remote address: LID 0000 QPN 0x0020 PSN 0xeb02b8 GID: 00:00:00:00:00:00:00:00:00:00:255:255:10:10:00:01--------------------------------------------------------------------------------------- #bytes #iterations BW peak[MiB/sec] BW average[MiB/sec] MsgRate[Mpps] 1024 5000 416.70 409.83 0.419671--------------------------------------------------------------------------------------- 参数方面：-d rxe0: 使用名为 rxe0 的 RDMA 设备。rxe0 是 Soft-RoCE (RXE) 的软件模拟设备，表明这是在用软件模拟的 RDMA 环境（通常 over Ethernet）。-R: 使用 RDMA CM 进行连接建立和数据交换。RDMA CM 是一个用于建立连接的抽象层，比传统的 IB 动词更灵活，尤其适用于 RoCE 和跨子网通信。-F: 在服务器端启动后永远等待客户端连接。如果没有这个参数，服务器可能在一段时间无连接后自动退出。-m 1024: 设置每次轮询完成队列 (CQ) 前要执行的测试迭代次数为 1024。这是一个与内部流水线优化相关的参数，会影响性能表现。-s 1024: 设置每次 RDMA 写操作传输的消息大小为 1024 字节。-p 19999: 指定服务器监听连接的 TCP 端口号 为 19999。客户端需要通过这个端口来连接。 输出结果中，测试在两台通过以太网（Soft-RoCE，即 RXE）连接的虚拟机之间进行，使用 1024 字节的消息大小。测试结果显示，平均带宽达到 409.83 MiB/s（约 3.2 Gbps），这表明 Soft-RoCE 在虚拟环境下的基本功能正常。 错误路径 &amp; 意外处理 ping 不通但 rping 能通ICMP 被规则屏蔽或 ARP 未解析（而 CM 的 UDP/4791 未被拦）： 123sudo nft flush ruleset；sudo sysctl -w net.ipv4.icmp_echo_ignore_all=0；arping -I vA 10.10.0.2 / arping -I vB 10.10.0.1 perftest 报 **Failed to modify QP … to RTR / Unable to Connect …**，检查两端 GID/设备/地址没对齐或启动顺序错误： 核对 ibv_devinfo -d rxe{0,1} -v 中的 GID[1] 分别为 ::ffff:10.10.0.1/2； 先启动服务端，再启动客户端； 考虑必要时将 MTU降档（如 -m 512 -s 256），稳定后再拉高。","link":"/2025/09/19/RDMA-1/"},{"title":"RDMA-Toy-1 PingPong和概念梳理","text":"概念对齐verbs 是 RDMA 的底层编程接口，目的是通过一组 ibv_* 调用把用户读写任务（WR）提交到队列（QP），网卡按这些请求在不经过内核协议栈的前提下执行 SEND/RECV、RDMA READ/WRITE、原子操作 等动作，并把结果以完成事件（CQE）的形式回报给应用。 ▸ 类比：BSD sockets 是用 TCP/UDP 传字节的系统调用集合，verbs 是用 RDMA 传内存的系统调用集合。sockets 交给内核协议栈；verbs 交给 RNIC 的硬件队列。 InfiniBand verbs 规范定义 HCA 能执行的动作（post、poll、create、modify等），因此在 Linux 用户态库中呈现为一组 ibv_* API。而不同厂商通过 provider（如 libmlx5、librxe）实现背后的具体硬件/软件行为，但对应用暴露统一的 verbs 抽象。 用户态库：libibverbs（rdma-core）+ 各 provider（libmlx5, librxe）。 字符设备：/dev/infiniband/uverbsX、rdma_cm 等承接与内核的 ioctl/事件通道。 这也是为什么混装发行版包与第三方 OFED 会报错“符号找不到”，因为应用链接的 lib 库与内核驱动/provider 实现必须匹配。 关键对象： Device / Context：网卡设备与用户态句柄（ibv_open_device）； PD（Protection Domain）：保护域，用于把 QP、MR、CQ 等资源组合； MR（Memory Region）：注册内存，获得 lkey/rkey 以供 DMA 访问（ibv_reg_mr）； CQ（Completion Queue）：完成队列，硬件把执行结果写成 CQE，使用 ibv_poll_cq 取出； QP（Queue Pair）：一对发送队列 SQ 与接收队列 RQ。是一个可靠连接 RC 的典型状态机：RESET→INIT→RTR→RTS； WR（Work Request）：用户指令，如 post a SEND 或 do RDMA WRITE； SGE（Scatter/Gather Entry）：描述这条 WR 要访问的内存片段（地址、长度、lkey）。 WQE：WR 进入硬件队列后的条目形式； WC（Work Completion）：CQ 里读到的一条完成记录（含 status、wr_id 等）。 HCA（Host Channel Adapter）：主机侧的 RDMA 适配器，硬件或软件实现，提供 QP/CQ/MR 等verbs能力，并执行 DMA。 lkey（local key）：本机访问某块注册内存（MR）时必须携带的访问令牌； rkey（remote key）：远端对这块 MR 进行 RDMA READ/WRITE 时必须提供的 授权令牌，要通过握手显式告诉对端。 RDMA 设备在主机侧的抽象，InfiniBand里叫 HCA；以太网 RoCE 场景是 RNIC。在 Linux verbs 里，应用通过 ibv_open_device() 打开 HCA/RNIC 得到上下文（ibv_context*）： 暴露 队列对（QP）、完成队列（CQ）、保护域（PD）、注册内存（MR） 等资源； 把应用通过 verbs post 的工作请求（WR）下发到硬件队列，并通过 DMA 直接访问内存； 把执行结果写入 CQE 供应用 ibv_poll_cq() 读取。 实现形态： 硬件 HCA：如 Mellanox/NVIDIA ConnectX 系列； 软件 HCA：如 Soft-RoCE（rxe） 数据通路： 发现/打开设备：ibv_get_device_list → ibv_open_device； 分配资源：ibv_alloc_pd、ibv_create_cq、ibv_create_qp； 注册内存：ibv_reg_mr（拿到 lkey/rkey）； 建链：把 QP 走状态机到 RTS（连接信息可用 RDMA-CM 交换，或 out-of-band）； 收发与完成： ibv_post_recv 贴接收缓冲； 发端 ibv_post_send（opcode 可选 IBV_WR_SEND/WRITE/READ/ATOMIC 等）； 循环 ibv_poll_cq 取 WC，检查 status==IBV_WC_SUCCESS。 ▸ RDMA-CM 与 verbs 的关系RDMA-CM（如 rping、perftest 的 -R）只负责找到RDMA对端；verbs 接口负责具体的数据通路行为。 伪代码描述： 1234567891011121314151617181920212223ctx = ibv_open_device(dev);pd = ibv_alloc_pd(ctx);cq = ibv_create_cq(ctx, CQ_DEPTH, ...);qp = ibv_create_qp(pd, { .send_cq=cq, .recv_cq=cq, .cap={...}, .qp_type=IBV_QPT_RC });mr_recv = ibv_reg_mr(pd, recv_buf, SZ, IBV_ACCESS_LOCAL_WRITE);mr_send = ibv_reg_mr(pd, send_buf, SZ, IBV_ACCESS_LOCAL_WRITE|IBV_ACCESS_REMOTE_WRITE|IBV_ACCESS_REMOTE_READ);// （连接信息通过 RDMA-CM 或自定义握手拿到）// 接收post_recv(recv_buf, lkey);// 发送一个 SENDpost_send(send_buf, lkey, IBV_WR_SEND);// 轮询 CQ 直到拿到 WCwhile (!done) { n = ibv_poll_cq(cq, &amp;wc); if (n &gt; 0 &amp;&amp; wc.status == IBV_WC_SUCCESS) handle(wc);} PingPong 实验为了能够深入理解RDMA的机制，笔者尝试动手实现了一个极简版本的PingPong代码，如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297#include &lt;cstdio&gt;#include &lt;cstdlib&gt;#include &lt;cstring&gt;#include &lt;cerrno&gt;#include &lt;getopt.h&gt;#include &lt;string&gt;#include &lt;algorithm&gt;#include &lt;cstdarg&gt; // &lt;-- for va_list, va_start, va_end#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;unistd.h&gt; // sysconf#include &lt;infiniband/verbs.h&gt;#include &lt;rdma/rdma_cma.h&gt;static const size_t kMsgSizeDefault = 1024;static const int kCQDepth = 256;static const int kQPDepth = 128;static const uint64_t WRID_RECV = 0xBEEF;static const uint64_t WRID_SEND = 0xCAFE;struct Options { bool is_server = false; std::string ip = &quot;10.10.0.1&quot;; uint16_t port = 19999; int iters = 10; bool verbose = false; size_t msg_size = kMsgSizeDefault;};struct Ctx { rdma_event_channel* ec = nullptr; rdma_cm_id* id = nullptr; ibv_pd* pd = nullptr; ibv_cq* cq = nullptr; ibv_qp* qp = nullptr; ibv_mr* mr_send = nullptr; ibv_mr* mr_recv = nullptr; char* send_buf = nullptr; char* recv_buf = nullptr;};static void die(const char* msg) { perror(msg); std::exit(EXIT_FAILURE);}static void diex(const char* msg, int err) { std::fprintf(stderr, &quot;%s: %s (%d)\\n&quot;, msg, strerror(err), err); std::exit(EXIT_FAILURE);}static void vlog(bool on, const char* fmt, ...) { if (!on) return; va_list ap; va_start(ap, fmt); std::vfprintf(stdout, fmt, ap); std::fputc('\\n', stdout); va_end(ap);}static const char* wc_opcode_str(int op) { switch (op) { case IBV_WC_SEND: return &quot;SEND&quot;; case IBV_WC_RECV: return &quot;RECV&quot;; case IBV_WC_RDMA_WRITE: return &quot;RDMA_WRITE&quot;; case IBV_WC_RDMA_READ: return &quot;RDMA_READ&quot;; default: return &quot;OTHER&quot;; }}static void post_one_recv(Ctx&amp; ctx, size_t msg_size) { ibv_sge sge{}; sge.addr = reinterpret_cast&lt;uint64_t&gt;(ctx.recv_buf); sge.length = (uint32_t)msg_size; sge.lkey = ctx.mr_recv-&gt;lkey; ibv_recv_wr wr{}; wr.wr_id = WRID_RECV; wr.sg_list = &amp;sge; wr.num_sge = 1; ibv_recv_wr* bad = nullptr; int rc = ibv_post_recv(ctx.qp, &amp;wr, &amp;bad); if (rc) diex(&quot;ibv_post_recv&quot;, rc);}static void post_one_send(Ctx&amp; ctx, size_t len) { ibv_sge sge{}; sge.addr = reinterpret_cast&lt;uint64_t&gt;(ctx.send_buf); sge.length = (uint32_t)len; sge.lkey = ctx.mr_send-&gt;lkey; ibv_send_wr wr{}; wr.wr_id = WRID_SEND; wr.sg_list = &amp;sge; wr.num_sge = 1; wr.opcode = IBV_WR_SEND; wr.send_flags = IBV_SEND_SIGNALED; ibv_send_wr* bad = nullptr; int rc = ibv_post_send(ctx.qp, &amp;wr, &amp;bad); if (rc) diex(&quot;ibv_post_send&quot;, rc);}static void wait_one_cqe(Ctx&amp; ctx, ibv_wc&amp; wc, bool verbose) { for (;;) { int n = ibv_poll_cq(ctx.cq, 1, &amp;wc); if (n &lt; 0) die(&quot;ibv_poll_cq&quot;); if (n == 0) continue; if (wc.status != IBV_WC_SUCCESS) { std::fprintf(stderr, &quot;CQE error: status=%d wr_id=0x%lx opcode=%d(%s)\\n&quot;, wc.status, (unsigned long)wc.wr_id, wc.opcode, wc_opcode_str(wc.opcode)); std::exit(EXIT_FAILURE); } if (verbose) { std::printf(&quot;[cq] wr_id=0x%lx opcode=%d(%s)\\n&quot;, (unsigned long)wc.wr_id, wc.opcode, wc_opcode_str(wc.opcode)); } return; }}static void usage(const char* prog) { std::fprintf(stderr, &quot;Usage: %s (-s|-c) -a &lt;ipv4&gt; [-p &lt;port&gt;] [-n &lt;iters&gt;] [-m &lt;msg_size&gt;] [-v]\\n&quot; &quot; -s server mode\\n&quot; &quot; -c client mode\\n&quot; &quot; -a &lt;ipv4&gt; bind/connect address (IPv4)\\n&quot; &quot; -p &lt;port&gt; TCP port for control channel (default 19999)\\n&quot; &quot; -n &lt;iters&gt; ping-pong iterations (default 10)\\n&quot; &quot; -m &lt;bytes&gt; message buffer size (default 1024)\\n&quot; &quot; -v verbose logging\\n&quot;, prog);}int main(int argc, char** argv) { Options opt; bool mode_set = false; int c; while ((c = getopt(argc, argv, &quot;sca:p:n:m:v&quot;)) != -1) { switch (c) { case 's': opt.is_server = true; mode_set = true; break; case 'c': opt.is_server = false; mode_set = true; break; case 'a': opt.ip = optarg; break; case 'p': opt.port = static_cast&lt;uint16_t&gt;(std::stoi(optarg)); break; case 'n': opt.iters = std::max(1, std::stoi(optarg)); break; case 'm': opt.msg_size = std::max(64, std::stoi(optarg)); break; case 'v': opt.verbose = true; break; default: usage(argv[0]); return 2; } } if (!mode_set) { usage(argv[0]); return 2; } setvbuf(stdout, nullptr, _IOLBF, 0); std::printf(&quot;[*] mode=%s ip=%s port=%u iters=%d msg=%zu\\n&quot;, opt.is_server ? &quot;server&quot; : &quot;client&quot;, opt.ip.c_str(), opt.port, opt.iters, opt.msg_size); Ctx ctx; ctx.ec = rdma_create_event_channel(); if (!ctx.ec) die(&quot;rdma_create_event_channel&quot;); if (rdma_create_id(ctx.ec, &amp;ctx.id, nullptr, RDMA_PS_TCP)) die(&quot;rdma_create_id&quot;); sockaddr_in sin{}; sin.sin_family = AF_INET; sin.sin_port = htons(opt.port); if (inet_pton(AF_INET, opt.ip.c_str(), &amp;sin.sin_addr) != 1) die(&quot;inet_pton&quot;); if (opt.is_server) { std::puts(&quot;[*] server: bind + listen&quot;); if (rdma_bind_addr(ctx.id, (sockaddr*)&amp;sin)) die(&quot;rdma_bind_addr&quot;); if (rdma_listen(ctx.id, 1)) die(&quot;rdma_listen&quot;); std::puts(&quot;[*] server: waiting CONNECT_REQUEST&quot;); rdma_cm_event* ev = nullptr; if (rdma_get_cm_event(ctx.ec, &amp;ev)) die(&quot;rdma_get_cm_event&quot;); if (ev-&gt;event != RDMA_CM_EVENT_CONNECT_REQUEST) { std::fprintf(stderr, &quot;expected CONNECT_REQUEST, got %d\\n&quot;, ev-&gt;event); std::exit(EXIT_FAILURE); } vlog(opt.verbose, &quot;[cm] CONNECT_REQUEST arrived&quot;); rdma_cm_id* child = ev-&gt;id; rdma_ack_cm_event(ev); ctx.id = child; } else { rdma_cm_event* ev = nullptr; if (rdma_resolve_addr(ctx.id, nullptr, (sockaddr*)&amp;sin, 2000)) die(&quot;rdma_resolve_addr&quot;); if (rdma_get_cm_event(ctx.ec, &amp;ev)) die(&quot;rdma_get_cm_event&quot;); vlog(opt.verbose, &quot;[cm] got event=%d (expected ADDR_RESOLVED=0)&quot;, ev-&gt;event); if (ev-&gt;event != RDMA_CM_EVENT_ADDR_RESOLVED) die(&quot;expected ADDR_RESOLVED&quot;); rdma_ack_cm_event(ev); if (rdma_resolve_route(ctx.id, 2000)) die(&quot;rdma_resolve_route&quot;); if (rdma_get_cm_event(ctx.ec, &amp;ev)) die(&quot;rdma_get_cm_event&quot;); vlog(opt.verbose, &quot;[cm] got event=%d (expected ROUTE_RESOLVED=1)&quot;, ev-&gt;event); if (ev-&gt;event != RDMA_CM_EVENT_ROUTE_RESOLVED) die(&quot;expected ROUTE_RESOLVED&quot;); rdma_ack_cm_event(ev); } // 资源创建 ctx.pd = ibv_alloc_pd(ctx.id-&gt;verbs); if (!ctx.pd) die(&quot;ibv_alloc_pd&quot;); ctx.cq = ibv_create_cq(ctx.id-&gt;verbs, kCQDepth, nullptr, nullptr, 0); if (!ctx.cq) die(&quot;ibv_create_cq&quot;); ibv_qp_init_attr qp_init{}; qp_init.qp_type = IBV_QPT_RC; qp_init.send_cq = ctx.cq; qp_init.recv_cq = ctx.cq; qp_init.cap.max_send_wr = kQPDepth; qp_init.cap.max_recv_wr = kQPDepth; qp_init.cap.max_send_sge = 1; qp_init.cap.max_recv_sge = 1; if (rdma_create_qp(ctx.id, ctx.pd, &amp;qp_init)) die(&quot;rdma_create_qp&quot;); ctx.qp = ctx.id-&gt;qp; vlog(opt.verbose, &quot;[qp] created qp_num=0x%x&quot;, ctx.qp-&gt;qp_num); // 分配并注册内存 long pagesz = sysconf(_SC_PAGESIZE); if (pagesz &lt;= 0) pagesz = 4096; if (posix_memalign((void**)&amp;ctx.send_buf, (size_t)pagesz, opt.msg_size)) die(&quot;posix_memalign send&quot;); if (posix_memalign((void**)&amp;ctx.recv_buf, (size_t)pagesz, opt.msg_size)) die(&quot;posix_memalign recv&quot;); memset(ctx.send_buf, 0, opt.msg_size); memset(ctx.recv_buf, 0, opt.msg_size); int access = IBV_ACCESS_LOCAL_WRITE | IBV_ACCESS_REMOTE_READ | IBV_ACCESS_REMOTE_WRITE; ctx.mr_send = ibv_reg_mr(ctx.pd, ctx.send_buf, opt.msg_size, access); ctx.mr_recv = ibv_reg_mr(ctx.pd, ctx.recv_buf, opt.msg_size, access); if (!ctx.mr_send || !ctx.mr_recv) die(&quot;ibv_reg_mr&quot;); // 先接收，避免对端首包 RNR post_one_recv(ctx, opt.msg_size); // 连接 rdma_conn_param conn{}; conn.initiator_depth = 1; conn.responder_resources = 1; conn.retry_count = 7; conn.rnr_retry_count = 7; if (opt.is_server) { if (rdma_accept(ctx.id, &amp;conn)) die(&quot;rdma_accept&quot;); } else { if (rdma_connect(ctx.id, &amp;conn)) die(&quot;rdma_connect&quot;); } rdma_cm_event* ev = nullptr; if (rdma_get_cm_event(ctx.ec, &amp;ev)) die(&quot;rdma_get_cm_event&quot;); if (ev-&gt;event != RDMA_CM_EVENT_ESTABLISHED) die(&quot;expected ESTABLISHED&quot;); rdma_ack_cm_event(ev); std::puts(&quot;[*] connection ESTABLISHED&quot;); // ping-pong 循环 ibv_wc wc; for (int i = 0; i &lt; opt.iters; ++i) { if (!opt.is_server) { size_t len = (size_t)std::snprintf(ctx.send_buf, opt.msg_size, &quot;ping %d&quot;, i); len = std::min(len + 1, opt.msg_size); // 带 '\\0' 便于对端直接打印 post_one_send(ctx, len); wait_one_cqe(ctx, wc, opt.verbose); // SEND 完成 wait_one_cqe(ctx, wc, opt.verbose); // RECV 完成 std::printf(&quot;[client] recv: %.*s\\n&quot;, (int)strnlen(ctx.recv_buf, opt.msg_size), ctx.recv_buf); post_one_recv(ctx, opt.msg_size); } else { wait_one_cqe(ctx, wc, opt.verbose); // RECV 完成（收到对端 ping） std::printf(&quot;[server] recv: %.*s\\n&quot;, (int)strnlen(ctx.recv_buf, opt.msg_size), ctx.recv_buf); size_t len = (size_t)std::snprintf(ctx.send_buf, opt.msg_size, &quot;pong %d&quot;, i); len = std::min(len + 1, opt.msg_size); post_one_send(ctx, len); wait_one_cqe(ctx, wc, opt.verbose); // SEND 完成 post_one_recv(ctx, opt.msg_size); } } rdma_disconnect(ctx.id); ibv_dereg_mr(ctx.mr_send); ibv_dereg_mr(ctx.mr_recv); ibv_destroy_qp(ctx.qp); ibv_destroy_cq(ctx.cq); ibv_dealloc_pd(ctx.pd); rdma_destroy_id(ctx.id); rdma_destroy_event_channel(ctx.ec); std::puts(&quot;done&quot;); return 0;} 源码分析 RDMA-CM 建立 1234567ctx.ec = rdma_create_event_channel(); // 事件通道rdma_create_id(ctx.ec, &amp;ctx.id, nullptr, RDMA_PS_TCP);// server：rdma_bind_addr + rdma_listen → 等 CONNECT_REQUEST// client：rdma_resolve_addr → ADDR_RESOLVED// rdma_resolve_route → ROUTE_RESOLVED RDMA-CM 目的是寻址与建链，数据面仍由 verbs 执行。在 RoCEv2 上，CM 实际走 UDP/4791 进行寻址；本 demo 额外使用一个 TCP 端口（默认 19999）作为参数交换的“控制通道”。 资源对象（PD / CQ / QP）初始化 123456789101112ctx.pd = ibv_alloc_pd(ctx.id-&gt;verbs);ctx.cq = ibv_create_cq(ctx.id-&gt;verbs, kCQDepth, nullptr, nullptr, 0);ibv_qp_init_attr qp_init{};qp_init.qp_type = IBV_QPT_RC;qp_init.send_cq = ctx.cq;qp_init.recv_cq = ctx.cq;qp_init.cap = { .max_send_wr=kQPDepth, .max_recv_wr=kQPDepth, .max_send_sge=1, .max_recv_sge=1 };rdma_create_qp(ctx.id, ctx.pd, &amp;qp_init); // 后续 connect/accept 会自动把 QP 推到 RTS RC（Reliable Connection） 模式下 QP 的状态机为 RESET→INIT→RTR→RTS。简单的，用 rdma_create_qp + rdma_connect/accept 让 CM 自动改 QP，避免手写 ibv_modify_qp。 先RECV 12post_one_recv(ctx, msg_size); // 进入循环前预投一条 RECV RNR（Receiver Not Ready）：对端发 SEND 而本端 RQ 没有可用 WQE 时，会返回 RNR NAK 并触发重试。先 RECV 是 RC 的标准做法。 建立链接 123if (server) rdma_accept(ctx.id, &amp;conn); else rdma_connect(ctx.id, &amp;conn);rdma_get_cm_event(...ESTABLISHED); 客户端循环体：构造 “ping i” → post_one_send() → 等 SEND 完成 → 再等 RECV 完成（收到服务器 “pong i”）； 服务端：先等 RECV 完成（收到 “ping i”）→ 构造 “pong i” → SEND 完成 → 补一条 RECV 进入下一轮。 发送数据 12345size_t len = (size_t)std::snprintf(ctx.send_buf, msg_size, &quot;ping %d&quot;, i);len = std::min(len + 1, msg_size); // '\\0'，便于打印post_one_send(ctx, len); CQ 轮询 123wait_one_cqe(ctx, wc, verbose); // 轮询直到拿到 1 条 CQEif (wc.status != IBV_WC_SUCCESS) { ...exit... } 这里保留了每条 SEND 必带 SIGNALED（每个发送有 CQE），为了便于观察。性能测试时可改成每 N 条 signal 一次。 优雅断开 123456789rdma_disconnect(ctx.id);ibv_dereg_mr(ctx.mr_send);ibv_dereg_mr(ctx.mr_recv);ibv_destroy_qp(ctx.qp);ibv_destroy_cq(ctx.cq);ibv_dealloc_pd(ctx.pd);rdma_destroy_id(ctx.id);rdma_destroy_event_channel(ctx.ec); 这里的关闭顺序很关键：断开链接 → 释放 MR/QP/CQ/PD 资源； 实验结果 编译 1g++ -O3 -Wall rdma_pingpong.cc -o rdma_pingpong -lrdmacm -libverbs 运行 服务端（在 10.10.0.1 侧）： 1sudo ./rdma_pingpong -s -a 10.10.0.1 -v 客户端（另一侧连 10.10.0.1）： 1sudo ./rdma_pingpong -c -a 10.10.0.1 -v 输出 客户端输出： 1234567891011121314151617181920212223242526272829303132333435363738sudo ./rdma_pingpong -c -a 10.10.0.1 -v[*] mode=client ip=10.10.0.1 port=19999 iters=10 msg=1024[cm] got event=0 (expected ADDR_RESOLVED=0)[cm] got event=2 (expected ROUTE_RESOLVED=1)[qp] created qp_num=0x2d[*] connection ESTABLISHED[cq] wr_id=0xcafe opcode=0(SEND)[cq] wr_id=0xbeef opcode=128(RECV)[client] recv: pong 0[cq] wr_id=0xcafe opcode=0(SEND)[cq] wr_id=0xbeef opcode=128(RECV)[client] recv: pong 1[cq] wr_id=0xcafe opcode=0(SEND)[cq] wr_id=0xbeef opcode=128(RECV)[client] recv: pong 2[cq] wr_id=0xcafe opcode=0(SEND)[cq] wr_id=0xbeef opcode=128(RECV)[client] recv: pong 3[cq] wr_id=0xcafe opcode=0(SEND)[cq] wr_id=0xbeef opcode=128(RECV)[client] recv: pong 4[cq] wr_id=0xcafe opcode=0(SEND)[cq] wr_id=0xbeef opcode=128(RECV)[client] recv: pong 5[cq] wr_id=0xcafe opcode=0(SEND)[cq] wr_id=0xbeef opcode=128(RECV)[client] recv: pong 6[cq] wr_id=0xcafe opcode=0(SEND)[cq] wr_id=0xbeef opcode=128(RECV)[client] recv: pong 7[cq] wr_id=0xcafe opcode=0(SEND)[cq] wr_id=0xbeef opcode=128(RECV)[client] recv: pong 8[cq] wr_id=0xcafe opcode=0(SEND)[cq] wr_id=0xbeef opcode=128(RECV)[client] recv: pong 9done 服务端输出： 123456789101112131415161718192021222324252627282930313233343536373839sudo ./rdma_pingpong -s -a 10.10.0.1 -v[*] mode=server ip=10.10.0.1 port=19999 iters=10 msg=1024[*] server: bind + listen[*] server: waiting CONNECT_REQUEST[cm] CONNECT_REQUEST arrived[qp] created qp_num=0x2e[*] connection ESTABLISHED[cq] wr_id=0xbeef opcode=128(RECV)[server] recv: ping 0[cq] wr_id=0xcafe opcode=0(SEND)[cq] wr_id=0xbeef opcode=128(RECV)[server] recv: ping 1[cq] wr_id=0xcafe opcode=0(SEND)[cq] wr_id=0xbeef opcode=128(RECV)[server] recv: ping 2[cq] wr_id=0xcafe opcode=0(SEND)[cq] wr_id=0xbeef opcode=128(RECV)[server] recv: ping 3[cq] wr_id=0xcafe opcode=0(SEND)[cq] wr_id=0xbeef opcode=128(RECV)[server] recv: ping 4[cq] wr_id=0xcafe opcode=0(SEND)[cq] wr_id=0xbeef opcode=128(RECV)[server] recv: ping 5[cq] wr_id=0xcafe opcode=0(SEND)[cq] wr_id=0xbeef opcode=128(RECV)[server] recv: ping 6[cq] wr_id=0xcafe opcode=0(SEND)[cq] wr_id=0xbeef opcode=128(RECV)[server] recv: ping 7[cq] wr_id=0xcafe opcode=0(SEND)[cq] wr_id=0xbeef opcode=128(RECV)[server] recv: ping 8[cq] wr_id=0xcafe opcode=0(SEND)[cq] wr_id=0xbeef opcode=128(RECV)[server] recv: ping 9[cq] wr_id=0xcafe opcode=0(SEND)done 实验结论 event=0 是 ADDR_RESOLVED，event=2 是 ROUTE_RESOLVED； opcode=0(SEND) 表示本端发送完成 CQE，opcode=128(RECV) 表示本端收到对端消息的 CQE； 客户端先 SEND 后 RECV，服务端先 RECV 后 SEND，是合理的 ping→pong 循环时序。 以客户端前 3 轮为例： SEND 完成（opcode=0, wr_id=0xCAFE），说明本地 SQ 中的 SEND WQE 已成功下发并确认，但不代表对端已经处理，只代表本端发送侧完成； RECV 完成（opcode=128, wr_id=0xBEEF），说明对端的 “pong i” 已到达并被 RQ 中的 RECV WQE 接住，本端从 CQ 中取到这条完成。 “[client] recv: pong i” 被打印，说明读取 recv_buf，并立刻 post_one_recv()，让下一轮对端的 SEND 不会 RNR。 总结本次 demo 用 RDMA-CM 简化了建链，用 SEND/RECV 实现了最简单的的 ping-pong实验，并且给出了 RDMA 相关内容的关键概念对齐。实验结果日志的每一行都对应一个机制节点：CM 事件（0/2/ESTABLISHED）→ QP 创建 → 首个 RECV 预投 → 循环中 客户端先 SEND 后 RECV、服务端先 RECV 后 SEND → 每次 RECV 完成后立刻补位。可以作为针对 RDMA 入门的极简了解。 下次预告： 通过初始 SEND 交换“远端 addr + rkey + len”； 构造 IBV_WR_RDMA_READ/WRITE 的 wr.wr.rdma.* 字段； 设计 RECV 池与 Doorbell 批量 post，减少 RNR 与 CQ 压力； 尝试把 RDMA 并入 io_uring 事件环。","link":"/2025/09/20/RDMA-2/"},{"title":"RocksDB-源码分析（1）BlockTable 读源码分析","text":"本文讨论了RocksDB的Prefetch和Async机制，在回顾LevelDB的SSTable读流程基础上，深入分析RocksDB的BlockBasedTableReader读流程、Prefetch相关函数及异步读取机制，还介绍了相关类设计和简单测试结果。包括： LevelDB的SSTable读流程：分为Open和Get两个步骤，Open包括文件句柄管理、Footer读取等；Get使用二分查找索引定位数据块，缓存优先读取，启用布隆过滤器且所有块未缓存读取时，与文件系统交互6次。 RocksDB的Open流程：调用FilePrefetchBuffer预取文件尾部获取Footer，加载元索引块等，缓存关键元数据，减少磁盘I/O操作。 PrefetchTail函数：确定预读取大小和范围，尝试文件系统预取，通过限制浪费比例确保I/O效率提升。 Get流程：用户发起请求，经过时间戳和过滤块匹配检查，创建索引迭代器遍历索引块和数据块，最终返回结果。 异步读取：非阻塞操作，提高并发性，与预取结合进一步提高系统并发处理能力。 相关类设计：FilePrefetchBuffer负责预取缓冲区管理；BlockFetcher独立Block的异步化读取操作，结合多种缓存和Prefetch机制。 简单测试：生成测试命令，对比启用和禁用异步扫描的测试结果，RocksDB的Prefetch和Async机制优化了读取性能。 LevelDB SSTable读回顾先简单来回顾一下LevelDB中的SSTable读流程，其时序逻辑如下图所示： sequenceDiagram participant Client participant DBImpl participant Version participant TableCache participant Table participant Block participant BlockIter participant FileSystem Client->>DBImpl: Get(key) DBImpl->>Version: Get() activate Version Version->>MemTable: Get() activate MemTable MemTable-->>Version: Not Found deactivate MemTable Version->>ImmutableMemTable: Get() activate ImmutableMemTable ImmutableMemTable-->>Version: Not Found deactivate ImmutableMemTable Note right of Version: SSTable Lookup Version->>TableCache: Get(file_number) activate TableCache TableCache->>TableCache: FindTable alt Table Not Cached TableCache->>FileSystem: NewRandomAccessFile activate FileSystem FileSystem-->>TableCache: file_handle deactivate FileSystem TableCache->>Table: Open activate Table Table->>FileSystem: ReadFooter activate FileSystem FileSystem-->>Table: footer_data deactivate FileSystem Table->>FileSystem: ReadIndexBlock activate FileSystem FileSystem-->>Table: index_block deactivate FileSystem Table->>Block: new IndexBlock end Table->>Block: NewIterator Block-->>Table: index_iter Table->>index_iter: Seek(key) activate index_iter loop Binary Search index_iter->>BlockIter: FindMidPoint BlockIter->>BlockIter: CompareKeys alt Key < Mid BlockIter->>BlockIter: AdjustLeft else BlockIter->>BlockIter: AdjustRight end end index_iter->>BlockIter: LinearScan BlockIter-->>index_iter: FoundHandle deactivate index_iter Table->>Table: BlockReader alt CacheHit Table->>BlockCache: Lookup BlockCache-->>Table: cached_block else Table->>FileSystem: ReadDataBlock activate FileSystem FileSystem-->>Table: raw_block deactivate FileSystem Table->>Block: new DataBlock Table->>BlockCache: Insert end Table->>Block: NewIterator Block-->>Table: data_iter Table->>data_iter: Seek(key) activate data_iter loop Binary Search data_iter->>BlockIter: FindRestartPoint BlockIter->>BlockIter: CompareEntries end loop LinearScan BlockIter->>BlockIter: ParseNextKey alt KeyMatch BlockIter->>BlockIter: ReturnValue end end data_iter-->>Table: value deactivate data_iter Table-->>TableCache: value deactivate Table TableCache-->>Version: value deactivate TableCache Version-->>DBImpl: value deactivate Version DBImpl-->>Client: value 简单来说，上述的流程主要分为Open和Get两个步骤，其中，Get触发TableFind后，会调用Open并向缓存中写入对该Table的句柄： SSTable 查找入口 通过 Version 对象获取 SSTable 元数据； 调用 TableCache 获取对应的 SSTable 实例（使用 LRU 缓存）； Open 流程： 文件句柄管理：通过 FileSystem 打开文件； Footer 读取：获取索引块和元数据块的位置； Index Block 读取：建立 key 到数据块的映射关系 Get流程： 使用二分查找索引快速定位 key 所在的数据块 内部通过 BlockIter 实现： 先通过 FindMidPoint 进行二分比较 再通过 LinearScan 进行精确匹配 数据块读取 缓存优先策略： 先检查 BlockCache（默认大小 8MB） 未命中时从文件读取并更新缓存 数据块查找过程 再次使用二分查找定位重启点 线性扫描块内条目进行精确匹配 解析出 key-value 对并返回结果 在经典LevelDB结构中，在启用布隆过滤器且所有块未缓存读取的情况下，SSTable时与文件系统的交互次数如下所示： 打开SSTable文件，NewRandomAccessFile：首次访问时创建文件句柄； 读取Footer元数据，从文件末尾读取48字节校验元数据 读取索引块（Index Block），根据Footer定位并读取索引块 读取元数据块（Metaindex Block），解析Footer获取元数据块位置并读取，用于定位过滤器 读取过滤块（Filter Block），从元数据块解析过滤块位置并读取 读取数据块（Data Block），通过索引块找到目标数据块的位置并读取 总计：6次文件交互 BlockBasedTable Format 如图所示，SST 文件从头到尾分成5个部分： 一个 block默认的block大小为4k，通常设置为64k（对应配置项：table_options.block_size）。 rocksdb的 sst 文件源于leveldb，主要的区别就是在于 MetaBlock 部分，rocksdb 的内容更多，leveldb 的 MetaBlock 当前只有 Filter 的内容。 Footer 固定48字节 指出 IndexBlock 和 MetaIndexBlock 在文件中的偏移量信息，它是元信息的元信息，它位于 sstable 文件的尾部; IndexBlock 占用一个 block 空间 记录了 DataBlock 相关的元信息; MetaIndexBlock 占用一个 block 空间 各个元信息的Block，包括Filter、Properties(整个table的属性信息)、Compression dictionary、Range deletion tombstone; MetaBlock 可能占用多个 block空间 存储布隆过滤器的二进制数据 及其他元信息数据; DataBlock 可能占用多个 block空间 存储实际的数据即键值对内容 BlockBasedTableReader 读流程RocksDB截止到编写该文档之前拥有多种文件结构，而BlockBasedTableFormat就是最常见的SST结构，在开始下面的章节之前，希望能够带着一些问题看具体的设计： 当我们在谈论Prefetch时，我们到底在谈论什么？ Prefetch和Cache的区别在什么地方？ 异步IO和Prefetch和缓存结构在LSM结构下如何协同工作？ Open 流程sequenceDiagram participant User participant BlockBasedTable participant FilePrefetchBuffer participant BlockFetcher participant Cache participant Filter participant IndexReader User->>BlockBasedTable: Open(file, options) BlockBasedTable->>FilePrefetchBuffer: PrefetchTail FilePrefetchBuffer-->>BlockBasedTable: PrefetchBuffer initialized BlockBasedTable->>BlockFetcher: ReadFooterFromFile BlockFetcher-->>BlockBasedTable: Footer read BlockBasedTable->>BlockFetcher: ReadMetaIndexBlock BlockFetcher-->>BlockBasedTable: Metaindex block read BlockBasedTable->>BlockFetcher: ReadPropertiesBlock BlockFetcher-->>BlockBasedTable: Properties block read BlockBasedTable->>BlockFetcher: ReadRangeDelBlock BlockFetcher-->>BlockBasedTable: Range deletion block read BlockBasedTable->>IndexReader: CreateIndexReader IndexReader-->>BlockBasedTable: Index reader created BlockBasedTable->>Filter: CreateFilterBlockReader Filter-->>BlockBasedTable: Filter reader created BlockBasedTable->>Cache: Insert index and filter blocks Cache-->>BlockBasedTable: Blocks cached BlockBasedTable-->>User: Table reader returned 简单来说，RocksDB的Open函数也是在同样的流程逻辑中被调用，当memtable查询实效后，转而在version列表中查找，如果TableCache不命中已经hold在内存中的Table，那么就需要调用Open开始新的SSTable读入，同时，具体的： BlockBasedTable 调用 FilePrefetchBuffer 预取文件尾部，获取Footer； 读取文件 Footer，通过 BlockFetcher 读取文件末尾的 48 字节 Footer，包含元索引块、索引块等关键元数据的位置信息； 加载元索引块，根据 Footer 定位MetaIndex Block，元索引块存储属性块、过滤器块等元数据的位置信息； 读取属性块，解析 SSTable 属性（键数量、压缩类型等），为后续读取提供必要的配置信息； 加载范围删除，读取并解析 Range Deletion Block，记录文件中被删除的键范围信息； 创建索引读取器，基于索引块创建 IndexReader； 初始化过滤器读取器，创建 FilterBlockReader，读取相应的布隆过滤器； 缓存关键元数据，将索引块和过滤器块存入 Cache，减少后续读取时的磁盘 I/O 操作； 返回完成初始化的TableReader。 结合上述LevelDB的Open 流程，实际上这里会发生多次File文件指针的读取，那么RocksDB首先做的就是调用PrefetchTail把相应的Footer都load到内存中来。 PrefetchTail 函数graph TD A[开始] --> B[确定预读取大小] B --> C{tail_size != 0?} C -->|是| D[使用 tail_size 作为预读取大小] C -->|否| E[从 tail_prefetch_stats 获取建议大小] E --> F{获取成功?} F -->|是| G[使用建议大小] F -->|否| H[使用默认大小] H --> J[计算预读取范围] J --> K{文件大小 < 预读取大小?} K -->|是| L[从文件开头读取整个文件] K -->|否| M[从文件尾部向前计算预读取范围] M --> N[尝试文件系统预取] N --> O{文件系统支持预取?} O -->|是| P[使用文件系统预取] O -->|否| Q[创建 FilePrefetchBuffer] L --> R[尝试文件系统预取] R --> S{文件系统支持预取?} S -->|是| T[使用文件系统预取] S -->|否| U[创建 FilePrefetchBuffer] T --> V[返回成功状态] U --> W[使用 FilePrefetchBuffer 预取] W --> V P --> V Q --> W V[返回成功状态] --> X[结束] 确定预读取大小： 如果 tail_size 不为 0，直接使用该值。 否则，尝试从 tail_prefetch_stats 获取建议大小。如果获取失败，则根据 prefetch_all 和 preload_all 参数使用默认值（512 KB 或 4 KB）。 计算预读取范围： 如果文件大小小于预读取大小，则从文件开头读取整个文件。 否则，从文件尾部向前计算预读取的起始偏移量和长度。 尝试文件系统预取： 如果文件系统支持预取，则直接使用文件系统的预取功能。 如果文件系统不支持预取，则创建一个 FilePrefetchBuffer 实例，并使用它来执行预读取操作。 返回成功状态： 无论使用哪种预取方式，最终都会返回成功状态。 关于tail_prefetch_stats的设计，一言以蔽之，通过限制浪费比例（≤12.5%），确保预取带来的I/O效率提升不会被过多的空间浪费抵消： 数据结构与数据收集 环形缓冲区：使用固定大小的环形缓冲区records_存储最近的kNumTracked次有效预取长度。当新数据到达时，覆盖旧数据，保持最新的记录。 RecordEffectiveSize方法：记录每次预取的有效长度，维护数据的更新。 算法步骤： a. 数据准备 复制并排序历史数据：将缓冲区中的数据复制到sorted向量中，并按升序排序。排序后便于后续遍历和计算。 b. 遍历候选预取大小 初始化参数：prev_size记录前一个候选大小，max_qualified_size记录当前满足条件的最大候选，wasted累计浪费量。 遍历排序后的数据：从第二个元素开始（i=1），逐个计算每个候选的浪费情况。 计算总读取量：read = sorted[i] * sorted.size()，假设所有历史记录均按当前候选大小预取。 更新浪费量：wasted += (sorted[i] - prev_size) * i。增量浪费为当前候选与前一个候选的差值乘以之前的记录数（i个记录会被填充到当前候选大小）。 判断条件：若累计浪费wasted ≤ read / 8，则当前候选大小合格，更新max_qualified_size。 c. 确定最终预取大小 上限限制：设置最大预取大小kMaxPrefetchSize=512KB，避免过度预取。 返回结果：取max_qualified_size与kMaxPrefetchSize的较小值作为最终建议值。 核心逻辑 浪费计算：假设历史记录按升序排列为[A, B, C, D, E]，当候选为C时： 所有记录的预取大小均设为C。 总读取量为C * 5。 浪费量为：(B - A)*1 + (C - B)*2（前两个记录的浪费）。 条件判断：若浪费不超过总读取量的1/8，则认为该候选大小合理。 示例 假设Prefetch历史记录排序后为[64KB, 128KB, 256KB, 512KB]： 候选256KB时： 总读取量：256KB * 4 = 1024KB。 浪费：(128-64)*1 + (256-128)*2 = 64 + 256 = 320KB。 条件：320KB ≤ 1024KB/8 = 128KB → 不满足，故256KB不合格。 候选128KB时： 总读取量：128KB * 4 = 512KB。 浪费：(128-64)*1 = 64KB。 条件：64KB ≤ 512KB/8 = 64KB → 满足，故128KB合格。 最终建议值为128KB（若未超过512KB上限）。 ReadFooterFromFile 函数graph TD A[开始] --> B[检查文件大小] B -->|文件大小小于最小编码长度| C[返回错误] B -->|文件大小足够| D[准备读取缓冲区] D --> E[确定读取偏移量] E --> F[尝试从预取缓冲区读取] F -->|预取缓冲区中有数据| G[读取数据成功] F -->|预取缓冲区中无数据| H[从文件读取] H -->|使用直接 I/O| I[通过 AlignedBuf 读取] H -->|不使用直接 I/O| J[直接读取到 footer_buf] I --> K[检查读取结果] J --> K K -->|读取失败| L[返回错误] K -->|读取成功| M[验证读取数据] M -->|数据不完整| N[获取实际文件大小并返回错误] M -->|数据完整| O[解析 Footer 数据] O --> P[验证魔数] P -->|验证失败| Q[返回错误] P -->|验证成功| R[返回成功] G --> M R[返回成功] --> S[结束] L --> S N --> S Q --> S ReadFooterFromFile 函数用于从文件中读取 SST 文件的 Footer，并验证其完整性。Footer 包含了文件的魔数（Magic Number）和元数据块的块句柄等重要信息。 prefetch 在这里的作用： 在 ReadFooterFromFile 函数中，prefetch 的作用是尝试从预取缓冲区中读取 Footer 数据。如果预取缓冲区中已经包含了所需的 Footer 数据，则可以直接使用，避免了从磁盘读取的开销。这在顺序读取或频繁访问相同文件的情况下特别有用，可以显著提高读取性能。 具体来说，prefetch_buffer-&gt;TryReadFromCache 方法会检查预取缓冲区中是否包含指定偏移量和长度的数据。如果包含，则直接返回数据；如果不包含，则返回 false，表示需要从文件中读取数据。 ReadMetaIndexBlock 函数graph TD A[开始] --> B[创建 BlockFetcher 对象] B --> C{启用异步读取且预取缓冲区存在?} C -->|是| D[调用 ReadAsyncBlockContents] C -->|否| E[调用 ReadBlockContents] D --> F[尝试从持久化缓存获取未压缩块] F -->|成功| G[设置压缩类型为无压缩并返回成功] F -->|失败| H[尝试从持久化缓存获取序列化块] H -->|失败| I[使用预取缓冲区异步预取数据块] I --> J[处理预取数据] J --> K[处理数据块的拖尾信息] K --> L{需要重新读取?} L -->|是| M[重新读取数据块] L -->|否| N[解压缩或获取块内容] N --> O[插入未压缩块到持久化缓存] O --> P[返回成功] E --> Q[尝试从持久化缓存获取未压缩块] Q -->|成功| R[设置压缩类型为无压缩并返回成功] Q -->|失败| S[尝试从预取缓冲区获取数据] S -->|成功| T[处理读取错误] S -->|失败| U[从文件读取数据块] U --> V[处理读取错误] V --> W[解压缩或获取块内容] W --> X[插入未压缩块到持久化缓存] X --> Y[返回成功] P --> Z[结束] Y --> Z ReadMetaIndexBlock 函数用于从文件中读取并解析 SST 文件的元数据索引块（Meta Index Block）。元数据索引块包含文件中其他元数据块（如属性块、过滤器块等）的块句柄，对于后续访问这些元数据块至关重要。 这里有几个关键的函数： 入口函数 ReadMetaIndexBlock 创建 Meta Index Block 对象：创建一个 Block_kMetaIndex 对象，用于存储读取和解析后的元数据索引块。 调用辅助函数：调用 ReadAndParseBlockFromFile 函数，传入文件、预取缓冲区、页脚、读取选项、块句柄等参数，读取并解析元数据索引块。 错误处理：如果读取失败，记录错误日志并返回错误状态。 返回结果：将解析后的元数据索引块和其迭代器返回给调用者。 辅助函数 ReadAndParseBlockFromFile 创建 BlockFetcher 对象：根据传入的参数创建一个 BlockFetcher 对象，用于处理块的读取和解析。 异步读取或同步读取：根据是否启用异步读取和预取缓冲区的存在，选择调用 ReadAsyncBlockContents 或 ReadBlockContents。 解析块内容：如果读取成功，使用 BlockCreateContext 创建并解析块内容。 返回状态：返回读取和解析的结果状态。 辅助类BlockFetcher  ReadAsyncBlockContents 尝试从持久化缓存获取未压缩块：如果成功获取，设置压缩类型为无压缩并返回成功。 尝试从持久化缓存获取序列化块：如果失败，使用预取缓冲区异步预取数据块。 处理预取数据：如果预取成功，处理数据块的拖尾信息；如果需要，重新读取以修复损坏的数据块。 解压缩或获取块内容：根据是否需要解压缩，对数据块进行相应的处理。 插入未压缩块到持久化缓存：如果需要，将未压缩的块插入到持久化缓存中。 辅助类ReadBlockContents 尝试从持久化缓存获取未压缩块：如果成功获取，设置压缩类型为无压缩并返回成功。 尝试从预取缓冲区获取数据：如果预取缓冲区中有数据，直接使用；否则从文件中读取数据块。 处理读取错误：如果读取失败且支持重新读取，尝试重新读取数据块。 解压缩或获取块内容：根据是否需要解压缩，对数据块进行相应的处理。 插入未压缩块到持久化缓存：如果需要，将未压缩的块插入到持久化缓存中。  prefetch 在这里的作用： 再次减少磁盘 I/O 次数：通过预取机制，在读取当前数据块的同时，预测并提前读取可能需要的后续数据块，减少磁盘 I/O 次数。 提高读取性能：预取的数据块存储在内存缓冲区中，后续读取时可以直接从缓冲区获取，避免了频繁的磁盘读取操作，提高了读取性能。 支持异步读取：与异步读取结合，允许在不阻塞主线程的情况下预取数据，进一步提高系统的并发处理能力。 异步读取： 非阻塞操作：异步读取允许在提交读取请求后立即返回，而不等待数据实际读取完成，避免等待。 提高并发性：在等待异步读取完成的同时，可以提前开始Get。 但是针对BlockFetcher这里并没有进行展开，其中封装了对ReadBlock的诸多处理逻辑，这部分在下文展开。 PrefetchIndexAndFilterBlocks 函数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263函数 PrefetchIndexAndFilterBlocks(ro, prefetch_buffer, meta_iter, new_table, prefetch_all, table_options, level, file_size, max_file_size_for_l0_meta_pin, lookup_context): // 查找过滤器块句柄和类型 如果 rep_-&gt;filter_policy 存在: name = rep_-&gt;filter_policy-&gt;CompatibilityName() 对于 每个 filter_type 和 prefix 在 [全量过滤器, 分区过滤器, 过时过滤器] 中: filter_block_key = prefix + name 如果 FindMetaBlock(meta_iter, filter_block_key, &amp;rep_-&gt;filter_handle) 成功: rep_-&gt;filter_type = filter_type 如果 filter_type 是过时过滤器: 记录警告日志 跳出循环 // 查找压缩字典块句柄 s = FindOptionalMetaBlock(meta_iter, kCompressionDictBlockName, &amp;rep_-&gt;compression_dict_handle) 如果 s 不成功: 返回 s // 确定缓存和预读取策略 use_cache = table_options.cache_index_and_filter_blocks maybe_flushed = (level == 0) 并且 (file_size &lt;= max_file_size_for_l0_meta_pin) 定义 is_pinned 函数: 参数 pinning_tier 和 fallback_pinning_tier 根据 pinning_tier 和 maybe_flushed 确定是否缓存 // 确定是否缓存和预读取索引块 pin_index = 如果是二级索引搜索则使用 pin_top_level_index，否则使用 pin_unpartitioned prefetch_index = prefetch_all 或者 pin_index // 创建索引读取器 s = new_table-&gt;CreateIndexReader(ro, prefetch_buffer, meta_iter, use_cache, prefetch_index, pin_index, lookup_context, &amp;index_reader) 如果 s 不成功: 返回 s rep_-&gt;index_reader = 移动(index_reader) // 缓存依赖的索引块分区 如果 prefetch_all 或者 pin_partition: s = rep_-&gt;index_reader-&gt;CacheDependencies(ro, pin_partition, prefetch_buffer) 如果 s 不成功: 返回 s // 确定是否缓存和预读过滤器块 pin_filter = 如果是分区过滤器则使用 pin_top_level_index，否则使用 pin_unpartitioned prefetch_filter = prefetch_all 或者 pin_filter 如果 rep_-&gt;filter_policy 存在: filter = new_table-&gt;CreateFilterBlockReader(ro, prefetch_buffer, use_cache, prefetch_filter, pin_filter, lookup_context) 如果 filter 不为空: 如果 prefetch_all 或者 pin_partition: s = filter-&gt;CacheDependencies(ro, pin_partition, prefetch_buffer) 如果 s 不成功: 返回 s rep_-&gt;filter = 移动(filter) // 创建压缩字典读取器 如果 rep_-&gt;compression_dict_handle 不为空: s = UncompressionDictReader::Create(this, ro, prefetch_buffer, use_cache, prefetch_all || pin_unpartitioned, pin_unpartitioned, lookup_context, &amp;uncompression_dict_reader) 如果 s 不成功: 返回 s rep_-&gt;uncompression_dict_reader = 移动(uncompression_dict_reader) 返回 s 入口函数 PrefetchIndexAndFilterBlocks 查找过滤器块句柄和类型： 根据过滤器策略的兼容名称，尝试查找不同类型的过滤器块（如全量过滤器、分区过滤器等）。 如果找到，设置过滤器类型和块句柄；如果找到的是过时的过滤器类型，记录警告日志。 查找压缩字典块句柄： 调用 FindOptionalMetaBlock 查找压缩字典块的块句柄。 确定缓存和预读取策略： 根据配置和文件级别，确定是否缓存和预读取索引块、过滤器块和压缩字典块。 创建索引读取器： 根据索引类型，创建相应的索引读取器（如分区索引读取器、二分查找索引读取器等）。 如果是哈希索引，检查是否缺少前缀提取器，如果是，则回退到二分查找索引。 缓存依赖的索引块分区： 如果需要预读取或缓存分区，调用索引读取器的 CacheDependencies 方法。 创建过滤器块读取器： 根据过滤器类型，创建相应的过滤器块读取器（如分区过滤器块读取器、全量过滤器块读取器等）。 如果需要预读取或缓存分区，调用过滤器块读取器的 CacheDependencies 方法。 创建压缩字典读取器： 如果存在压缩字典块句柄，创建压缩字典读取器。 辅助函数 FindOptionalMetaBlock 和 FindMetaBlock 查找元数据块： 使用元数据索引迭代器查找指定名称的元数据块。 如果找到，解析块句柄并返回；如果未找到，返回空块句柄和相应状态。 辅助函数 CreateIndexReader 确定索引块句柄： 根据文件页脚格式版本，从页脚中获取索引块句柄，或通过元数据索引迭代器查找。 创建索引读取器： 根据索引类型，创建相应的索引读取器。 辅助函数 CreateFilterBlockReader 创建过滤器块读取器： 根据过滤器类型，创建相应的过滤器块读取器。 辅助函数 UncompressionDictReader::Create 和 ReadUncompressionDictionary 读取压缩字典： 如果需要预读取或不使用缓存，读取压缩字典块并缓存。 创建压缩字典读取器。 Get 流程sequenceDiagram participant User participant BlockBasedTable participant FilterBlockReader participant IndexIterator participant DataBlockIter participant BlockCache participant Prefetcher User->>BlockBasedTable: Get(key) BlockBasedTable->>BlockBasedTable: TimestampMayMatch() alt Timestamp不匹配 BlockBasedTable-->>User: 返回Status::OK() else BlockBasedTable->>FilterBlockReader: FullFilterKeyMayMatch() FilterBlockReader-->>BlockBasedTable: may_match alt may_match=false BlockBasedTable-->>User: 返回Status::OK() else BlockBasedTable->>IndexIterator: NewIndexIterator() IndexIterator->>BlockBasedTable: iiter loop 遍历索引块 BlockBasedTable->>IndexIterator: Seek(key) IndexIterator->>BlockBasedTable: 返回IndexValue(v) BlockBasedTable->>BlockBasedTable: NewDataBlockIterator() BlockBasedTable->>Prefetcher: PrefetchIfNeeded() Prefetcher->>BlockCache: 检查/预取数据块 BlockCache-->>Prefetcher: 数据块句柄 BlockBasedTable->>DataBlockIter: SeekForGet(key) DataBlockIter->>BlockBasedTable: may_exist alt may_exist loop 遍历数据块条目 DataBlockIter->>GetContext: SaveValue() GetContext-->>DataBlockIter: 是否继续 end end end end end BlockBasedTable-->>User: 返回Status 用户发起请求 用户调用BlockBasedTable的Get(key)方法，传入要查找的键key，启动数据获取流程。 时间戳匹配检查 BlockBasedTable接收到请求后，首先调用自身的TimestampMayMatch()方法，检查当前时间戳是否匹配。 如果时间戳不匹配，BlockBasedTable直接向用户返回Status::OK()，表示操作完成但未找到匹配数据，流程结束。 如果时间戳匹配，流程进入下一步。 过滤块匹配检查 BlockBasedTable调用FilterBlockReader的FullFilterKeyMayMatch()方法，传入完整过滤键，检查该键是否可能在过滤块中匹配。 FilterBlockReader执行匹配检查后，将结果may_match返回给BlockBasedTable。 如果may_match为false，说明过滤块中明确不存在该键，BlockBasedTable向用户返回Status::OK()，流程结束。 如果may_match为true，说明过滤块中可能存在该键，流程继续。 创建索引迭代器 BlockBasedTable调用IndexIterator的NewIndexIterator()方法，创建一个新的索引迭代器。 IndexIterator返回创建好的索引迭代器iiter给BlockBasedTable。 遍历索引块 进入循环，遍历索引块： BlockBasedTable调用IndexIterator的Seek(key)方法，在索引块中查找与键key匹配的索引项。 IndexIterator返回找到的索引值v给BlockBasedTable。 BlockBasedTable根据索引值v，调用自身的NewDataBlockIterator()方法，创建一个新的数据块迭代器。 BlockBasedTable调用Prefetcher的PrefetchIfNeeded()方法，检查是否需要进行数据预取。 Prefetcher检查或预取数据块，与BlockCache交互，获取数据块句柄，并将句柄返回给Prefetcher。 BlockBasedTable调用DataBlockIter的SeekForGet(key)方法，在数据块中查找键key。 DataBlockIter返回是否可能存在匹配数据的标志may_exist给BlockBasedTable。 如果may_exist为true，进入一个循环，遍历数据块条目： DataBlockIter调用GetContext的SaveValue()方法，保存找到的值。 GetContext返回是否继续遍历的指示给DataBlockIter。 循环结束后，继续遍历索引块，直到所有可能的索引项都被检查完毕。 返回结果给用户 完成所有遍历和查找操作后，BlockBasedTable向用户返回最终的Status，表示数据获取操作的结果。 再次重申相关类的职责的划分： BlockBasedTable：作为核心协调者，负责接收用户请求，进行初步的时间戳和过滤块匹配检查，创建和管理索引迭代器及数据块迭代器，与预取器交互，以及最终将结果返回给用户。 FilterBlockReader：负责执行过滤块的匹配检查，快速筛选出不可能包含目标键的块，提高查找效率。 IndexIterator：用于遍历索引块，查找与目标键匹配的索引项，提供索引值以便进一步定位数据块。 DataBlockIter：在数据块中进行具体的数据查找，根据给定的键定位到对应的条目，并与GetContext协作保存找到的值。 BlockCache：缓存数据块，提高数据访问速度，当需要预取数据块时，提供数据块句柄。 Prefetch：负责根据需要进行数据预取操作，提前将可能需要的数据块加载到缓存中，减少后续访问的延迟。 BlockBasedTableIterator SeekImplgraph TD A[开始SeekImpl] --> B{首次调用?} B -- 是 --> C[设置seek_key_prefix_for_readahead_trimming_] C --> D[重置缓存查找变量] D --> E{自动调整readahead_size?} E -- 是 --> F[设置readahead_cache_lookup_=true] E -- 否 --> G F --> G[CheckPrefixMayMatch] G -- 前缀不匹配 --> H[重置数据迭代器] G -- 匹配 --> I{需要重新定位索引?} I -- 是 --> J[IndexIterator.Seektarget] I -- 否 --> K[检查是否同一数据块] J --> K K --> L{同一数据块且有效?} L -- 是 --> M[直接使用现有数据块迭代器] L -- 否 --> N[初始化数据块] N --> O{异步预取开启?} O -- 是 --> P[AsyncInitDataBlock首次尝试] P -- 返回TryAgain --> Q[设置async_read_in_progress_=true] O -- 否 --> R[InitDataBlock同步初始化] Q --> S[结束首次调用] R --> M M --> T[执行数据块Seek] T --> U[FindKeyForward前向查找] U --> V[检查边界条件] V --> W[断言验证位置] W --> Z[结束流程] B -- 否 --> X[SeekSecondPass二次处理] X --> Y[AsyncInitDataBlock二次尝试] Y --> T 首次调用流程 首次调用判断：SeekImpl进入之后判断是否是首次调用。如果是首次调用，则设置用于读取预取修剪的前缀键，以优化读取性能；否则跳转到“SeekSecondPass二次处理”。 设置与重置：在首次调用的情况下，设置seek_key_prefix_for_readahead_trimming_后，重置缓存查找变量，为后续操作做准备。 自动调整判断：判断是否自动调整readahead_size。如果是，设置readahead_cache_lookup_=true，表示启用读取预取缓存查找；否则直接进入“CheckPrefixMayMatch”节点。 前缀匹配检查：在“CheckPrefixMayMatch”节点，检查前缀是否匹配。前缀不匹配则重置数据迭代器，重新开始数据查找；匹配则进一步判断是否需要重新定位索引。 索引重新定位：如果需要重新定位索引，执行IndexIterator.Seektarget操作，对索引进行定位；否则检查是否同一数据块。 数据块处理：检查是否同一数据块且有效。如果是，直接使用现有数据块迭代器；否则初始化数据块。初始化数据块时，判断异步预取是否开启。开启则进行AsyncInitDataBlock首次尝试，若返回TryAgain，设置async_read_in_progress_=true；不开启则同步初始化数据块。 数据查找与结束：数据块初始化后，执行数据块Seek操作，进行数据查找。通过FindKeyForward前向查找，检查边界条件，断言验证位置，最终结束流程。 非首次调用流程 二次处理：如果不是首次调用，进入“SeekSecondPass二次处理”，再次尝试AsyncInitDataBlock，然后执行数据块Seek操作，继续后续的数据查找流程，直至结束。 异步预取工作流程： 首次Seek调用： 检查async_read_in_progress_标志 触发AsyncInitDataBlock(is_first_pass=true) 通过PrefetchIfNeeded发起异步IO请求 如果数据不在缓存中，返回Status::TryAgain 设置async_read_in_progress_=true 二次Seek调用： 进入SeekSecondPass 调用AsyncInitDataBlock(is_first_pass=false) 使用已预取的数据块初始化block_iter_ 执行常规Seek操作 总结： 预取机制：系统通过预取数据到缓冲区，减少用户实际读取时的等待时间，提高性能。 异步 IO：文件读取操作是异步的，不会阻塞用户线程，适合处理大量数据或高延迟的 IO 操作。 缓冲区管理：缓冲区会动态分配和清理，确保资源高效利用，同时通过状态标记（如 async_read_in_progress）管理读取过程； 回调机制：IO 完成后通过回调通知； FilePrefetchBuffer结构设计sequenceDiagram participant User participant FilePrefetchBuffer participant RandomAccessFileReader participant FileSystem participant BufferInfo User->>FilePrefetchBuffer: PrefetchAsync(opts, reader, offset, n, result) FilePrefetchBuffer->>FilePrefetchBuffer: 中止未完成IO (AbortAllIOs) FilePrefetchBuffer->>FilePrefetchBuffer: 清理过期数据 (ClearOutdatedData) alt 数据已在缓冲区 FilePrefetchBuffer->>User: 立即返回Status::OK else 需要新预取 FilePrefetchBuffer->>BufferInfo: 分配新缓冲区 (AllocateBuffer) FilePrefetchBuffer->>BufferInfo: 计算预取范围 (ReadAheadSizeTuning) FilePrefetchBuffer->>RandomAccessFileReader: ReadAsync(请求) RandomAccessFileReader->>FileSystem: 提交异步IO请求 FileSystem-->>RandomAccessFileReader: 返回IO句柄 RandomAccessFileReader-->>FilePrefetchBuffer: 存储io_handle FilePrefetchBuffer->>BufferInfo: 标记async_read_in_progress=true end loop 异步回调处理 FileSystem->>FilePrefetchBuffer: PrefetchAsyncCallback(请求结果) FilePrefetchBuffer->>BufferInfo: 更新缓冲区数据 FilePrefetchBuffer->>BufferInfo: async_read_in_progress=false end User->>FilePrefetchBuffer: TryReadFromCache FilePrefetchBuffer->>BufferInfo: 检查数据有效性 BufferInfo-->>User: 返回数据切片 用户发起预取请求 向 FilePrefetchBuffer 发起 PrefetchAsync 请求，传入参数包括选项（opts）、文件读取器（reader）、偏移量（offset）、预取字节数（n）以及结果回调（result）。 这是整个流程的起点，用户希望从文件的某个位置开始预取数据。 预取缓冲区的初始化操作 FilePrefetchBuffer 收到请求后，首先执行两个内部操作： 中止未完成的 IO 操作（AbortAllIOs）：确保没有遗留的 IO 请求在处理，避免冲突。 清理过期数据（ClearOutdatedData）：移除缓冲区中不再需要或已经失效的数据，为新数据腾出空间。 检查数据是否已在缓冲区 FilePrefetchBuffer 检查请求的数据是否已经存在于缓冲区中。 如果数据已在缓冲区（数据已在缓冲区分支）： 直接向 User 返回 Status::OK，表示预取成功，无需进一步操作。 如果需要新预取（需要新预取分支）： FilePrefetchBuffer 向 BufferInfo 发起两个请求： 分配新缓冲区（AllocateBuffer）：为即将预取的数据分配新的缓冲空间。 计算预取范围（ReadAheadSizeTuning）：根据策略计算本次预取的实际范围，可能比用户请求的范围更大，以提高效率。 FilePrefetchBuffer 调用 RandomAccessFileReader 的 ReadAsync 方法，发起异步读取请求。 RandomAccessFileReader 将读取请求提交给 FileSystem，由后者执行实际的 IO 操作。 FileSystem 返回 IO 句柄给 RandomAccessFileReader，后者将其存储在 FilePrefetchBuffer 中。 FilePrefetchBuffer 通过 BufferInfo 标记 async_read_in_progress=true，表示异步读取正在进行。 异步回调处理 当 FileSystem 完成 IO 操作后，会触发回调 PrefetchAsyncCallback，将请求结果传递给 FilePrefetchBuffer。 FilePrefetchBuffer 收到回调后，通过 BufferInfo 更新缓冲区数据，并将 async_read_in_progress 标记为 false，表示读取完成。 这个过程可能循环多次，处理多个 IO 请求的回调。 用户尝试从缓存读取数据 User 调用 FilePrefetchBuffer 的 TryReadFromCache 方法，尝试从缓冲区中读取数据。 FilePrefetchBuffer 通过 BufferInfo 检查数据的有效性，确保数据完整且可用。 最终，BufferInfo 将数据切片返回给 User，完成整个流程。 TryReadFromCachegraph TD A[TryReadFromCache调用] --> B{数据在缓冲区?} B -- 是 --> C[直接返回数据] B -- 否 --> D{需要预取?} D -- 是 --> E[触发Prefetch/PrefetchAsync] E --> F{同步模式?} F -- 是 --> G[同步读取数据到缓冲区] F -- 否 --> H[提交异步IO请求] H --> I[注册回调函数] I --> J[后台处理IO完成] J --> K[更新缓冲区状态] D -- 否 --> L[返回未命中] subgraph 异步处理流程 H --> M[文件系统处理请求] M --> N[数据就绪触发回调] N --> O[将数据拷贝到缓冲区] O --> P[标记IO完成] end 检查数据是否在缓冲区 判断数据是否已经在缓冲区中： 如果数据在缓冲区，直接返回数据，流程结束。 如果数据不在缓冲区，进入下一步判断是否需要预取。 判断是否需要预取 判断当前情况下是否需要进行数据预取： 如果需要预取，触发Prefetch或PrefetchAsync操作，进入预取流程。 如果不需要预取，直接返回未命中结果，流程结束。 预取流程 触发预取操作：根据配置或策略，决定是进行同步预取还是异步预取。 同步模式判断：判断是否采用同步模式进行预取： 如果是同步模式，直接进行同步读取数据到缓冲区的操作。 如果是异步模式，提交异步IO请求，并注册回调函数，等待后台处理IO完成。 异步处理流程 提交异步IO请求：将IO请求提交给文件系统。 注册回调函数：为IO完成事件注册回调函数，以便在数据就绪时进行后续处理。 后台处理IO完成：在后台等待IO操作完成。 更新缓冲区状态：IO完成后，更新缓冲区的状态，反映数据已加载到缓冲区。 异步完成流程 文件系统处理请求：文件系统接收到IO请求后，进行相应的处理。 数据就绪触发回调：当数据准备就绪时，触发之前注册的回调函数。 将数据拷贝到缓冲区：在回调函数中，将数据从文件系统拷贝到缓冲区。 标记IO完成：完成数据拷贝后，标记IO操作为已完成。 BlockFetcher类设计classDiagram class BlockFetcher { -file_ : RandomAccessFileReader* -prefetch_buffer_ : FilePrefetchBuffer* -heap_buf_ : CacheAllocationPtr -compressed_buf_ : CacheAllocationPtr -direct_io_buf_ : AlignedBuf -ReadBlock(bool) : void -TryGetFromPrefetchBuffer() : bool -TryGetSerializedBlockFromPersistentCache() : bool +ReadBlockContents() : IOStatus } class FilePrefetchBuffer { +TryReadFromCache(...) : bool } class PersistentCacheHelper { +LookupUncompressed(...) : Status +InsertSerialized(...) : void } class RandomAccessFileReader { +Read(...) : IOStatus +MultiRead(...) : IOStatus } BlockFetcher --> FilePrefetchBuffer : 使用 BlockFetcher --> PersistentCacheHelper : 使用 BlockFetcher --> RandomAccessFileReader : 使用 BlockFetcher --> Compression : 解压处理  FilePrefetchBuffer：BlockFetcher 通过其 prefetch_buffer_ 属性与 FilePrefetchBuffer 交互，调用 TryReadFromCache 方法尝试从预取缓冲区获取数据。  PersistentCacheHelper：BlockFetcher 使用 PersistentCacheHelper 来从持久化缓存中获取未压缩数据或插入序列化数据。 RandomAccessFileReader：BlockFetcher 通过 file_ 属性与 RandomAccessFileReader 交互，直接从文件读取数据。 Compression：BlockFetcher 在需要时调用 Compression 组件进行解压处理。 ReadBlock 流程sequenceDiagram participant Caller participant BlockFetcher participant PersistentCache participant FilePrefetchBuffer participant FileReader Caller->>BlockFetcher: ReadBlockContents() alt 持久化缓存命中 BlockFetcher->>PersistentCache: LookupUncompressed PersistentCache-->>BlockFetcher: 返回未压缩块 else 预取缓冲区命中 BlockFetcher->>FilePrefetchBuffer: TryReadFromCache FilePrefetchBuffer-->>BlockFetcher: 返回数据切片 BlockFetcher->>BlockFetcher: 处理尾部校验 else 需要文件读取 BlockFetcher->>BlockFetcher: PrepareBufferForBlockFromFile BlockFetcher->>FileReader: Read/MultiRead FileReader->>FileSystem: 提交IO请求 FileSystem-->>FileReader: 返回数据 FileReader-->>BlockFetcher: 返回IO状态 BlockFetcher->>BlockFetcher: 处理尾部校验 alt 数据压缩 BlockFetcher->>Compression: 解压处理 end BlockFetcher->>PersistentCache: 插入缓存 end BlockFetcher-->>Caller: 返回IO状态 调用ReadBlockContents 流程起始于Caller调用BlockFetcher的ReadBlockContents()方法，表示请求读取某个数据块的内容。 检查持久化缓存是否命中 BlockFetcher首先尝试从持久化缓存中获取数据，调用PersistentCache的LookupUncompressed方法。 如果持久化缓存命中，PersistentCache返回未压缩块给BlockFetcher，流程直接进入结束步骤，返回IO状态给调用者。 如果持久化缓存未命中，流程进入下一步。 检查预取缓冲区是否命中 BlockFetcher调用FilePrefetchBuffer的TryReadFromCache方法，尝试从预取缓冲区中读取数据。 如果预取缓冲区命中，FilePrefetchBuffer返回数据切片给BlockFetcher。 BlockFetcher处理尾部校验，确保数据的完整性。 流程进入结束步骤，返回IO状态给调用者。 如果预取缓冲区未命中，流程进入下一步。 需要文件读取 BlockFetcher准备从文件中读取数据块，调用自身的PrepareBufferForBlockFromFile方法。 BlockFetcher调用FileReader的Read/MultiRead方法，提交IO读取请求。 FileReader与文件系统交互，提交IO请求给FileSystem。 FileSystem处理IO请求后，将数据返回给FileReader。 FileReader将IO状态返回给BlockFetcher。 BlockFetcher处理尾部校验，确保数据的完整性。 如果数据是压缩的，BlockFetcher调用Compression组件进行解压处理。 BlockFetcher将解压后的数据插入到PersistentCache中，以便后续读取可以命中缓存。 流程进入结束步骤，返回IO状态给调用者。 返回IO状态 无论通过哪种方式获取数据，BlockFetcher最终都会将IO状态返回给Caller，表示读取操作的结果。 graph TD Start[ReadBlockContents调用] --> CheckPersistentCache{持久缓存检查} CheckPersistentCache -- 命中 --> ReturnOK[返回成功] CheckPersistentCache -- 未命中 --> CheckPrefetch{预取缓冲检查} CheckPrefetch -- 命中 --> VerifyChecksum[校验数据] VerifyChecksum -- 失败 --> RetryRead[重试读取] CheckPrefetch -- 未命中 --> ReadFromFile[文件读取] ReadFromFile --> PrepareBuffer[准备缓冲区] PrepareBuffer --> SelectBuffer{选择缓冲策略} SelectBuffer -- 小数据 --> UseStackBuf[栈缓冲] SelectBuffer -- 压缩数据 --> UseCompressedBuf[压缩缓冲] SelectBuffer -- 普通数据 --> UseHeapBuf[堆缓冲] ReadFromFile --> PerformIO[执行IO操作] PerformIO -- 直接IO --> DirectIORead[对齐读取] PerformIO -- 普通IO --> NormalRead[常规读取] PerformIO --> CheckIntegrity[完整性检查] CheckIntegrity -- 损坏 --> RetryRead CheckIntegrity -- 正常 --> ProcessTrailer[处理尾部] ProcessTrailer --> Decompress{需要解压?} Decompress -- 是 --> Uncompress[解压数据] Decompress -- 否 --> StoreResult[存储结果] Uncompress --> UpdateCache[更新缓存] StoreResult --> UpdateCache UpdateCache --> ReturnOK 流程再次重申： 缓存优先策略：流程首先尝试从持久化缓存和预取缓冲区中获取数据，只有在两者都未命中时才进行文件读取，这种策略可以显著提高读取性能，减少IO操作的开销。 数据完整性校验：在从不同来源获取数据后，都会进行尾部校验，确保数据的完整性和正确性。 异步IO处理：异步IO操作，提高系统的并发处理能力。 缓存更新：在从文件读取数据并解压后，会将数据插入到持久化缓存中，这样后续的读取请求可以直接命中缓存，避免重复的文件读取和解压操作。 graph LR A[读取请求] --> B{缓冲策略选择} B -->|小数据| C[栈缓冲 stack_buf_] B -->|压缩数据| D[压缩池 compressed_buf_] B -->|普通数据| E[堆缓冲 heap_buf_] B -->|直接IO| F[对齐缓冲 direct_io_buf_] C & D & E & F --> G[BlockContents] G --> H[用户返回后释放] 这里再放一张BlockFetcher中的设计巧思，针对不同的数据大小，区分了不同的缓存池，能够针对不同的IO特点进行特定优化。 简单的bench生成命令： 12345rocks_db_bench —db=prefix_scan —env_uri=ws://ws.flash.ftw3preprod1 -logtostderr=false -benchmarks=&quot;fillseqdeterministic&quot; -key_size=32 -value_size=512 -num=5000000 -num_levels=4 -multiread_batched=true -use_direct_reads=false -adaptive_readahead=true -threads=1 -cache_size=10485760000 -async_io=false -multiread_stride=40000 -disable_auto_compactions=true -compaction_style=1 -bloom_bits=10 数据库结构 Level 0: 包含 4 个 SST 文件，大小分别为 24828520、49874113、100243447、201507232 字节 Level 1: 包含 6 个 SST 文件，总大小为 405046844 字节 Level 2: 包含 13 个 SST 文件，总大小为 814190051 字节 Level 3: 包含 23 个 SST 文件，总大小为 1515327216 字节 Scan : 12345rocks_db_bench -use_existing_db=true —db=prefix_scan -benchmarks=&quot;seekrandom&quot; -key_size=32 -value_size=512 -num=5000000 -batch_size=8 -multiread_batched=true -use_direct_reads=false -duration=60 -ops_between_duration_checks=1 -readonly=true -threads=4 -cache_size=300000000 -async_io=true -multiread_stride=40000 -statistics —env_uri=ws://ws.flash.ftw3preprod1 -logtostderr=false -adaptive_readahead=true -bloom_bits=10 -seek_nexts=65536 测试结果: 启用异步扫描 Latency (micros/op) 414442.3 Throughput (MB/s) 326.2 IOPS (ops/sec) 9 Operations 581 Found Keys 145/145 禁用异步扫描 Latency (micros/op) 848858.67 Throughput (MB/s) 158.1 IOPS (ops/sec) 4 Operations 284 Found Keys 74/74 总结 RocksDB针对原本LevelDB再SSTable读取过程中遇到的多次IO进行细节化的Prefetch，在Open和Get不同阶段进行针对性的Prefetch动作； 在Open阶段中，Prefetch了所需的Footer内容，一次IO，后续对MetaIndexBlock、FilterBlock的读取都是放在内存中进行，提高效率；在Get阶段中，在具体的SeekImpl实现中，使用异步Block读取，来优化首次读取的性能，减少了用户等待的时间。 针对Prefetch和异步的需求，分开设计了FilePrefetchBuffer和BlockFetcher两个核心类，FilePrefetchBuffer优化SSTable中的Prefetch读取，并结合BlockCache将读入的Block存入内存中，BlockFetcher类独立了Block的异步化读取操作，结合持久化缓存、BlockCache、Prefetch、文件系统级别的Prefetch，解压为一体，向上层提供了简洁的Block读取操作，屏蔽了错综复杂的IO逻辑；","link":"/2025/03/07/RocksDB-1/"},{"title":"数据库事务模型分析（1）—— 计算模型","text":"事务服务的计算模型可以通过多种方法精确创建，和抽象程度有关，而建模数据库事务服务的系统需要遵循一些基本的方法论以及相关实际因素，本系列所作的建模一般分为以下5个步骤： 首先定义数据对象的基础操作，这些操作被认为是原子的，并且独立于其他操作，这一步并没有明确定义什么是数据对象以及需要考虑什么类型数据的基础操作。因此本系列定义两种不同的计算模型：页模型和对象模型； 对事务以及事务执行顺序建模，将其视为在数据对象上基础操作的一个（全/偏）序列； 本系列使用调度或者历史的概念作为对事务执行并发的抽象； 在所有句法正确的调度中，必须从中识别出保证ACID“正确”的调度； 需要“算法”或者“协议”来即时地创建事务正确的调度，当事务提交之后，可以被动态执行； 上述的“页模型”是一个非常简单的模型，可以观察在存储层中，数据页是如何被事务优雅的读写的。接下来可以看到这个模型以一种非常优雅的方式建模了并发控制和事务恢复的实现，并且可以描述非常多（但不是全部）系统实现中的重要语义。“对象模型”是预留给数据库的上层操作，比如访问层或者查询处理层的操作，很容易想到把上层数据应用的业务数据对象考虑在内，可以推导出更复杂的对抽象数据结构ADT的语义，借由ADT来执行下层数据对象的操作。 页模型定义页模型基于一个基本假设： 所有对数据的高层操作都会转化为对数据页的读写操作，并且假设每个页的操作都是不可分的，无论是内存还是在硬盘上。 在用页模型给出事务的一般定义之前，先看一个简化的版本，把事务看作具有全序关系的一组操作步骤，而一般页模型允许偏序关系：一个事务$t$(在页模型中)是形如$r(x)$或者$w(x)$的操作步骤的有限序列，记作： $$t = p_1…p_n$$其中，$n &lt; \\infty$, 对于$1 \\leq i \\leq n$，$x \\in D$，有$p_i \\in {r(x), w(x)}$，这样就从事务执行的细节中抽象出事务执行的读写序列了。 在同一个事务中，操作步骤由$p_i$表示，即事务中第$i$步操作，存在多个事务的情况下，可以给每个事务添加一个唯一的事务id，作为步骤的另外一个下标，$p_{ij}$就代表了第$i$个事务的第$j$步，但是在页模型中为了简化问题，可以简单的从上下文推断出第几步，因此不显式给出步数。只考虑一个事务，可以表示为：$$p_j = r/w(x)$$ 理解为事务第$j$步读/写页数据。实际上，无需把一个事务的所有步骤排成一个严格先后执行的序列，可以放松对事务执行步骤全序的要求，接下来定义事务的偏序：设$A$为任意一个集合，$R \\subseteq A \\times A$，如果对于任意的元素$a, b, c \\in A$，下面三个关系都满足，则称关系$R$是集合$A$上的一个偏序： $(a, a) \\in R$ 自反性 $(a, b) \\in R \\wedge (b,a) \\in R \\Rightarrow a = b$ 反对称性 $(a, b) \\in R \\wedge (b,c) \\in R \\Rightarrow (a,c) \\in R$ 传递性 偏序是全序的一个特例，全序在偏序的定义上又增强了约束，即对于任意两个不同的元素$a, b \\in A$，或者$(a,b) \\in R$或者$(b,a) \\in R$，也就是说对两个不同的数据操作在事务集合中的顺序满足唯一约束。 下面根据偏序定义可以给出在页模型下的事务定义：一个事务$t$是具体有$r(x)$或$w(x)$形式的多个操作步骤的偏序集合，其中$x \\in D$，对同一个页数据的读写操作和写写操作是有序的，正式的，事务可以表示为一个二元组：$$t=(op, &lt;)$$ 其中，$op$是具有$r(x)$或者$w(x)$的有限集合，$x \\in D$且$x&lt; \\subseteq op \\times op$是在$op$集合上的偏序关系，如果${p,q} \\subseteq op$且$p, q$都对同一个页数据项进行访问，且$p,q$是少有一个是写操作，则有$p &lt; q \\wedge q &lt; p$。 根据页事务模型的数学定义可推理出，偏序关系下，不允许同一数据项的读写操作或者写写操作乱序。 对象模型对象模型可以认为是页模型的替代或者推广，实际上，页模型是隐含在对象模型中。对象模型提供了一个表达在任意类型对象上进行的操作框架，而处于提高性能的需要，可能还需要利用被调用操作的语义。实际上，在一个对象及其操作的实现过程中，是需要调用底层对象的操作。 例如在数据库访问层中，比如索引查找，需要调用存储层面向页的操作，类似的调用层次还在业务对象集合中。上图中描述了一个事务，标记为$t_1$，执行了一个SQL的Select语句，从数据库中获取所有名叫“tom”的人员信息，在获取结果后，执行一个SQL语句插入一个新信息。 SQL语句在SQL引擎侧已经被转化为查询处理层的操作。在这个例子中，假设包含两个记录$x$和$y$，查询操作调用存储层的读写页操作。这里假设要读B+树的根节点，并且简单起见，只有两层。叶子节点标记为$l$，其中包含所有符合“tom”的列表，通过主键获取满足条件的记录$x$和$y$，访问每个记录各需要一次页读取，页分别记为$p$和$q$，最后还需要调用一个SQL的Insert语句插入新的信息：首先读取存储层的元数据页，找到有足够空闲的页$p$，读取页$p$，把要插入的记录写入，然后写回该页，最后还要在索引上把新记录的主键加入到B+树上名字叫“tom”的列表中。 上述例子中，实际上是把事务看作一棵树，而被调用的操作作为树节点，为了保证事务树是独立的并且允许严格推理，要求事务树的叶节点必须是页模型的基本读写操作。如果一个给定事务树的叶节点全部或者部分与页读写不对应，唯一能做的就是扩展这棵树把叶子节点全部变成页读写操作。 而这种扩展是动态的，跟踪在执行过程中一个高级事务中的所有低级数据操作，而不是从操作的静态结构生成的一个调用层次图。而跟踪一个事务内部操作的顺序，一种简单的方法就是给所有的叶节点确定一个顺序，和上述图中从左到右的事务顺序一致。所以像页模型一样，也同样要求事务树的所有叶节点保持偏序关系。 现在可以正式给出对象模型事务的定义：一个事务$t$是一棵有限树，节点进行如下标记： 事务标识符来标记根节点； 被调用操作的名称和参数标记内部非叶非根节点； 页模型的读写操作标记叶子节点；在叶子节点上定义了偏序关系“$&lt;$”，使得所有的叶子节点操作$p,q$，其中$p$形如$w(x)$, q形如$r(x) \\wedge w(x)$，或相反，都满足$p &lt; q \\vee q &lt; p$。 对象模型中事务对应的树不一定是平衡的，也就是说从根节点到所有叶子节点的路径不一定是等长的，而为了清晰起见，讨论问题时把属于同样对象操作类型或者接口的操作放在事务树的同层，所以当后续利用对象模型考虑并发执行时，经常放在完全平衡的事务树中。 这种事务树所有叶节点到根节点的路径是等长的，成为分层事务或者多级事务。 在上述的讨论中，有一个问题被忽略了，偏序只在树的叶子节点被提及和定义。内部节点的顺序实际上在叶子节点被约束后已经被隐式定义了。例如，对两个内部节点操作$a$和$b$，如果在叶子节点偏序“&lt;”下，$a$节点下的所有后代叶子节点先于$b$节点下的所有后代叶子节点处理，则称$a$先于$b$，这需要后代叶子节点集合之间有序，否则称$a$和$b$是并发操作。如果想找到中间节点对存储引擎的数据影响，可以检查其子女节点以及更远后代之间的交叉情况，直到叶子节点，其中的偏序关系保证了数据解释的明确性。 最后简要的阐述后续系列如何使用引入的对象模型作为高级并发控制算法的基础来结束本节。当考虑多个事务以并发或者并行的方式执行时，要把所有的事务树组合，形成一个操作的森林，然后检查叶子节点之间的偏序关系，以及隐含推导出来的高层操作顺序。与单层事务不同的是，需要在所有事务树的所有叶子节点集合上来定义偏序关系。而由于内部节点之间的顺序可以由叶子节点的偏序关系推导，可以研究高层节点之间的并发或者并行。而增强事务性能的关键，是高层内部节点操作的语义特性纳入事务调度的考虑范围。","link":"/2024/05/09/Transaction-1/"},{"title":"数据库事务模型分析（2）—— 等价关系","text":"在本节中，基于上一节的页模型，提出并发执行正确性的概念，并建立多个事务在时间上交叉执行的模型，提出有关并发事务调度的概念。首先要明确的是，事务的执行是高度动态的，完全建模会带来巨大的理解成本，实际上，当事务到来，并发控制需要对该事务做出执行决定，并使能其和系统中正在运行的事务正确同步。其次，需要考虑执行失败而被终止的事务，在正常执行的路径中，包含提交终止。这些操作的处理方式和直觉很不一样。 并发执行的事务之间的数据访问操作存在潜在的冲突，比如“修改后丢失”、“读不一致”、“脏读”等问题，作为事务执行顺序的保证来说，这些异常都是要避免的，给外部执行一致的印象。 为了判断事务执行时，何种情况是期望的，何种情况是应该避免的，调度器需要在线的应用这些规则。首先，本文假定调度中包含了事务结束标志成功或者不成功，具体的，一个事务成功的结束使用$c$表示，即事务在没有被中断的情况下完成了内部所有的数据操作，而失败事务的结束使用$a$表示，一个被终止的事务不应该对底层数据产生任何影响，这由恢复过程保证。 调度和历史设$T={t_1,…,t_n}$是一个有限事务集合，对每一个$t_i \\in T$，都有$t_i=(op_i,&lt;_i)$，其中$op_i$是$t_i$的操作集合，$&lt;_i$表示操作的顺序，$i \\in [1, n]$。$T$中的一个历史$s=(op(s),&lt;_s)$定义如下： $op(s) \\subseteq \\bigcup_{i=1}^{n}op_i \\bigcup_{i=1}^{n}{a_i,c_i}$，并且$\\bigcup_{i=1}^{n}op_i \\subseteq op(s)$，即事务的历史$s$是由事务所有操作的并集和每个事务的终止操作组成； $ \\forall i \\in [1,n], c_i \\in op(s) \\Leftrightarrow a_i \\notin op(s)$，也即每个事务都有结束，成功或者失败，但不能同时存在； $ \\bigcup_{i=1}^{n}&lt;_i \\subseteq &lt;_s$，也即所有事务的顺序都包含在由$s$给出的偏序中； $ \\forall i \\in [1,n], \\forall p \\in op_i, p &lt;_s a_i | p &lt;_s c_i$，也即成功或者失败总是作为事务最后一步出现； 对每一个来自不同事务的一对操作$p,q \\in op(s)$，如果访问同一个数据页并且其中至少一个为写操作，满足$p&lt;_s q|q &lt;_s p$。 因此一个历史，或者满足偏序的事务来说，必须：包含所有事务的所有操作（1），使每个事务都包含唯一的结束符（2），保持每个事务中操作的顺序（3），以结束符作为每个事务的最后一步（4），安排冲突操作的顺序（5）。由于（1）和（2），字面上历史也被称作完整调度。 同样的，也可以给出串行历史的调度：一个事务序列历史$s$为串行的，当且仅当在$s$中对历史中的任意两个事务$t_i$和$t_j, i \\neq j$，$t_j$的所有操作都在$t_i$之后，或者相反。 调度正确性判定准则这一部分的目标是给出调度的正确性准则，如果$S$是所有调度的集合，正确性准则可以在形式上看作一个映射：$$\\sigma.S \\rightarrow {0,1}$$这个映射对每个调度$s \\in S$返回一个布尔值，因此，对于调度$s \\in S$，如果$s$是正确的，那么$\\sigma(s)=1$，也即：$$correct(S):={ s \\in S | \\sigma(s) = 1}$$ 针对一个调度，判断其正确性的准则至少需要满足以下要求： $S$ 中至少存在某些正确的调度； $s \\in correct(S)$是可以有效判定的，不存在停机问题； 对于给定的事务集合，可以找到调度最优解。 本系列中要保持底层数据的完整性，并且每个事务都能保持数据的完整性。而串行事务执行总是正确的，在合适的选择事务调度的等价关系后，使用串行历史作为正确性的度量标准，因此可以给出在所有调度的集合$S$上的等价关系：$$([S]\\approx) ={[s] \\approx | s \\in S }$$其中，$[S]\\approx$是通过$\\approx$得到的所有等价类的集合，显然，一个调度集合中的正确调度都是两两等价的，因此任意一个调度$s$都可以代表这个类。而对可以选出串行调度的调度集合，这种调度集合中的调度被成为可串行化的。因此在接下来的分析中，本文会做以下两件事： 定义调度的等价概念； 通过和串行历史的等价定义“可串行化”。 调度Herbrand等价语义接下来本文将通过一种语义的概念在调度之间建立等价关系。精确地描述未知的事务语义是困难的，因此为了明确事务调度在事务上下文的语义，先明确在调度中出现的操作的语义，然后再定义事务调度本身。 本文在考虑调度等价的问题时，先对事务失败回滚的情况做出忽略，稍后在后面的章节对该问题进行展开。 对一个任意的调度$s$，有如下假设： 一个事务$t_i \\in trans(s)$的一个操作$r_i(x) \\in s$读取在$r_i(x)$之前出现的最后一个$w_j(x), j \\neq i$写入的值； 一个操作$w_j(x) \\in s$写入的新值潜在依赖于$t_i$中$w_j(x)$之前的属于$active(s) \\cup committed(s)$的事务中读取的所有数据的值。 对于第一个假设，可能存在一个问题是，并不是调度中每个读操作前都有一个写操作，比如：$$s=r_1(x)r_2(x)w_1(x)r_2(x)…$$为了统一不同的情况，假设在每个调度的头部都有一个假想的初始事务$t_0$，$t_0$写入调度中提及的所有数据项后提交，这种情况下，上面提到的调度变成：$$s=w_0(x)w_0(y)c_0r_1(x)r_2(x)w_1(x)r_2(x)…$$简单来说，初始事务为一个给定的调度定义了初始状态。 设$s$是一个调度，操作$r_i(x), w_i(x) \\in op(x)$的Herbrand语义$H_s$如下递归定义： $H_s(r_i(x)) := H_s(w_j(x))$, 其中$w_j(x),j \\neq i$，是$s$中在$r_i(x)$之前的对$x$的最后一个写操作； $H_s(w_i(x)) := f_{ix}(H_s(r_i(y_1)),…,H_s(r_i(y_m)))$，其中$r_i(y_j), 1 \\leq j \\leq m$，是事务$t_i$在调度$s$里发生在$w_i(x)$之前所有读操作，而$f_{ix}$是一个未知m元函数。假设在每一个事务中，对每一个数据项至多只有一个写操作，易验证对每个操作$p \\in op(s)$，$H_s(p)$的定义都是合理的。其次，初始事务$t_0$对数据的写操作和一个0元函数$f_{0x}$联系起来。 而在此基础上可以进一步推广“事务的Herbrand空间”，也即Herbrand全域。设$D={x,y,z…}$是一个数据的有限集合，对于一个事务$t$，设$op(t)$是事务$t$对数据的操作集合，事务$t_i(i&gt;0)$的Herbrand空间$HU$是满足下面条件的最小符合集合： 对每个$x \\in D$，有$f_{0x}() \\in HU$； 若$w_j(x) \\in op(t_i), |{r_i(y)|(y \\in D)r_i(y) &lt; t_i w_i(x)}|=m$，并且如果$v_1,…,v_m \\ in HU$，那么$f_{ix}(v_1,…v_m) \\in HU$。 因此调度中对数据操作的语义范围是Herbrand空间值的集合，但Herbrand空间纯粹是基于数学符号的语义构造，没有真实事务读写真实数据的任何信息。 最后根据Herbrand全域定义可以推出调度的Herbrand语义，所以调度$s$的语义是映射：$$H[s]:D \\rightarrow HU$$进一步的，$$H[s] (s) := H_s(w_i(s))$$其中对于每个$x \\in D$，$w_i(x)$是$s$对$x$的最后一个写操作。也就是说，调度$s$的Herbrand语义是$s$中最终写入值得集合。而再次重申暂时不对异常终止做考虑。上述得定义虽然普通，但有趣得是，能适用于任意复杂具体事务的解释，但同样的，Herbrand一般性的代价是具体处理起来异常复杂。有了以上定义，可以依据此定义不同可串行的事务调度形式，或者说定义了和串行调度等价的联系，而从根本上说，这样的等价关系定义只对历史（也就是完整调度）有意义，后面本系列会讨论如何定义任意调度的正确性问题，在这种情况下，调度也不必是历史。","link":"/2024/05/12/Transaction-2/"}],"tags":[{"name":"debug","slug":"debug","link":"/tags/debug/"},{"name":"DuckDB","slug":"DuckDB","link":"/tags/DuckDB/"},{"name":"Performance","slug":"Performance","link":"/tags/Performance/"},{"name":"cpp2x","slug":"cpp2x","link":"/tags/cpp2x/"},{"name":"CUDA, PTX","slug":"CUDA-PTX","link":"/tags/CUDA-PTX/"},{"name":"paper","slug":"paper","link":"/tags/paper/"},{"name":"rdma","slug":"rdma","link":"/tags/rdma/"},{"name":"rocksdb","slug":"rocksdb","link":"/tags/rocksdb/"},{"name":"transaction","slug":"transaction","link":"/tags/transaction/"}],"categories":[],"pages":[{"title":"Whoami","text":"Hi 👋 My name is HuanbingDistributed System &amp; Database Dev 🌍 I’m based in Shanghai 🖥️ See my portfolio at Blog ✉️ You can contact me at luhuanbing084@gmail.com 🚀 I’m currently working on TemplateDB 🧠 I’m learning TLA+ &amp; TSDB 🤝 I’m open to collaborating on TiKV &amp; Databend &amp; RisingWave ⚡ I am Iron man Skills Socials","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}]}